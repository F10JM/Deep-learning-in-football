{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f18393-e7a1-4d7b-96e1-ba158b139757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6520745-9cbd-4fc7-b8e7-e247bdc23d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import flags\n",
    "import sys\n",
    "sys.argv = sys.argv[:1]\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from yolov3_tf2.models import YoloV3\n",
    "from yolov3_tf2.dataset import transform_images\n",
    "from yolov3_tf2.utils import convert_boxes\n",
    "\n",
    "from deep_sort import preprocessing\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "from tools import generate_detections as gdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d7e20d-3356-42a8-9d41-c68c8799e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cosine_distance = 0.5\n",
    "nn_budget = None\n",
    "nms_max_overlap = 0.8\n",
    "\n",
    "model_filename = 'model_data/mars-small128.pb'\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric('cosine', max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a712cea-2bf1-4db2-9d28-c6a3563eacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eacdaca-b24f-4fe3-b3b2-0db78438805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded: /Users/fadijemmali/Desktop/Tracker/faster_rcnn_inception_resnet_v2_1024x1024/export/checkpoint/ckpt-0\n"
     ]
    }
   ],
   "source": [
    "model_path='/Users/fadijemmali/Desktop/Tracker/faster_rcnn_inception_resnet_v2_1024x1024/export'\n",
    "configs = config_util.get_configs_from_pipeline_file(os.path.join(model_path,'pipeline.config'))\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "\n",
    "# Load the latest checkpoint\n",
    "latest_ckpt = tf.train.latest_checkpoint(os.path.join(model_path,'checkpoint'))\n",
    "if latest_ckpt:\n",
    "    ckpt.restore(latest_ckpt).expect_partial()\n",
    "    print(f\"Checkpoint loaded: {latest_ckpt}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No checkpoint found in\", os.path.join(model_path,'checkpoint'))\n",
    "\n",
    "@tf.function\n",
    "def rcnn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3111fdd6-b226-41e1-ab7a-590020bece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [{'name':'ball', 'id':1}, {'name':'goalkeeper', 'id':2},{'name':'player', 'id':3},{'name':'referee', 'id':4}]\n",
    "\n",
    "with open('label_map.pbtxt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')\n",
    "category_index = label_map_util.create_category_index_from_labelmap('label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4756cd5e-5a49-4c4e-a05d-27419bb79c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fn2(img, threshold=0.25):\n",
    "    image_np = np.array(img)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = rcnn(input_tensor)\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    \n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    boxes = detections['detection_boxes']\n",
    "    boxes = boxes[:, [1, 0, 3, 2]]  # Rearranging box coordinates\n",
    "    scores = detections['detection_scores']\n",
    "    classes = detections['detection_classes']\n",
    "\n",
    "    ## Filter out detections with scores less than the threshold\n",
    "    keep = scores > threshold\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    classes = classes[keep]\n",
    "\n",
    "    # Process class names\n",
    "    class_names = ['person' if category_index[class_id + 1]['name'] in ['player', 'goalkeeper', 'referee'] else category_index[class_id + 1]['name'] for class_id in classes]\n",
    "    \n",
    "    return boxes, scores, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3c76ce-f69e-4022-9b9a-df4074de17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_white.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_white.mp4', codec, vid_fps, (vid_width, vid_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7f341-d087-48e2-a803-903264268a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_data = []\n",
    "current=1\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    boxes, scores, names= detect_fn2(img)\n",
    "    #allowed_classes=['player']\n",
    "    #deleted_indx = []\n",
    "    #for j in range(len(boxes)):\n",
    "    #    if not (names[j] in allowed_classes):\n",
    "    #        deleted_indx.append(j)\n",
    "    #boxes = np.delete(boxes, deleted_indx, axis=0)\n",
    "    #names = np.delete(names, deleted_indx, axis=0)\n",
    "    #scores = np.delete(scores, deleted_indx, axis=0)\n",
    "\n",
    "    converted_boxes = convert_boxes(img, boxes)\n",
    "    features = encoder(img, converted_boxes)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in\n",
    "                   zip(converted_boxes, scores, names, features)]\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "\n",
    "    cmap = plt.get_cmap('tab20b')\n",
    "    colors = [cmap(i)[:3] for i in np.linspace(0,1,20)]\n",
    "\n",
    "    current_count = int(0)\n",
    "\n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "        \n",
    "        bbox = track.to_tlbr()\n",
    "        class_name= track.get_class()\n",
    "        color = colors[int(track.track_id) % len(colors)]\n",
    "        color = [i * 255 for i in color]\n",
    "\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color, 2)\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)\n",
    "                    +len(str(track.track_id)))*17, int(bbox[1])),color, -1)\n",
    "        cv2.putText(img, class_name+\"-\"+str(track.track_id), (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "        object_data = {\n",
    "            \"box\": track.to_tlbr(),\n",
    "            \"id\": track.track_id,\n",
    "            \"class\": track.get_class()\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_frames_data.append(frame_objects)\n",
    "        \n",
    "    fps = 1./(time.time()-t1)\n",
    "    cv2.putText(img, \"FPS: {:.2f}\".format(fps), (0,30), 0, 1, (0,0,255), 2)\n",
    "    #cv2.resizeWindow('output', 1024, 768)\n",
    "    #cv2.imshow('output', img)\n",
    "    \n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "    print(f\"frame{current} done\")\n",
    "    current+=1\n",
    "\n",
    "vid.release()\n",
    "\n",
    "out.release()\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "#all_frames_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2640974-c6a5-4650-badb-005dce61d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nonblack_np(img):\n",
    "    \"\"\"Return the number of pixels in img that are not black.\n",
    "    img must be a Numpy array with colour values along the last axis.\n",
    "\n",
    "    \"\"\"\n",
    "    return img.any(axis=-1).sum()\n",
    "def red(image):\n",
    "    \n",
    "    # Convert the image from BGR to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define lower and upper range of yellow in HSV\n",
    "    lower_red1 = np.array([0, 120, 70])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 120, 70])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "   \n",
    "    mask_red1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "    \n",
    "    # Combine masks\n",
    "    full_mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "    red_regions = cv2.bitwise_and(image, image, mask=full_mask_red)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(red_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    \n",
    "    return ratio\n",
    "    \n",
    "def green(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([40, 40, 40])  # Lower bound of light green\n",
    "    upper_green = np.array([70, 255, 255])  # Upper bound of light green\n",
    "    \n",
    "    # Create masks for the yellow range\n",
    "    mask_green = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "    \n",
    "    green_regions = cv2.bitwise_and(image, image, mask=mask_green)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(green_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    return ratio\n",
    "def yellow(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([30, 255, 255])\n",
    "    \n",
    "    # Create masks for the yellow range\n",
    "    mask_yellow = cv2.inRange(hsv_image, lower_yellow, upper_yellow)\n",
    "    \n",
    "    yellow_regions = cv2.bitwise_and(image, image, mask=mask_yellow)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(yellow_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "def white(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the range for white color\n",
    "    lower_white = np.array([0, 0, 200])\n",
    "    upper_white = np.array([180, 55, 255])\n",
    "\n",
    "    # Create masks for the white range\n",
    "    mask_white = cv2.inRange(hsv_image, lower_white, upper_white)\n",
    "    \n",
    "    white_regions = cv2.bitwise_and(image, image, mask=mask_white)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(white_regions)\n",
    "    ratio = color_pix / tot_pix\n",
    "    \n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e49c1f1-847f-4484-baf4-c4e40850573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Specify the file name\n",
    "file_name = \"/Users/fadijemmali/Desktop/Tracker/data/all_frames_data3.json\"\n",
    "def convert_to_serializable(data):\n",
    "   if isinstance(data, np.ndarray):\n",
    "        return data.tolist()\n",
    "   elif isinstance(data, list):\n",
    "        return [convert_to_serializable(item) for item in data]\n",
    "   elif isinstance(data, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in data.items()}\n",
    "   else:\n",
    "        return data\n",
    "\n",
    "# Convert all ndarrays to lists\n",
    "serializable_data = convert_to_serializable(all_frames_data)\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    json.dump(serializable_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c9e930-4194-40d2-89b1-c5a537fbfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load from JSON file\n",
    "file_name = \"/Users/fadijemmali/Desktop/Tracker/data/all_frames_data3.json\"\n",
    "with open(file_name, 'r') as file:\n",
    "    all_frames_data = json.load(file)\n",
    "\n",
    "# Now loaded_data contains the data from the JSON file\n",
    "\n",
    "data=all_frames_data.copy()\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b1ff286-368b-4d70-aa75-e808fdf13d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "%matplotlib inline\n",
    "vid = cv2.VideoCapture('./data/video/red_white.mp4')\n",
    "\n",
    "for frame in data:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:\n",
    "        xmin, ymin, xmax, ymax = obj['box']\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        crop_img = img[ymin:ymax, xmin:xmax]\n",
    "        if crop_img.size > 0:\n",
    "            class_name= obj['class']\n",
    "            id=obj['id']\n",
    "            if class_name=='person':\n",
    "                if red(crop_img)>0.05 or white(crop_img)>0.05:\n",
    "                    if red(crop_img)>white(crop_img):\n",
    "                        obj['class']='team1'\n",
    "                    else:\n",
    "                        obj['class']='team2'\n",
    "                    \n",
    "    #persons=[obj for obj in frame if obj['class'] == 'person']\n",
    "    #team1=[obj for obj in frame if obj['class'] == 'team1']\n",
    "    #team2=[obj for obj in frame if obj['class'] == 'team2']\n",
    "    #balls=[obj for obj in frame if obj['class'] == 'ball']\n",
    "    \n",
    "    last_id_per_class = {}\n",
    "    color_dict = {\n",
    "    \"team1\": (0, 0, 255),    # Red color for team1 (BGR format)\n",
    "    \"team2\": (0, 255, 255),    # yellow color for team2\n",
    "    \"person\": (255, 0, 255), # Purple color for person (mix of red and blue)\n",
    "    \"ball\": (255, 255, 255)  # White color for ball\n",
    "    }\n",
    "vid.release()\n",
    "\n",
    "vid = cv2.VideoCapture('./data/video/red_white.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_white.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "\n",
    "labels_per_id = defaultdict(list)\n",
    "\n",
    "for frame in data:  # Iterate over each frame\n",
    "    for obj in frame:  # Iterate over each detection in the frame\n",
    "        if obj['id'] == 20:\n",
    "            obj['class']='person'\n",
    "        player_id = obj['id']\n",
    "        label = obj['class']\n",
    "        labels_per_id[player_id].append(label)\n",
    "\n",
    "# Dictionary to store the most common label for each ID\n",
    "most_common_label_per_id = {}\n",
    "\n",
    "for player_id, labels in labels_per_id.items():\n",
    "    # Determine the most common label\n",
    "    most_common_label = Counter(labels).most_common(1)[0][0]\n",
    "    most_common_label_per_id[player_id] = most_common_label\n",
    "\n",
    "\n",
    "new_id_mapping = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for old_id, label in most_common_label_per_id.items():\n",
    "    # Increment the counter for the label and assign the new ID\n",
    "    new_id_mapping[label]['counter'] += 1\n",
    "    new_id_mapping[label][old_id] = new_id_mapping[label]['counter']\n",
    "\n",
    "def get_new_id(old_id, label):\n",
    "    \"\"\"\n",
    "    Given an old ID and its label, returns a new ID that is unique within the label category.\n",
    "    \"\"\"\n",
    "    return new_id_mapping[label][old_id]\n",
    "\n",
    "for frame in data:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:\n",
    "        id = obj['id']\n",
    "        # Assign the most common label to this detection\n",
    "        obj['class'] = most_common_label_per_id[id]\n",
    "        class_name = obj['class']\n",
    "        obj['id']=get_new_id(obj['id'],obj['class'])\n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "        #cv2.rectangle(img, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)\n",
    "        #            +len(str(id)))*17, int(bbox[1])),color, -1)\n",
    "        if obj['class']=='person' or obj['class']=='ball'  :\n",
    "            cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.4,\n",
    "                    color_dict[obj['class']], 2)\n",
    "        else:\n",
    "            cv2.putText(img,'player' + str(obj['id']), (int(bbox[0]), int(bbox[1]-10)), 0, 0.4,\n",
    "                    color_dict[obj['class']], 2)\n",
    "\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c11cf-b658-47dd-a00b-4501bc149475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
