{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce7b83e-6674-42c4-8411-b5a742073f25",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "running detection model with low confidence threshold (0.3)\n",
    "\n",
    "getting rid of detections other than ball under confidence threshhold (0.6)\n",
    "\n",
    "\n",
    "running tracking models on it\n",
    "\n",
    "\n",
    "converting to cvat format\n",
    "\n",
    "adding missing ball detections manually with cvat (+ fixing other detection and tracking results)\n",
    "\n",
    "exporting the annotations\n",
    "\n",
    "processing results: team detection + ball possession\n",
    "\n",
    "plotting the tracking data on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15eb3c5-6802-410e-9209-3ed873041e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49548741-5768-4354-a635-9108e4ca7fc4",
   "metadata": {},
   "source": [
    "# Deep-Sort Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0392b731-e233-4f12-8603-89a31399efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort import preprocessing\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "from tools import generate_detections as gdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9736b4b2-b41f-4f50-bac4-d79f17ff255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_boxes(image, boxes):\n",
    "    returned_boxes = []\n",
    "    for box in boxes:\n",
    "        box[0] = (box[0] * image.shape[1]).astype(int)\n",
    "        box[1] = (box[1] * image.shape[0]).astype(int)\n",
    "        box[2] = (box[2] * image.shape[1]).astype(int)\n",
    "        box[3] = (box[3] * image.shape[0]).astype(int)\n",
    "        box[2] = int(box[2]-box[0])\n",
    "        box[3] = int(box[3]-box[1])\n",
    "        box = box.astype(int)\n",
    "        box = box.tolist()\n",
    "        if box != [0,0,0,0]:\n",
    "            returned_boxes.append(box)\n",
    "    return returned_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b3a1c2-cf83-4d65-9f5c-aa8fd8f1d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cosine_distance = 0.5\n",
    "nn_budget = None\n",
    "nms_max_overlap = 0.8\n",
    "\n",
    "model_filename = 'deep_sort/mars-small128.pb'\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric('cosine', max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e1d79a-0355-4c7e-ac79-ac412c92f258",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Capture video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28a059-53ee-450e-ba05-799270b6c3a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/foot-5frames.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-5frames.mp4', codec, vid_fps, (vid_width, vid_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20c944-170b-4a72-8618-63ae6c248e30",
   "metadata": {},
   "source": [
    "### Tracking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5471b-d92f-427e-8ca2-3f291e132c00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_frames_data = []\n",
    "i=1\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "    img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    boxes, scores, names= detect_fn(img)\n",
    "\n",
    "    allowed_classes=['player']\n",
    "    deleted_indx = []\n",
    "    for j in range(len(boxes)):\n",
    "        if not (names[j] in allowed_classes):\n",
    "            deleted_indx.append(j)\n",
    "    boxes = np.delete(boxes, deleted_indx, axis=0)\n",
    "    names = np.delete(names, deleted_indx, axis=0)\n",
    "    scores = np.delete(scores, deleted_indx, axis=0)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "    converted_boxes = convert_boxes(img, boxes)\n",
    "    features = encoder(img, converted_boxes)\n",
    "\n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in\n",
    "                   zip(converted_boxes, scores, names, features)]\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "\n",
    "    cmap = plt.get_cmap('tab20b')\n",
    "    colors = [cmap(i)[:3] for i in np.linspace(0,1,20)]\n",
    "\n",
    "    current_count = int(0)\n",
    "\n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "        \n",
    "        bbox = track.to_tlbr()\n",
    "        class_name= track.get_class()\n",
    "        color = colors[int(track.track_id) % len(colors)]\n",
    "        color = [i * 255 for i in color]\n",
    "\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])), 2)\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)\n",
    "                    +len(str(track.track_id)))*17, int(bbox[1])), -1)\n",
    "        cv2.putText(img, class_name+\"-\"+str(track.track_id), (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    (255, 255, 255), 2)\n",
    "\n",
    "        object_data = {\n",
    "            \"box\": track.to_tlbr(),\n",
    "            \"id\": track.track_id,\n",
    "            \"class\": track.get_class()\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_frames_data.append(frame_objects)\n",
    "        \n",
    "    fps = 1./(time.time()-t1)\n",
    "    cv2.putText(img, \"FPS: {:.2f}\".format(fps), (0,30), 0, 1, (0,0,255), 2)\n",
    "    cv2.resizeWindow('output', 1024, 768)\n",
    "    cv2.imshow('output', img)\n",
    "    out.write(img)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "vid.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658402f-b848-4b03-838c-8aaf21671ab9",
   "metadata": {},
   "source": [
    "### Display video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8f8c5-56f7-4dbd-a7a7-eddcddd6a579",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def display_video_frames(video_path):\n",
    "    # Capture the video from the file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Read until video is completed\n",
    "    while cap.isOpened():\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_count += 1\n",
    "            # Convert the frame from BGR (OpenCV default) to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            plt.figure(figsize=(20,20))\n",
    "            # Display the resulting frame\n",
    "            plt.imshow(frame_rgb)\n",
    "            plt.title(f\"Frame {frame_count}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c4cc4-d5c7-486b-894a-914634a4a4e5",
   "metadata": {},
   "source": [
    "# Color filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b9121-ce30-40f4-8a05-545cbe678998",
   "metadata": {},
   "source": [
    "### Clustering by average color of player region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc04910-b194-4f9c-a115-de8bb4446cd9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "img=res\n",
    "colors=[]\n",
    "for i in range(len(frame)):\n",
    "    object=frame[i]\n",
    "    bbox=object['box']\n",
    "    bbox_int = bbox.astype(int)\n",
    "    # Extract each coordinate\n",
    "    xmin, ymin, xmax, ymax = bbox_int\n",
    "    # Extract the region of interest (ROI) using integer coordinates\n",
    "    player_region = img[ymin:ymax, xmin:xmax]\n",
    "    average_color = player_region.mean(axis=(0, 1))\n",
    "    colors.append(average_color)\n",
    "    plt.imshow(player_region)\n",
    "    plt.show()\n",
    "colors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e2c8e-08fa-4679-8b1c-1b661bf6964e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Assuming 'colors' is a list of average color values of players\n",
    "# Each color is a list or array of [R, G, B] values\n",
    "colors = np.array(colors)  # Convert to NumPy array for sklearn\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=2, n_init=10, random_state=0).fit(colors)\n",
    "labels = kmeans.labels_  # Get cluster labels for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d8ddb-dadf-4840-9b76-48d240e1cb7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(frame)):\n",
    "    object=frame[i]\n",
    "    label=labels[i]\n",
    "    bbox=object['box']\n",
    "    bbox_int = bbox.astype(int)\n",
    "    # Extract each coordinate\n",
    "    xmin, ymin, xmax, ymax = bbox_int\n",
    "    # Extract the region of interest (ROI) using integer coordinates\n",
    "    player_region = img[ymin:ymax, xmin:xmax]\n",
    "    print(label)\n",
    "    plt.imshow(player_region)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4a277-bb2a-421d-9371-ce3a5adfbda4",
   "metadata": {},
   "source": [
    "### Applying color masks to filer green color of grass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab92348-c162-42b3-b041-bfea28c9ad8c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv2.imread('/Users/fadijemmali/Desktop/Tracker/data/video/frames/image1.png')\n",
    "\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define range of green color in HSV\n",
    "lower_green = np.array([40,40, 40])\n",
    "upper_green = np.array([70, 255, 255])\n",
    "\n",
    "# Create a binary mask for green color\n",
    "mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "# Optional: Apply the mask to the image (setting green areas to black)\n",
    "res = cv2.bitwise_and(image, image, mask=~mask)\n",
    "res_bgr = cv2.cvtColor(res,cv2.COLOR_HSV2BGR)\n",
    "res_gray = cv2.cvtColor(res,cv2.COLOR_BGR2GRAY)\n",
    "kernel = np.ones((13,13),np.uint8)\n",
    "thresh = cv2.threshold(res_gray,127,255,cv2.THRESH_BINARY_INV |  cv2.THRESH_OTSU)[1]\n",
    "thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "# Now, when processing player regions, use the mask to exclude green areas\n",
    "plt.imshow(res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b326b-6427-4902-91c8-158e166eff7d",
   "metadata": {},
   "source": [
    "# Other utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa13c1f-835c-4066-931e-4762f9dc3531",
   "metadata": {},
   "source": [
    "### Drawing circles under players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac45934-0e1e-4e16-8d60-4751f222f057",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to draw an ellipse under detected objects\n",
    "def draw_ellipse(image, box, color, thickness=4):\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    bottom_center = (int((x_min + x_max) / 2), int(y_max))\n",
    "    axes = (int((x_max - x_min) / 2), int(0.35 * (x_max - x_min)))\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, -45, 235, color, thickness, cv2.LINE_4)\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, 0, 360, color, thickness)\n",
    "    cv2.ellipse(\n",
    "        image,\n",
    "        center=bottom_center,\n",
    "        axes=axes,\n",
    "        angle=0.0,\n",
    "        startAngle=-45,\n",
    "        endAngle=235,\n",
    "        color=color,\n",
    "        thickness=thickness,\n",
    "        lineType=cv2.LINE_4\n",
    "    )\n",
    "# Color definitions for different objects\n",
    "COLORS = {\n",
    "    \"ball\": (255, 255, 255),      # White\n",
    "    \"goalkeeper\": (133, 1, 1),    # Dark Red\n",
    "    \"player\": (0, 212, 187),      # Greenish\n",
    "    \"referee\": (0, 255, 255)      # Yellow\n",
    "}\n",
    "THICKNESS = 4\n",
    "\n",
    "\n",
    "for object in frame:\n",
    "    box = object[\"box\"]\n",
    "    class_name = object[\"class\"]\n",
    "    color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "    draw_ellipse(img, box, color, THICKNESS)\n",
    "\n",
    "# Display the frame\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8e730-7029-4921-94f7-8531ed3d7ac9",
   "metadata": {},
   "source": [
    "### Detecting ball possesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78659df-8786-4663-8767-bb02ca2d719f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PLAYER_IN_POSSESSION_PROXIMITY = 30\n",
    "MARKER_WIDTH = 10\n",
    "MARKER_HEIGHT = 10\n",
    "MARKER_MARGIN = 20\n",
    "PLAYER_MARKER_COLOR = (0, 0, 255)  # Red in BGR\n",
    "BALL_MARKER_COLOR = (0, 255, 0)  # Green in BGR\n",
    "MARKER_CONTOUR_COLOR = (0, 0, 0)  # Black in BGR\n",
    "MARKER_CONTOUR_THICKNESS = 1\n",
    "\n",
    "def calculate_marker(anchor):\n",
    "    x, y = anchor\n",
    "    return np.array([\n",
    "        [x - MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN],\n",
    "        [x, y - MARKER_MARGIN],\n",
    "        [x + MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN]\n",
    "    ], np.int32)\n",
    "\n",
    "def draw_marker(image, anchor, color):\n",
    "    marker_contour = calculate_marker(anchor)\n",
    "    cv2.fillPoly(image, [marker_contour], color)\n",
    "    cv2.polylines(image, [marker_contour], isClosed=True, color=MARKER_CONTOUR_COLOR, thickness=MARKER_CONTOUR_THICKNESS)\n",
    "\n",
    "def get_player_in_possession(players, ball):\n",
    "    if len(ball) != 1:\n",
    "        return None\n",
    "    ball_x, ball_y = ball[0]['center']\n",
    "    for player in players:\n",
    "        player_x, player_y = player['center']\n",
    "        if abs(player_x - ball_x) <= PLAYER_IN_POSSESSION_PROXIMITY and abs(player_y - ball_y) <= PLAYER_IN_POSSESSION_PROXIMITY:\n",
    "            return player\n",
    "\n",
    "for object in frame:\n",
    "    x_min, y_min, x_max, y_max = object['box']\n",
    "    object['center']=[(x_min+x_max)//2, (y_min+y_max)//2]\n",
    "\n",
    "players = [det for det in frame if det['class'] == 'player']\n",
    "ball = [det for det in frame if det['class'] == 'ball']\n",
    "player_in_possession = get_player_in_possession(players, ball)\n",
    "img2=img.copy()\n",
    "if player_in_possession:\n",
    "    draw_marker(img2, player_in_possession['center'], PLAYER_MARKER_COLOR)\n",
    "for b in ball:\n",
    "    draw_marker(img2, b['center'], BALL_MARKER_COLOR)\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6da2f-8710-478e-a56b-93d71c1e8d7b",
   "metadata": {},
   "source": [
    "# Getting detections from tensorflow model (Fast-RCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdbaaa-a2a3-4fd4-ae69-40de559eb972",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e6160-22a4-4f07-8e80-4532646ea23d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fab1d2-2b65-435f-a96f-714d8c8af0e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_path=r\"C:\\Users\\Fadi\\Desktop\\Tracker\\faster_rcnn_inception_resnet_v2_1024x1024\\export2\"\n",
    "configs = config_util.get_configs_from_pipeline_file(os.path.join(model_path,'pipeline.config'))\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "\n",
    "# Load the latest checkpoint\n",
    "latest_ckpt = tf.train.latest_checkpoint(os.path.join(model_path,'checkpoint'))\n",
    "if latest_ckpt:\n",
    "    ckpt.restore(latest_ckpt).expect_partial()\n",
    "    print(f\"Checkpoint loaded: {latest_ckpt}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No checkpoint found in\", os.path.join(model_path,'checkpoint'))\n",
    "\n",
    "@tf.function\n",
    "def rcnn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274a332-00bd-4919-aac2-ba81a9024f67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "category_index ={1: {'id': 1, 'name': 'ball'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2b214-7e78-4ed7-a368-b5a551cf641c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "category_index ={1: {'id': 1, 'name': 'ball'},\n",
    "                 2: {'id': 2, 'name': 'goalkeeper'},  \n",
    "                 3: {'id': 3, 'name': 'player'},\n",
    "                 4: {'id': 4, 'name': 'referee'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76822eca-429d-4ef9-805f-1a2316cfd15f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections = rcnn(input_tensor)\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "            for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'],\n",
    "          detections['detection_classes']+label_id_offset,\n",
    "          detections['detection_scores'],\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          min_score_thresh=.6,\n",
    "          agnostic_mode=False)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00b0d4-aca9-43d9-9e4c-9764a89be946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_fn(img, threshold=0.3):\n",
    "    image_np = np.array(img)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = rcnn(input_tensor)\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    \n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    \n",
    "    boxes = detections['detection_boxes']\n",
    "    boxes = boxes[:, [1, 0, 3, 2]]  # Rearranging box coordinates\n",
    "    scores = detections['detection_scores']\n",
    "    classes = detections['detection_classes']\n",
    "\n",
    "    ## Filter out detections with scores less than the threshold\n",
    "    keep = scores > threshold\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    classes = classes[keep]\n",
    "\n",
    "    # Process class names\n",
    "    class_names = [category_index[class_id + label_id_offset]['name'] for class_id in classes]\n",
    "    \n",
    "    return boxes, scores, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae4042-348a-41a9-b6dc-ae1145d60c0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_fn(img, threshold=0.5, ball_threshold=0.05):\n",
    "    image_np = np.array(img)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = rcnn(input_tensor)\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    \n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    \n",
    "    boxes = detections['detection_boxes']\n",
    "    boxes = boxes[:, [1, 0, 3, 2]]  # Rearranging box coordinates\n",
    "    scores = detections['detection_scores']\n",
    "    classes = detections['detection_classes']\n",
    "\n",
    "\n",
    "    # Apply different thresholds\n",
    "    keep = [(scores[i] > (ball_threshold if classes[i] ==0  else threshold)) for i in range(len(scores))]\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    classes = classes[keep]\n",
    "\n",
    "    # Process class names\n",
    "    class_names = [category_index[class_id + label_id_offset]['name'] for class_id in classes]\n",
    "    \n",
    "    return boxes, scores, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ddae8-6779-4632-b4e8-0aa6edc49026",
   "metadata": {},
   "source": [
    "# Funtion to detect player team color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712862d-a064-4015-a67a-f2825f8ccf06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def count_nonblack_np(img):\n",
    "    \"\"\"Return the number of pixels in img that are not black.\n",
    "    img must be a Numpy array with colour values along the last axis.\n",
    "\n",
    "    \"\"\"\n",
    "    return img.any(axis=-1).sum()\n",
    "def red(image):\n",
    "    \n",
    "    # Convert the image from BGR to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define lower and upper range of yellow in HSV\n",
    "    lower_red1 = np.array([0, 120, 70])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 120, 70])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "   \n",
    "    mask_red1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "    \n",
    "    # Combine masks\n",
    "    full_mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "    red_regions = cv2.bitwise_and(image, image, mask=full_mask_red)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(red_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    \n",
    "    return ratio\n",
    "    \n",
    "def green(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([40, 40, 40])  # Lower bound of light green\n",
    "    upper_green = np.array([70, 255, 255])  # Upper bound of light green\n",
    "    \n",
    "    # Create masks for the yellow range\n",
    "    mask_green = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "    \n",
    "    green_regions = cv2.bitwise_and(image, image, mask=mask_green)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(green_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    return ratio\n",
    "def yellow(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([30, 255, 255])\n",
    "    \n",
    "    # Create masks for the yellow range\n",
    "    mask_yellow = cv2.inRange(hsv_image, lower_yellow, upper_yellow)\n",
    "    \n",
    "    yellow_regions = cv2.bitwise_and(image, image, mask=mask_yellow)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(yellow_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "def white(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the range for white color\n",
    "    lower_white = np.array([0, 0, 200])\n",
    "    upper_white = np.array([180, 55, 255])\n",
    "\n",
    "    # Create masks for the white range\n",
    "    mask_white = cv2.inRange(hsv_image, lower_white, upper_white)\n",
    "    \n",
    "    white_regions = cv2.bitwise_and(image, image, mask=mask_white)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(white_regions)\n",
    "    ratio = color_pix / tot_pix\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6f1ed-262f-4a23-992c-f30288d5d7ec",
   "metadata": {},
   "source": [
    "# Getting detections from yolov5 model (custom weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762cbee7-8c51-440f-b59e-9f0747e5e863",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba18d0-0bd8-4ee0-a157-6cfcce9aa049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!python yolov5/detect.py --weights best.pt --img 1280 --conf 0.25 --source data/video/red_white.mp4 --name custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd8d10-dd59-400e-9789-e0266666d8de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "WEIGHTS_PATH = \"ball.pt\"\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', WEIGHTS_PATH, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e8e27-4a3f-4148-8cb0-e959b530e32a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.conf = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdb089-2958-4f3a-94a1-305d1d419c87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_fn(img):\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    res= model(img, size=1280)\n",
    "    detections = res.pred[0]\n",
    "\n",
    "    # Extract bounding boxes, scores, and class IDs\n",
    "    boxes = detections[:, :4].numpy()  # x_center, y_center, width, height\n",
    "    scores = detections[:, 4].numpy()\n",
    "    class_ids = detections[:, 5].int().numpy()\n",
    "    \n",
    "    # Convert class IDs to class names\n",
    "    class_names = [model.names[i] for i in class_ids]\n",
    "\n",
    "\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        x_min_normalized = x_min / width\n",
    "        y_min_normalized = y_min / height\n",
    "        x_max_normalized = x_max / width\n",
    "        y_max_normalized = y_max / height\n",
    "        normalized_boxes.append([x_min_normalized, y_min_normalized, x_max_normalized, y_max_normalized])\n",
    "\n",
    "    return(np.array(normalized_boxes),scores,class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03663adf-80ef-41a9-8c5f-cd88fbd7d920",
   "metadata": {},
   "source": [
    "# Running detection and tracking on video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58f23c-af38-4655-901b-c81b8d30ac6b",
   "metadata": {},
   "source": [
    "### Getting detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08c40d-a543-4857-8b97-864651041740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "all_detection_data = []\n",
    "current=1\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "\n",
    "    boxes, scores, names= detect_fn_yolov9(img)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        object_data = {\n",
    "            \"box\":boxes[i],\n",
    "            \"score\": scores[i],\n",
    "            \"class\": names[i]\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_detection_data.append(frame_objects)\n",
    "        \n",
    "    print(f\"frame{current}\", end=\" \")\n",
    "    current+=1\n",
    "\n",
    "vid.release()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Convert execution time to minutes\n",
    "execution_time_minutes = execution_time / 60\n",
    "\n",
    "# Print the execution time in minutes\n",
    "print()\n",
    "print(f\"Execution time: {execution_time_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cecde-59c6-4620-be51-2596d79f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert(obj):\n",
    "    \"\"\"\n",
    "    Convert objects that are not serializable by default.\n",
    "    This includes numpy arrays and numpy data types.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    raise TypeError(\"Object of type '%s' is not JSON serializable\" % type(obj).__name__)\n",
    "\n",
    "# Assuming all_detection_data is your array of arrays of dictionaries\n",
    "# ...\n",
    "\n",
    "# Write the array to a JSON file with custom serialization for numpy arrays and data types\n",
    "with open('all_detection_data_5min.json', 'w') as file:\n",
    "    json.dump(all_detection_data, file, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828f802-fbbf-46a6-801e-01f5e2286902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the array from the JSON file\n",
    "with open(\"all_detection_data_vid1 (1).json\", 'r') as file:\n",
    "    loaded_data = json.load(file)\n",
    "\n",
    "all_detection_data=loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12e586-6a68-4c64-943b-4f2b461b33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=all_detection_data_filtered\n",
    "total_frames = len(data)\n",
    "frames_with_ball_detected = 0\n",
    "\n",
    "for frame in data:\n",
    "    for detection in frame:\n",
    "        if detection['class'] == 'ball':\n",
    "            frames_with_ball_detected += 1\n",
    "            break  # Break as we only need to detect at least one ball per frame\n",
    "\n",
    "percentage_of_frames_with_ball = (frames_with_ball_detected / total_frames) * 100\n",
    "\n",
    "print(f\"Percentage of frames where the ball is detected: {percentage_of_frames_with_ball:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce3a83-aa5d-4775-a9d1-8576890b33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_detection_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec61b3-1ee6-438a-9421-80dbbc270ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within(player_bbox, ball_bbox, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Check if the ball bounding box is within the player bounding box by a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - player_bbox: The bounding box of the player (x1, y1, x2, y2).\n",
    "    - ball_bbox: The bounding box of the detected ball (x1, y1, x2, y2).\n",
    "    - threshold: The minimum fraction of the ball's area that must be inside the player's bounding box to consider it inside.\n",
    "\n",
    "    Returns:\n",
    "    - True if the ball is within the player bounding box by at least the given threshold, False otherwise.\n",
    "    \"\"\"\n",
    "    # Calculate the intersection of the two bounding boxes\n",
    "    x1_inter = max(player_bbox[0], ball_bbox[0])\n",
    "    y1_inter = max(player_bbox[1], ball_bbox[1])\n",
    "    x2_inter = min(player_bbox[2], ball_bbox[2])\n",
    "    y2_inter = min(player_bbox[3], ball_bbox[3])\n",
    "\n",
    "    # Calculate the area of the intersection\n",
    "    width_inter = max(0, x2_inter - x1_inter)\n",
    "    height_inter = max(0, y2_inter - y1_inter)\n",
    "    area_inter = width_inter * height_inter\n",
    "\n",
    "    # Calculate the area of the ball bounding box\n",
    "    width_ball = ball_bbox[2] - ball_bbox[0]\n",
    "    height_ball = ball_bbox[3] - ball_bbox[1]\n",
    "    area_ball = width_ball * height_ball\n",
    "\n",
    "    # Check if the intersection area is at least the threshold of the ball's area\n",
    "    if area_ball == 0:  # Prevent division by zero\n",
    "        return False\n",
    "    if (area_inter / area_ball) >= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "all_detection_data_filtered=[]\n",
    "for frame in all_detection_data:\n",
    "    other=[obj for obj in frame if obj['class']!='ball']\n",
    "    filtered_balls=[]\n",
    "    for obj in frame:\n",
    "        if obj['class']=='ball':\n",
    "            inside = any(is_within(player['box'], obj['box']) for player in other)\n",
    "            if not inside:\n",
    "                filtered_balls.append(obj)\n",
    "    all_detection_data_filtered.append(other+filtered_balls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b84b6e-9735-4204-ae8d-bb16d93ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('a9f16c_8.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-new_vid_1_2.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "color_dict = {\n",
    "\"team1\": (0, 0, 255),    \n",
    "\"team2\": (0, 255, 0),    \n",
    "\"referee\": (255,255,0), \n",
    "\"goalkeeper\": (255, 0, 0), \n",
    "\"player\": (255, 255, 0), \n",
    "\"ball\": (255, 255, 255)  \n",
    "}\n",
    "\n",
    "for frame in all_detection_data_filtered:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:  \n",
    "        if obj['class']!='ball' and obj['score']>0.9:\n",
    "            class_name = obj['class']\n",
    "    \n",
    "            bbox=obj['box']\n",
    "            xmin = bbox[0] * vid_width\n",
    "            ymin = bbox[1] * vid_height\n",
    "            xmax = bbox[2] * vid_width\n",
    "            ymax = bbox[3] * vid_height\n",
    "    \n",
    "            cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),color_dict[obj['class']], 2)\n",
    "            score= round(obj['score'],2)\n",
    "            cv2.putText(img,f\"{obj['class']} : {score}\", (int(xmin), int(ymin-10)), 0, 0.5,\n",
    "                        color_dict[obj['class']], 1)\n",
    "    for obj in ball_results[current]:  \n",
    "        class_name = obj['class']\n",
    "\n",
    "        bbox=obj['box']\n",
    "        xmin = bbox[0] \n",
    "        ymin = bbox[1] \n",
    "        xmax = bbox[2] \n",
    "        ymax = bbox[3] \n",
    "\n",
    "        cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),color_dict[obj['class']], 2)\n",
    "        score= round(obj['score'],2)\n",
    "        cv2.putText(img,f\"{obj['class']} : {score}\", (int(xmin), int(ymin-10)), 0, 0.5,\n",
    "                    color_dict[obj['class']], 1)\n",
    "\n",
    "\n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f435cd9-d0dd-4f73-a3bf-9d43a588d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    vid = cv2.VideoCapture(f'C:\\\\Users\\\\Fadi\\\\Desktop\\\\Rush drones\\\\video{i}.MP4')\n",
    "    \n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(f'C:\\\\Users\\\\Fadi\\\\Desktop\\\\Rush drones\\\\results_video{i}_yolov9.MP4', codec, vid_fps, (vid_width, vid_height))\n",
    "    \n",
    "    current=0\n",
    "    color_dict = {  \n",
    "    \"referee\": (0, 0, 0), \n",
    "    \"goalkeeper\":  (255, 0, 0), \n",
    "    \"player\": (0, 255, 255), \n",
    "    \"ball\": (0, 0, 255)  \n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # Read the array from the JSON file\n",
    "    with open(f'C:\\\\Users\\\\Fadi\\\\Downloads\\\\json_files (3)\\\\content\\\\json_files\\\\detection_data_video{i}_yolov9', 'r') as file:\n",
    "        loaded_data = json.load(file)\n",
    "    \n",
    "    all_detection_data=loaded_data\n",
    "    \n",
    "    for frame in all_detection_data:\n",
    "        _, img = vid.read()\n",
    "        if img is None:\n",
    "            print('Completed')\n",
    "            break\n",
    "        for obj in frame:  \n",
    "        \n",
    "                class_name = obj['class']\n",
    "        \n",
    "                bbox=obj['box']\n",
    "                xmin = bbox[0] * vid_width\n",
    "                ymin = bbox[1] * vid_height\n",
    "                xmax = bbox[2] * vid_width\n",
    "                ymax = bbox[3] * vid_height\n",
    "        \n",
    "                cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),color_dict[obj['class']], 2)\n",
    "                score= round(obj['score'],2)\n",
    "                cv2.putText(img,f\"{obj['class']} : {score}\", (int(xmin), int(ymin-10)), 0, 0.5,\n",
    "                            color_dict[obj['class']], 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        current+=1\n",
    "        out.write(img)\n",
    "        \n",
    "        #if cv2.waitKey(1) == ord('q'):\n",
    "        #    break\n",
    "        \n",
    "                \n",
    "    vid.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f'video{i} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78a607-b96a-4b9a-8b79-d55dc376872a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    vid = cv2.VideoCapture(f'C:\\\\Users\\\\Fadi\\\\Desktop\\\\Rush drones\\\\video{i}.MP4')\n",
    "    \n",
    "    codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(f'C:\\\\Users\\\\Fadi\\\\Desktop\\\\Rush drones\\\\final_results_video{i}.MP4', codec, vid_fps, (vid_width, vid_height))\n",
    "    \n",
    "    current=0\n",
    "    color_dict = {  \n",
    "    \"referee\": (0, 0, 0), \n",
    "    \"goalkeeper\":  (255, 0, 0), \n",
    "    \"player\": (0, 255, 255), \n",
    "    \"ball\": (0, 0, 255)  \n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # Read the array from the JSON file\n",
    "    with open(f'C:\\\\Users\\\\Fadi\\\\Downloads\\\\json_files (1)\\\\content\\\\json_files\\\\detection_data_video{i}_rcnn', 'r') as file:\n",
    "        loaded_data = json.load(file)\n",
    "    \n",
    "    all_detection_data_rcnn=loaded_data\n",
    "\n",
    "    with open(f'C:\\\\Users\\\\Fadi\\\\Downloads\\\\json_files (1)\\\\content\\\\json_files\\\\detection_data_video{i}_yolov5', 'r') as file:\n",
    "        loaded_data = json.load(file)\n",
    "    \n",
    "    all_detection_data_yolov5=loaded_data\n",
    "    \n",
    "    for i,frame in enumerate(all_detection_data_yolov5):\n",
    "        _, img = vid.read()\n",
    "        if img is None:\n",
    "            print('Completed')\n",
    "            break\n",
    "        for obj in frame:  \n",
    "        \n",
    "                class_name = obj['class']\n",
    "        \n",
    "                bbox=obj['box']\n",
    "                xmin = bbox[0] * vid_width\n",
    "                ymin = bbox[1] * vid_height\n",
    "                xmax = bbox[2] * vid_width\n",
    "                ymax = bbox[3] * vid_height\n",
    "        \n",
    "                cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),color_dict[obj['class']], 2)\n",
    "                score= round(obj['score'],2)\n",
    "                cv2.putText(img,f\"{obj['class']} : {score}\", (int(xmin), int(ymin-10)), 0, 0.5,\n",
    "                            color_dict[obj['class']], 1)\n",
    "\n",
    "        for obj in all_detection_data_rcnn[i]:\n",
    "            if obj['class']=='ball':\n",
    "                class_name = obj['class']\n",
    "        \n",
    "                bbox=obj['box']\n",
    "                xmin = bbox[0] * vid_width\n",
    "                ymin = bbox[1] * vid_height\n",
    "                xmax = bbox[2] * vid_width\n",
    "                ymax = bbox[3] * vid_height\n",
    "        \n",
    "                cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),color_dict[obj['class']], 2)\n",
    "                score= round(obj['score'],2)\n",
    "                cv2.putText(img,f\"{obj['class']} : {score}\", (int(xmin), int(ymin-10)), 0, 0.5,\n",
    "                            color_dict[obj['class']], 1)\n",
    "        out.write(img)\n",
    "        \n",
    "        #if cv2.waitKey(1) == ord('q'):\n",
    "        #    break\n",
    "        \n",
    "                \n",
    "    vid.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f'video{i} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc976946-03a1-4501-b9c5-7a53f2d915c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Downloads\\Tracker\\Tracker\\data\\video\\red_yellow.mp4\")\n",
    "    \n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(r\"C:\\Users\\Fadi\\Downloads\\Tracker\\Tracker\\data\\video\\red_yellow_yolov9.mp4\", codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "color_dict = {  \n",
    "\"referee\": (0, 0, 0), \n",
    "\"goalkeeper\":  (255, 0, 0), \n",
    "\"player\": (0, 255, 255), \n",
    "\"ball\": (0, 0, 255)  \n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "# Read the array from the JSON file\n",
    "with open(r\"C:\\Users\\Fadi\\Downloads\\json_files (3)\\content\\json_files\\detection_data_red_yellow_yolov9\", 'r') as file:\n",
    "    loaded_data = json.load(file)\n",
    "\n",
    "all_detection_data=loaded_data\n",
    "\n",
    "for frame in all_detection_data:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:  \n",
    "    \n",
    "            class_name = obj['class']\n",
    "    \n",
    "            bbox=obj['box']\n",
    "            xmin = bbox[0] * vid_width\n",
    "            ymin = bbox[1] * vid_height\n",
    "            xmax = bbox[2] * vid_width\n",
    "            ymax = bbox[3] * vid_height\n",
    "    \n",
    "            cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),color_dict[obj['class']], 2)\n",
    "            score= round(obj['score'],2)\n",
    "            cv2.putText(img,f\"{obj['class']} : {score}\", (int(xmin), int(ymin-10)), 0, 0.5,\n",
    "                        color_dict[obj['class']], 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    current+=1\n",
    "    out.write(img)\n",
    "    \n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "    \n",
    "            \n",
    "vid.release()\n",
    "out.release()\n",
    "\n",
    "print(f'video{i} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc3114b-8898-4010-b7cb-244db417b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video has 700 frames.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Replace 'path_to_video.mp4' with the path to your video file\n",
    "video_path = r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\X1.mp4\"\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the number of frames\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "vid_width,vid_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# Print the frame count\n",
    "print(f\"The video has {frame_count} frames.\")\n",
    "\n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70002d06-5b1b-4a89-a50e-9037ed6335d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'C:\\\\Users\\\\Fadi\\\\Downloads\\\\json_files (3)\\\\content\\\\json_files\\\\detection_data_video13_yolov9', 'r') as file:\n",
    "    loaded_data = json.load(file)\n",
    "    \n",
    "all_detection_data=loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304f1f0f-a4a8-4054-ba84-d615be4ab015",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detection_data=all_detection_data[0:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16eaa779-3342-4975-87ae-6db8e55066ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\video13.mp4\")\n",
    "    \n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\video13_test1.mp4\", codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "color_dict = {  \n",
    "\"referee\": (0, 0, 0), \n",
    "\"goalkeeper\":  (255, 0, 0), \n",
    "\"player\": (0, 255, 255), \n",
    "\"ball\": (0, 0, 255)  \n",
    "}\n",
    "\n",
    "for i in range(700):\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in all_detection_data[i]:\n",
    "        class_name = obj['class']\n",
    "    \n",
    "        bbox=obj['box']\n",
    "        xmin = bbox[0] * vid_width\n",
    "        ymin = bbox[1] * vid_height\n",
    "        xmax = bbox[2] * vid_width\n",
    "        ymax = bbox[3] * vid_height\n",
    "\n",
    "        cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),color_dict[obj['class']], 2)\n",
    "        score= round(obj['score'],2)\n",
    "        cv2.putText(img,f\"{obj['class']} : {score}\", (int(xmin), int(ymin-10)), 0, 0.5,\n",
    "                    color_dict[obj['class']], 1)\n",
    "    out.write(img)\n",
    "\n",
    "    \n",
    "            \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f00b432b-3db9-46aa-b341-60375ed7ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Extracted 700 frames.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_first_700_frames(input_video_path, output_video_path):\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    # Get video frame rate and size to use for the output video\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Use 'XVID' for avi format\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Read and write frames until we have 700 frames or video ends\n",
    "    count = 0\n",
    "    while cap.isOpened() and count < 700:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Write the frame into the output file\n",
    "            out.write(frame)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release everything when done\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Finished. Extracted {count} frames.\")\n",
    "\n",
    "# Example usage\n",
    "input_video_path = r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\video13.mp4\"\n",
    "output_video_path = r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\X1.mp4\"\n",
    "extract_first_700_frames(input_video_path, output_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff83f0-a472-4eaf-b366-bf1692def308",
   "metadata": {},
   "source": [
    "### Norfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332925e-24ef-4d54-80dd-fd812e51d79d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from norfair import Tracker, Detection\n",
    "import cv2  # OpenCV for video handling\n",
    "\n",
    "color_dict = {\n",
    "\"team1\": (0, 0, 255),    \n",
    "\"team2\": (0, 255, 0),    \n",
    "\"referee\": (255,255,0), \n",
    "\"goalkeeper\": (255, 0, 0), \n",
    "\"player\": (255, 255, 0), \n",
    "\"ball\": (255, 255, 255)  \n",
    "}\n",
    "\n",
    "# Define a distance function for the tracker\n",
    "def euclidean_distance(detection, tracked_object):\n",
    "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
    "\n",
    "# Initialize the tracker\n",
    "tracker = Tracker(\n",
    "    distance_function=\"iou\",\n",
    "    distance_threshold=30,  # Set according to your needs\n",
    ")\n",
    "\n",
    "# Function to convert detection data to Norfair Detection objects\n",
    "def detections_to_norfair_detections(frame):\n",
    "    norfair_detections = []\n",
    "    for detection in frame:\n",
    "        # Convert box format (xmin, ymin, xmax, ymax) to centroid (x_center, y_center)\n",
    "        xmin, ymin, xmax, ymax = detection['box']\n",
    "        bbox = np.array([[xmin,ymin],[xmax,ymax]])\n",
    "        score = np.array([detection['score'],detection['score']])\n",
    "        label = detection['class']\n",
    "        norfair_detections.append(Detection(points=bbox, scores=score, label=label))\n",
    "    return norfair_detections\n",
    "\n",
    "# Load your video\n",
    "video_path = './data/video/red_yellow.mp4'\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_yellow21.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break  # Video is over\n",
    "    \n",
    "    detections = all_detection_data[frame_idx] \n",
    "    norfair_detections = detections_to_norfair_detections(detections)\n",
    "    \n",
    "    # Update the tracker\n",
    "    tracked_objects = tracker.update(detections=norfair_detections)\n",
    "    \n",
    "    # Optional: visualize tracking\n",
    "    for obj in tracked_objects:\n",
    "        if obj.last_detection is not None:\n",
    "            bbox=obj.estimate.tolist()[0]+obj.estimate.tolist()[1]\n",
    "            bbox[0] = bbox[0] * vid_width\n",
    "            bbox[1] = bbox[1] * vid_height\n",
    "            bbox[2] = bbox[2] * vid_width\n",
    "            bbox[3] = bbox[3] * vid_height\n",
    "            \n",
    "            class_name=obj.last_detection.label\n",
    "            id=obj.id\n",
    "            cv2.rectangle(frame, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[class_name], 2)\n",
    "\n",
    "            cv2.putText(frame,f\"{class_name} : {id}\", (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                        color_dict[class_name], 2)\n",
    "    # Display the frame\n",
    "    out.write(frame)\n",
    "    \n",
    "\n",
    "    \n",
    "    frame_idx += 1\n",
    "\n",
    "vid.release()\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31be95e-ba0e-4927-a5eb-f09264358db1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from norfair import Tracker, Detection, Video\n",
    "from norfair.camera_motion import MotionEstimator\n",
    "from norfair.distances import mean_euclidean\n",
    "\n",
    "# Function to convert detections to Norfair format\n",
    "def detections_to_norfair_detections(detections):\n",
    "    norfair_detections = []\n",
    "    for detection in detections:\n",
    "        xmin, ymin, xmax, ymax = detection['box']\n",
    "        bbox = np.array([[xmin, ymin], [xmax, ymax]])\n",
    "        score = np.array([detection['score'], detection['score']])\n",
    "        label = detection['class']\n",
    "        norfair_detections.append(Detection(points=bbox, scores=score, label=label))\n",
    "    return norfair_detections\n",
    "\n",
    "# Initialize video, tracker, and motion estimator\n",
    "video_path = './data/video/red_yellow.mp4'\n",
    "video = Video(input_path=video_path)\n",
    "player_tracker = Tracker(distance_function=mean_euclidean, distance_threshold=250)\n",
    "ball_tracker = Tracker(distance_function=mean_euclidean, distance_threshold=150)\n",
    "motion_estimator = MotionEstimator()\n",
    "\n",
    "ball_trajectory = []  # To store ball positions for drawing the trajectory\n",
    "\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_yellow21.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "frame_idx = 0\n",
    "for frame in video:\n",
    "    # Assume you have a way to get your detections for players and the ball\n",
    "    player_detections_raw = all_data[frame_idx]  # Your method to get player detections\n",
    "    ball_detections_raw = ball_data[frame_idx]  # Your method to get ball detections\n",
    "    \n",
    "    # Convert detections to Norfair format\n",
    "    player_detections = detections_to_norfair_detections(player_detections_raw)\n",
    "    ball_detections = detections_to_norfair_detections(ball_detections_raw)\n",
    "\n",
    "    # Update motion estimator and get transformations\n",
    "    coord_transformations = motion_estimator.update(detections=player_detections + ball_detections, frame=frame)\n",
    "\n",
    "    # Update trackers with detections and coordinate transformations\n",
    "    player_tracker.update(detections=player_detections, coord_transformations=coord_transformations)\n",
    "    ball_tracks = ball_tracker.update(detections=ball_detections, coord_transformations=coord_transformations)\n",
    "\n",
    "    for track in player_tracks + ball_tracks:\n",
    "        # Draw bounding box\n",
    "        bbox = track.estimate\n",
    "        cv2.rectangle(frame, (int(bbox[0][0]), int(bbox[0][1])), (int(bbox[1][0]), int(bbox[1][1])), (255, 0, 0), 2)\n",
    "        \n",
    "        # Write class name and ID above the bounding box\n",
    "        label = f\"{track.label}-{track.id}\"\n",
    "        cv2.putText(frame, label, (int(bbox[0][0]), int(bbox[0][1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    # Draw trajectory\n",
    "    for track in ball_tracks:\n",
    "        if track.live_points.any():\n",
    "            ball_trajectory.append(track.estimate)\n",
    "            if len(ball_trajectory) > 1:\n",
    "                for i in range(len(ball_trajectory)-1):\n",
    "                    cv2.line(frame, ball_trajectory[i], ball_trajectory[i+1], (0, 255, 0), 2)\n",
    "   \n",
    "    out.write(frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "\n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de179598-d52e-41ba-a6e0-f508e8b8cb82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from norfair import Tracker, Detection, Video, distances\n",
    "from norfair.camera_motion import MotionEstimator, HomographyTransformationGetter\n",
    "\n",
    "# Assuming 'detections_to_norfair_detections' function is defined elsewhere\n",
    "# Function to convert detections to Norfair format\n",
    "def detections_to_norfair_detections(detections):\n",
    "    norfair_detections = []\n",
    "    for detection in detections:\n",
    "        xmin, ymin, xmax, ymax = detection['box']\n",
    "        bbox = np.array([[xmin, ymin], [xmax, ymax]])\n",
    "        score = np.array([detection['score'], detection['score']])\n",
    "        label = detection['class']\n",
    "        norfair_detections.append(Detection(points=bbox, scores=score, label=label))\n",
    "    return norfair_detections\n",
    "    \n",
    "# Initialize the motion estimator\n",
    "motion_estimator = MotionEstimator(\n",
    "    max_points=500,\n",
    "    min_distance=7,\n",
    "    transformations_getter=HomographyTransformationGetter(),\n",
    ")\n",
    "\n",
    "video_path = './data/video/red_yellow.mp4'\n",
    "video = Video(input_path=input_video_path)\n",
    "\n",
    "player_tracker = Tracker(\n",
    "    distance_function=distances.iou, \n",
    "    distance_threshold=0.5,\n",
    "    hit_counter_max=30,\n",
    "    initialization_delay=3\n",
    ")\n",
    "ball_tracker = Tracker(\n",
    "    distance_function=distances.iou, \n",
    "    distance_threshold=0.5,\n",
    "    hit_counter_max=30,\n",
    "    initialization_delay=3\n",
    ")\n",
    "\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_yellow21.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "frame_idx = 0\n",
    "for frame in video:\n",
    "    # Assume you have a way to get your detections for players and the ball\n",
    "    player_detections_raw = all_data[frame_idx]  # Your method to get player detections\n",
    "    ball_detections_raw = ball_data[frame_idx]  # Your method to get ball detections\n",
    "    \n",
    "    player_detections = detections_to_norfair_detections(player_detections_raw)\n",
    "    ball_detections = detections_to_norfair_detections(ball_detections_raw)\n",
    "\n",
    "    # Create a mask for motion estimation to ignore detected objects\n",
    "    mask = np.ones(frame.shape[:2], dtype=np.uint8)  # Assuming frame is grayscale for mask\n",
    "    for det in player_detections + ball_detections:\n",
    "        bbox = det.points.astype(int)\n",
    "        cv2.rectangle(mask, (bbox[0, 0], bbox[0, 1]), (bbox[1, 0], bbox[1, 1]), 0, -1)\n",
    "\n",
    "    # Update motion estimator\n",
    "    coord_transformations = motion_estimator.update(frame, mask=mask)\n",
    "\n",
    "    # Update trackers with detections and coordinate transformations\n",
    "    player_tracks = player_tracker.update(\n",
    "        detections=player_detections, \n",
    "        coord_transformations=coord_transformations\n",
    "    )\n",
    "    ball_tracks = ball_tracker.update(\n",
    "        detections=ball_detections, \n",
    "        coord_transformations=coord_transformations\n",
    "    )\n",
    "\n",
    "    # Draw bounding boxes and IDs for players and balls\n",
    "    for track in player_tracks + ball_tracks:\n",
    "        bbox = track.estimate\n",
    "        cv2.rectangle(frame, (int(bbox[0][0]), int(bbox[0][1])), (int(bbox[1][0]), int(bbox[1][1])), (255, 0, 0), 2)\n",
    "        label = f\"{track.label}-{track.id}\"\n",
    "        cv2.putText(frame, label, (int(bbox[0][0]), int(bbox[0][1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "\n",
    "vid.release()\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23248e23-8514-43c4-8133-f78fe331d4fc",
   "metadata": {},
   "source": [
    "### getting ball data before tracking (new approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199271c-325f-4283-a569-d0cc532538d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "ball_data=[]\n",
    "for frame in all_detection_data:\n",
    "    all=[]\n",
    "    ball=[]\n",
    "    for object in frame:\n",
    "        if object['class']=='ball':\n",
    "            ball.append(object)\n",
    "        else:\n",
    "            all.append(object)\n",
    "    all_data.append(all)\n",
    "    ball_data.append(ball)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e56433-7f61-491e-ba0a-8f900088b5a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a counter for frames with no ball detections\n",
    "no_detection_count = 0\n",
    "\n",
    "# Total number of frames\n",
    "total_frames = len(ball_data)\n",
    "\n",
    "# Iterate through each frame in your ball_data array\n",
    "for frame in ball_data:\n",
    "    # Check if there are no ball detections in the frame\n",
    "    if len(frame) == 0:\n",
    "        no_detection_count += 1\n",
    "\n",
    "# Calculate the percentage of frames with no ball detections\n",
    "no_detection_percentage = (no_detection_count / total_frames) * 100\n",
    "\n",
    "# Print or return the percentage\n",
    "print(f\"Percentage of frames with no ball detections: {no_detection_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c93da-266c-4ee0-8bc6-3727a0d6fa55",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "before_tracking_data=[]\n",
    "for i in range (len(all_data)):\n",
    "    frame = all_data[i]\n",
    "    objects=[]\n",
    "    for object in frame:\n",
    "        if object['score']>0.5:\n",
    "            objects.append(object)\n",
    "    for ball in ball_data[i]:\n",
    "        objects.append(ball)\n",
    "    before_tracking_data.append(objects) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0510d-d611-4e17-bd1f-e802ce1ab736",
   "metadata": {},
   "source": [
    "### getting ball data before tracking (old approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bbc3578-119a-40f4-bd22-5269ee57e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "ball_data=[]\n",
    "for frame in all_detection_data:\n",
    "    all=[]\n",
    "    ball=[]\n",
    "    for object in frame:\n",
    "        if object['class']=='ball':\n",
    "            ball.append(object)\n",
    "        else:\n",
    "            all.append(object)\n",
    "    all_data.append(all)\n",
    "    ball_data.append(ball)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5434ca9-9751-427f-9c92-863cc29563a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each frame in your ball_data array\n",
    "for i, frame in enumerate(ball_data):\n",
    "    # Check if there is more than one ball detection in the frame\n",
    "    if len(frame) > 1:\n",
    "        # Find the detection with the highest score\n",
    "        highest_score_detection = max(frame, key=lambda detection: detection['score'])\n",
    "        \n",
    "        # Keep only the detection with the highest score\n",
    "        ball_data[i] = [highest_score_detection]\n",
    "\n",
    "# Now, ball_data will contain only the ball with the highest score in frames with multiple detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a43b4a6-dc25-4f28-bc35-53ccdf105050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of frames with no ball detections: 56.714285714285715%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter for frames with no ball detections\n",
    "no_detection_count = 0\n",
    "\n",
    "# Total number of frames\n",
    "total_frames = len(ball_data)\n",
    "\n",
    "# Iterate through each frame in your ball_data array\n",
    "for frame in ball_data:\n",
    "    # Check if there are no ball detections in the frame\n",
    "    if len(frame) == 0:\n",
    "        no_detection_count += 1\n",
    "\n",
    "# Calculate the percentage of frames with no ball detections\n",
    "no_detection_percentage = (no_detection_count / total_frames) * 100\n",
    "\n",
    "# Print or return the percentage\n",
    "print(f\"Percentage of frames with no ball detections: {no_detection_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c2552a7-3dbf-4297-a6ea-c2d707f87ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video frame dimensions\n",
    "frame_width = vid_width\n",
    "frame_height = vid_height\n",
    "\n",
    "# Convert normalized box coordinates to absolute pixel values\n",
    "def convert_normalized_box(box, width, height):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return [xtl * width, ytl * height, xbr * width, ybr * height]\n",
    "\n",
    "\n",
    "# Adjust ball_data to use absolute pixel values\n",
    "ball_data_absolute = []\n",
    "for frame in ball_data:\n",
    "    balls=[]\n",
    "    for ball in frame:\n",
    "        normalized_box = ball['box']\n",
    "        absolute_box = convert_normalized_box(normalized_box, frame_width, frame_height)\n",
    "        balls.append({'score': ball['score'], 'class': ball['class'], 'box': absolute_box})\n",
    "    ball_data_absolute.append(balls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c24956-af44-4242-b89b-c5f985a06f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tracking_data=[]\n",
    "for i in range (len(all_data)):\n",
    "    frame = all_data[i]\n",
    "    objects=[]\n",
    "    for object in frame:\n",
    "        objects.append(object)\n",
    "    before_tracking_data.append(objects) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881415c-036a-4133-93b0-0bfdda7bd101",
   "metadata": {},
   "source": [
    "### Getting tracking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e72144dd-e043-4bbb-afbf-7c09544ce2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before_tracking_data=all_detection_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4e66c9-397e-44e4-9880-35cb5610d826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame1  frame2  frame3  frame4  frame5  frame6  frame7  frame8  frame9  frame10  frame11  frame12  frame13  frame14  frame15  frame16  frame17  frame18  frame19  frame20  frame21  frame22  frame23  frame24  frame25  frame26  frame27  frame28  frame29  frame30  frame31  frame32  frame33  frame34  frame35  frame36  frame37  frame38  frame39  frame40  frame41  frame42  frame43  frame44  frame45  frame46  frame47  frame48  frame49  frame50  frame51  frame52  frame53  frame54  frame55  frame56  frame57  frame58  frame59  frame60  frame61  frame62  frame63  frame64  frame65  frame66  frame67  frame68  frame69  frame70  frame71  frame72  frame73  frame74  frame75  frame76  frame77  frame78  frame79  frame80  frame81  frame82  frame83  frame84  frame85  frame86  frame87  frame88  frame89  frame90  frame91  frame92  frame93  frame94  frame95  frame96  frame97  frame98  frame99  frame100  frame101  frame102  frame103  frame104  frame105  frame106  frame107  frame108  frame109  frame110  frame111  frame112  frame113  frame114  frame115  frame116  frame117  frame118  frame119  frame120  frame121  frame122  frame123  frame124  frame125  frame126  frame127  frame128  frame129  frame130  frame131  frame132  frame133  frame134  frame135  frame136  frame137  frame138  frame139  frame140  frame141  frame142  frame143  frame144  frame145  frame146  frame147  frame148  frame149  frame150  frame151  frame152  frame153  frame154  frame155  frame156  frame157  frame158  frame159  frame160  frame161  frame162  frame163  frame164  frame165  frame166  frame167  frame168  frame169  frame170  frame171  frame172  frame173  frame174  frame175  frame176  frame177  frame178  frame179  frame180  frame181  frame182  frame183  frame184  frame185  frame186  frame187  frame188  frame189  frame190  frame191  frame192  frame193  frame194  frame195  frame196  frame197  frame198  frame199  frame200  frame201  frame202  frame203  frame204  frame205  frame206  frame207  frame208  frame209  frame210  frame211  frame212  frame213  frame214  frame215  frame216  frame217  frame218  frame219  frame220  frame221  frame222  frame223  frame224  frame225  frame226  frame227  frame228  frame229  frame230  frame231  frame232  frame233  frame234  frame235  frame236  frame237  frame238  frame239  frame240  frame241  frame242  frame243  frame244  frame245  frame246  frame247  frame248  frame249  frame250  frame251  frame252  frame253  frame254  frame255  frame256  frame257  frame258  frame259  frame260  frame261  frame262  frame263  frame264  frame265  frame266  frame267  frame268  frame269  frame270  frame271  frame272  frame273  frame274  frame275  frame276  frame277  frame278  frame279  frame280  frame281  frame282  frame283  frame284  frame285  frame286  frame287  frame288  frame289  frame290  frame291  frame292  frame293  frame294  frame295  frame296  frame297  frame298  frame299  frame300  frame301  frame302  frame303  frame304  frame305  frame306  frame307  frame308  frame309  frame310  frame311  frame312  frame313  frame314  frame315  frame316  frame317  frame318  frame319  frame320  frame321  frame322  frame323  frame324  frame325  frame326  frame327  frame328  frame329  frame330  frame331  frame332  frame333  frame334  frame335  frame336  frame337  frame338  frame339  frame340  frame341  frame342  frame343  frame344  frame345  frame346  frame347  frame348  frame349  frame350  frame351  frame352  frame353  frame354  frame355  frame356  frame357  frame358  frame359  frame360  frame361  frame362  frame363  frame364  frame365  frame366  frame367  frame368  frame369  frame370  frame371  frame372  frame373  frame374  frame375  frame376  frame377  frame378  frame379  frame380  frame381  frame382  frame383  frame384  frame385  frame386  frame387  frame388  frame389  frame390  frame391  frame392  frame393  frame394  frame395  frame396  frame397  frame398  frame399  frame400  frame401  frame402  frame403  frame404  frame405  frame406  frame407  frame408  frame409  frame410  frame411  frame412  frame413  frame414  frame415  frame416  frame417  frame418  frame419  frame420  frame421  frame422  frame423  frame424  frame425  frame426  frame427  frame428  frame429  frame430  frame431  frame432  frame433  frame434  frame435  frame436  frame437  frame438  frame439  frame440  frame441  frame442  frame443  frame444  frame445  frame446  frame447  frame448  frame449  frame450  frame451  frame452  frame453  frame454  frame455  frame456  frame457  frame458  frame459  frame460  frame461  frame462  frame463  frame464  frame465  frame466  frame467  frame468  frame469  frame470  frame471  frame472  frame473  frame474  frame475  frame476  frame477  frame478  frame479  frame480  frame481  frame482  frame483  frame484  frame485  frame486  frame487  frame488  frame489  frame490  frame491  frame492  frame493  frame494  frame495  frame496  frame497  frame498  frame499  frame500  frame501  frame502  frame503  frame504  frame505  frame506  frame507  frame508  frame509  frame510  frame511  frame512  frame513  frame514  frame515  frame516  frame517  frame518  frame519  frame520  frame521  frame522  frame523  frame524  frame525  frame526  frame527  frame528  frame529  frame530  frame531  frame532  frame533  frame534  frame535  frame536  frame537  frame538  frame539  frame540  frame541  frame542  frame543  frame544  frame545  frame546  frame547  frame548  frame549  frame550  frame551  frame552  frame553  frame554  frame555  frame556  frame557  frame558  frame559  frame560  frame561  frame562  frame563  frame564  frame565  frame566  frame567  frame568  frame569  frame570  frame571  frame572  frame573  frame574  frame575  frame576  frame577  frame578  frame579  frame580  frame581  frame582  frame583  frame584  frame585  frame586  frame587  frame588  frame589  frame590  frame591  frame592  frame593  frame594  frame595  frame596  frame597  frame598  frame599  frame600  frame601  frame602  frame603  frame604  frame605  frame606  frame607  frame608  frame609  frame610  frame611  frame612  frame613  frame614  frame615  frame616  frame617  frame618  frame619  frame620  frame621  frame622  frame623  frame624  frame625  frame626  frame627  frame628  frame629  frame630  frame631  frame632  frame633  frame634  frame635  frame636  frame637  frame638  frame639  frame640  frame641  frame642  frame643  frame644  frame645  frame646  frame647  frame648  frame649  frame650  frame651  frame652  frame653  frame654  frame655  frame656  frame657  frame658  frame659  frame660  frame661  frame662  frame663  frame664  frame665  frame666  frame667  frame668  frame669  frame670  frame671  frame672  frame673  frame674  frame675  frame676  frame677  frame678  frame679  frame680  frame681  frame682  frame683  frame684  frame685  frame686  frame687  frame688  frame689  frame690  frame691  frame692  frame693  frame694  frame695  frame696  frame697  frame698  frame699  frame700  Completed\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\X1.mp4\")\n",
    "all_frames_data = []\n",
    "current=0\n",
    "\n",
    "    \n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    boxes, scores, names= [] , [], []\n",
    "    for obj in before_tracking_data[current]:\n",
    "        boxes.append(obj['box'])\n",
    "        scores.append(obj['score'])\n",
    "        names.append(obj['class'])\n",
    "\n",
    "    boxes, scores, names= np.array(boxes), np.array(scores), np.array(names)\n",
    "    \n",
    "    converted_boxes = convert_boxes(img, boxes)\n",
    "    features = encoder(img, converted_boxes)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in\n",
    "                   zip(converted_boxes, scores, names, features)]\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "\n",
    "\n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "        \n",
    "        bbox = track.to_tlbr()\n",
    "        class_name= track.get_class()\n",
    "\n",
    "        object_data = {\n",
    "            \"box\": track.to_tlbr(),\n",
    "            \"id\": track.track_id,\n",
    "            \"class\": track.get_class()\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_frames_data.append(frame_objects)\n",
    "        \n",
    "    print(f\"frame{current+1} \", end=\" \")\n",
    "    current+=1\n",
    "\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44bb3a-63d4-432f-a3b2-04955df18a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert(obj):\n",
    "    \"\"\"\n",
    "    Convert objects that are not serializable by default.\n",
    "    This includes numpy arrays and numpy data types.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    raise TypeError(\"Object of type '%s' is not JSON serializable\" % type(obj).__name__)\n",
    "\n",
    "# Assuming all_detection_data is your array of arrays of dictionaries\n",
    "# ...\n",
    "\n",
    "# Write the array to a JSON file with custom serialization for numpy arrays and data types\n",
    "with open('all_frames_data_new_vid_1.json', 'w') as file:\n",
    "    json.dump(all_frames_data, file, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051df5c1-332e-4aef-afcd-82820a35ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the array from the JSON file\n",
    "with open('all_frames_data.json', 'r') as file:\n",
    "    all_frames_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef5a57-9721-4258-8ce0-7bc82bf4a593",
   "metadata": {},
   "source": [
    "# Converting tracking data to different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58ff97-fb62-4351-8a5b-98f14bc1cbeb",
   "metadata": {},
   "source": [
    "### Pascal VOC  XML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645a710-4d83-49fc-8634-cbe83aae5342",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "\n",
    "detections=all_frames_data\n",
    "# Create the 'cvat' folder and subdirectories if they don't exist\n",
    "cvat_folder = 'cvat2'\n",
    "image_sets_path = os.path.join(cvat_folder, 'ImageSets/Main')\n",
    "annotations_path = os.path.join(cvat_folder, 'Annotations')\n",
    "os.makedirs(image_sets_path, exist_ok=True)\n",
    "os.makedirs(annotations_path, exist_ok=True)\n",
    "\n",
    "# Create default.txt file\n",
    "with open(os.path.join(image_sets_path, 'default.txt'), 'w') as f:\n",
    "    for i in range(len(detections)):\n",
    "        f.write(f'frame_{i:06d}\\n')\n",
    "\n",
    "\n",
    "width=1920\n",
    "height=1080\n",
    "\n",
    "# Iterate through each frame and each object\n",
    "for frame_idx, frame in enumerate(detections):\n",
    "    annotation = ET.Element('annotation')\n",
    "    folder = ET.SubElement(annotation, 'folder')\n",
    "    folder.text = 'frame'\n",
    "    filename = ET.SubElement(annotation, 'filename')\n",
    "    filename.text = f'frame_{frame_idx:06d}.PNG'\n",
    "    source = ET.SubElement(annotation, 'source')\n",
    "    ET.SubElement(source, 'database').text = 'Unknown'\n",
    "    ET.SubElement(source, 'annotation').text = 'Unknown'\n",
    "    ET.SubElement(source, 'image').text = 'Unknown'\n",
    "    size = ET.SubElement(annotation, 'size')\n",
    "    ET.SubElement(size, 'width').text = str(width)  # Adjust according to your video resolution\n",
    "    ET.SubElement(size, 'height').text = str(height)  # Adjust according to your video resolution\n",
    "    ET.SubElement(size, 'depth').text = '3'  # For color images, depth is 3\n",
    "    ET.SubElement(annotation, 'segmented').text = '0'\n",
    "\n",
    "    for obj in frame:\n",
    "        object_elem = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(object_elem, 'name').text = obj['class']\n",
    "        ET.SubElement(object_elem, 'truncated').text = '0'\n",
    "        ET.SubElement(object_elem, 'occluded').text = '0'\n",
    "        ET.SubElement(object_elem, 'difficult').text = '0'\n",
    "        bndbox = ET.SubElement(object_elem, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(obj['box'][0])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(obj['box'][1])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(obj['box'][2])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(obj['box'][3])\n",
    "        \n",
    "        attributes = ET.SubElement(object_elem, 'attributes')\n",
    "\n",
    "        # Rotation attribute\n",
    "        rotation_attr = ET.SubElement(attributes, 'attribute')\n",
    "        ET.SubElement(rotation_attr, 'name').text = 'rotation'\n",
    "        ET.SubElement(rotation_attr, 'value').text = '0.0'\n",
    "\n",
    "        # Track ID attribute\n",
    "        track_id_attr = ET.SubElement(attributes, 'attribute')\n",
    "        ET.SubElement(track_id_attr, 'name').text = 'track_id'\n",
    "        ET.SubElement(track_id_attr, 'value').text = str(obj['id'])\n",
    "\n",
    "        # Keyframe attribute\n",
    "        keyframe_attr = ET.SubElement(attributes, 'attribute')\n",
    "        ET.SubElement(keyframe_attr, 'name').text = 'keyframe'\n",
    "        if frame_idx % 10 == 0:\n",
    "            ET.SubElement(keyframe_attr, 'value').text = 'True'\n",
    "        else:\n",
    "            ET.SubElement(keyframe_attr, 'value').text = 'False'\n",
    "    \n",
    "\n",
    "    # Write to XML file in the 'cvat' folder\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    tree.write(os.path.join(annotations_path, f'frame_{frame_idx:06d}.xml'))\n",
    "\n",
    "print(\"XML files generated successfully in the 'cvat2' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9bf78a-1876-4064-bfa3-558a7ba768e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file),\n",
    "                       os.path.relpath(os.path.join(root, file),\n",
    "                                       os.path.join(path, '..')))\n",
    "\n",
    "zip_file_name = 'cvat2.zip'\n",
    "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipdir('cvat/', zipf)\n",
    "\n",
    "print(f\"Created ZIP file: {zip_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd09ea-7159-4739-a140-df58c2fe5b28",
   "metadata": {},
   "source": [
    "### MOT 1.1 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de38c32-abc5-4905-9c07-de1c7ef81e4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Your tracking results\n",
    "tracking_results = all_frames_data\n",
    "\n",
    "# Label names\n",
    "labels = [\"player\", \"referee\", \"goalkeeper\", \"ball\"]\n",
    "\n",
    "# Create the 'gt' folder if it doesn't exist\n",
    "gt_folder = 'gt'\n",
    "os.makedirs(gt_folder, exist_ok=True)\n",
    "\n",
    "# Generate labels.txt and gt.txt inside the 'gt' folder\n",
    "with open(os.path.join(gt_folder, 'labels.txt'), 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write(f'{label}\\n')\n",
    "\n",
    "def xyxy_to_xywh(box):\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    return [round(x_min,1), round(y_min,1), round(x_max - x_min,1), round(y_max - y_min,1)]\n",
    "\n",
    "with open(os.path.join(gt_folder, 'gt.txt'), 'w') as f:\n",
    "    for frame_idx, frame in enumerate(tracking_results):\n",
    "        for obj in frame:\n",
    "            box = xyxy_to_xywh(obj['box'])\n",
    "            class_id = labels.index(obj['class']) + 1\n",
    "            line = f'{frame_idx + 1},{obj[\"id\"]},{box[0]},{box[1]},{box[2]},{box[3]},1,{class_id},1.0\\n'\n",
    "            f.write(line)\n",
    "\n",
    "# Zip the 'gt' folder\n",
    "zip_file_name = 'gt.zip'\n",
    "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(gt_folder):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), gt_folder))\n",
    "\n",
    "print(\"Created ZIP file: \" + zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc8794-84f2-4832-a0eb-f409f29e902b",
   "metadata": {},
   "source": [
    "### CVAT 1.1 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "efe757a4-a7c7-4fa5-b8eb-fb50a7b79ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations.xml file generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "def prettify(elem, level=0):\n",
    "    indent = \"\\n\" + level*\"  \"\n",
    "    if len(elem):\n",
    "        if not elem.text or not elem.text.strip():\n",
    "            elem.text = indent + \"  \"\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "        for elem in elem:\n",
    "            prettify(elem, level+1)\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "    else:\n",
    "        if level and (not elem.tail or not elem.tail.strip()):\n",
    "            elem.tail = indent\n",
    "\n",
    "# Example tracking data\n",
    "tracking_data = all_frames_data\n",
    "\n",
    "# Convert box coordinates from xyxy to xtl, ytl, xbr, ybr format\n",
    "def convert_box(box):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return {'xtl': xtl, 'ytl': ytl, 'xbr': xbr, 'ybr': ybr}\n",
    "\n",
    "# Create the root element\n",
    "annotations = ET.Element('annotations')\n",
    "ET.SubElement(annotations, 'version').text = '1.1'\n",
    "\n",
    "# Create tracks for each object\n",
    "tracks = {}\n",
    "for frame_data in tracking_data:\n",
    "    for obj in frame_data:\n",
    "        obj_id = obj['id']\n",
    "        if obj_id not in tracks:\n",
    "            track = ET.SubElement(annotations, 'track', id=str(obj_id), label=obj['class'], source='manual')\n",
    "            tracks[obj_id] = track\n",
    "\n",
    "# Process each frame for every object\n",
    "for frame_idx in range(len(tracking_data)):\n",
    "    current_frame_objects = {obj['id']: obj for obj in tracking_data[frame_idx]}\n",
    "    for obj_id, track in tracks.items():\n",
    "        if obj_id in current_frame_objects:\n",
    "            obj = current_frame_objects[obj_id]\n",
    "            box = convert_box(obj['box'])\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "        else:\n",
    "            # Object is not in the current frame\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='1', occluded='0', \n",
    "                          xtl='0', ytl='0', xbr='0', ybr='0', z_order='0')\n",
    "\n",
    "# Write to XML file\n",
    "prettify(annotations)\n",
    "tree = ET.ElementTree(annotations)\n",
    "tree.write('annotations_X1_0.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(\"annotations.xml file generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450ee17-9d69-46ff-94f4-307d9ac0fa77",
   "metadata": {},
   "source": [
    "# Seperating results by class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af7c13-7a9f-46ec-9700-aa82f7c44201",
   "metadata": {},
   "source": [
    "### getting ball data before tracking (old approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cc832-c45b-4fa9-87ee-bd3c3df0ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "ball_data=[]\n",
    "for frame in all_detection_data_filtered:\n",
    "    all=[]\n",
    "    ball=[]\n",
    "    for object in frame:\n",
    "        if object['class']=='ball':\n",
    "            ball.append(object)\n",
    "        else:\n",
    "            all.append(object)\n",
    "    all_data.append(all)\n",
    "    ball_data.append(ball)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a8ddc-15c2-4187-93e6-849ecefb9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each frame in your ball_data array\n",
    "for i, frame in enumerate(ball_data):\n",
    "    # Check if there is more than one ball detection in the frame\n",
    "    if len(frame) > 1:\n",
    "        # Find the detection with the highest score\n",
    "        highest_score_detection = max(frame, key=lambda detection: detection['score'])\n",
    "        \n",
    "        # Keep only the detection with the highest score\n",
    "        ball_data[i] = [highest_score_detection]\n",
    "\n",
    "# Now, ball_data will contain only the ball with the highest score in frames with multiple detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1ee8c-572c-4d58-ac41-6295f14400d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for frames with no ball detections\n",
    "no_detection_count = 0\n",
    "\n",
    "# Total number of frames\n",
    "total_frames = len(ball_data)\n",
    "\n",
    "# Iterate through each frame in your ball_data array\n",
    "for frame in ball_data:\n",
    "    # Check if there are no ball detections in the frame\n",
    "    if len(frame) == 0:\n",
    "        no_detection_count += 1\n",
    "\n",
    "# Calculate the percentage of frames with no ball detections\n",
    "no_detection_percentage = (no_detection_count / total_frames) * 100\n",
    "\n",
    "# Print or return the percentage\n",
    "print(f\"Percentage of frames with no ball detections: {no_detection_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96758a0a-9f79-4139-b3e6-ed69cfa5199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video frame dimensions\n",
    "frame_width = 1920\n",
    "frame_height = 1080\n",
    "\n",
    "# Convert normalized box coordinates to absolute pixel values\n",
    "def convert_normalized_box(box, width, height):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return [xtl * width, ytl * height, xbr * width, ybr * height]\n",
    "\n",
    "\n",
    "# Adjust ball_data to use absolute pixel values\n",
    "ball_data_absolute = []\n",
    "for frame in ball_data:\n",
    "    if frame:\n",
    "        ball = frame[0]\n",
    "        normalized_box = ball['box']\n",
    "        absolute_box = convert_normalized_box(normalized_box, frame_width, frame_height)\n",
    "        ball_data_absolute.append([{'score': ball['score'], 'class': ball['class'], 'box': absolute_box}])\n",
    "    else:\n",
    "        ball_data_absolute.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8cf26-3fba-49dd-8218-21ee84b55c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before_tracking_data=[]\n",
    "# for i in range (len(all_data)):\n",
    "#     frame = all_data[i]\n",
    "#     objects=[]\n",
    "#     for object in frame:\n",
    "#         if object['score']>0.5:\n",
    "#             objects.append(object)\n",
    "#     before_tracking_data.append(objects) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29833cf-348b-4f09-9416-477d767d39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking_data_without_balls = [[obj for obj in frame if obj['class'] != 'ball'] for frame in all_frames_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc1214-59fa-4310-a181-8ed26d09ddf9",
   "metadata": {},
   "source": [
    "### Converting the seperated results to CVAT format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27659b57-215b-4ed2-90b3-8ce4e25cb298",
   "metadata": {},
   "source": [
    "##### Old approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a91090-588c-46ef-b68e-52e3689ef3d6",
   "metadata": {},
   "source": [
    "ball_data_abosulte : ball detection results without tracking (not normalized)\n",
    "\n",
    "all_frames_data: tracking results without ball (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71593918-d6f1-4429-b7b9-5d74c4f18eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_data=[]\n",
    "for frame in ball_data_absolute:\n",
    "    objects=[]\n",
    "    for object in frame:\n",
    "        if object['score']>0.3:\n",
    "            objects.append(object)\n",
    "    ball_data.append(objects) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3f10af-a782-4bd4-a973-219d9ee3b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations.xml file generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tracking_data = all_frames_data\n",
    "\n",
    "def prettify(elem, level=0):\n",
    "    indent = \"\\n\" + level*\"  \"\n",
    "    if len(elem):\n",
    "        if not elem.text or not elem.text.strip():\n",
    "            elem.text = indent + \"  \"\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "        for elem in elem:\n",
    "            prettify(elem, level+1)\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "    else:\n",
    "        if level and (not elem.tail or not elem.tail.strip()):\n",
    "            elem.tail = indent\n",
    "\n",
    "def convert_box(box):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return {'xtl': xtl, 'ytl': ytl, 'xbr': xbr, 'ybr': ybr}\n",
    "\n",
    "annotations = ET.Element('annotations')\n",
    "ET.SubElement(annotations, 'version').text = '1.1'\n",
    "\n",
    "# Create tracks for each object in tracking_data\n",
    "tracks = {}\n",
    "for frame_data in tracking_data:\n",
    "    for obj in frame_data:\n",
    "        obj_id = obj['id']\n",
    "        if obj_id not in tracks:\n",
    "            track = ET.SubElement(annotations, 'track', id=str(obj_id), label=obj['class'], source='manual')\n",
    "            tracks[obj_id] = track\n",
    "\n",
    "# Add track for the ball\n",
    "ball_track_id = max(tracks.keys(), default=0) + 1\n",
    "ball_track = ET.SubElement(annotations, 'track', id=str(ball_track_id), label='ball', source='manual')\n",
    "last_ball_box = None\n",
    "\n",
    "# Process each frame for every object\n",
    "for frame_idx in range(len(tracking_data)-1):\n",
    "    current_frame_objects = {obj['id']: obj for obj in tracking_data[frame_idx]}\n",
    "    for obj_id, track in tracks.items():\n",
    "        if obj_id in current_frame_objects:\n",
    "            obj = current_frame_objects[obj_id]\n",
    "            box = convert_box(obj['box'])\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "        else:\n",
    "            # Object is not in the current frame\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='1', occluded='0', \n",
    "                          xtl='0', ytl='0', xbr='0', ybr='0', z_order='0')\n",
    "    \n",
    "    # Process ball\n",
    "    ball_in_frame = ball_data[frame_idx]\n",
    "    if ball_in_frame:\n",
    "        last_ball_box = convert_box(ball_in_frame[0]['box'])\n",
    "        ET.SubElement(ball_track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                      xtl=str(last_ball_box['xtl']), ytl=str(last_ball_box['ytl']), xbr=str(last_ball_box['xbr']), ybr=str(last_ball_box['ybr']), z_order='0')\n",
    "    elif last_ball_box:\n",
    "        ET.SubElement(ball_track, 'box', frame=str(frame_idx), keyframe='0', outside='0', occluded='0', \n",
    "                      xtl=str(last_ball_box['xtl']), ytl=str(last_ball_box['ytl']), xbr=str(last_ball_box['xbr']), ybr=str(last_ball_box['ybr']), z_order='0')\n",
    "\n",
    "# Write to XML file\n",
    "prettify(annotations)\n",
    "tree = ET.ElementTree(annotations)\n",
    "tree.write('annotations_X1_0.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(\"annotations.xml file generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f23f6-142d-4e24-9302-5b18f7836e47",
   "metadata": {},
   "source": [
    "##### New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b6b0f-b308-487f-ac0e-d81ed620c457",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tracking_data = all_frames_data\n",
    "\n",
    "def prettify(elem, level=0):\n",
    "    indent = \"\\n\" + level*\"  \"\n",
    "    if len(elem):\n",
    "        if not elem.text or not elem.text.strip():\n",
    "            elem.text = indent + \"  \"\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "        for elem in elem:\n",
    "            prettify(elem, level+1)\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "    else:\n",
    "        if level and (not elem.tail or not elem.tail.strip()):\n",
    "            elem.tail = indent\n",
    "\n",
    "def convert_box(box):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return {'xtl': xtl, 'ytl': ytl, 'xbr': xbr, 'ybr': ybr}\n",
    "\n",
    "annotations = ET.Element('annotations')\n",
    "ET.SubElement(annotations, 'version').text = '1.1'\n",
    "\n",
    "# Create tracks for each object\n",
    "tracks = {}\n",
    "last_seen_boxes = {}\n",
    "for frame_data in tracking_data:\n",
    "    for obj in frame_data:\n",
    "        obj_id = obj['id']\n",
    "        if obj_id not in tracks:\n",
    "            track = ET.SubElement(annotations, 'track', id=str(obj_id), label=obj['class'], source='manual')\n",
    "            tracks[obj_id] = track\n",
    "        last_seen_boxes[obj_id] = convert_box(obj['box'])\n",
    "\n",
    "# Process each frame for every object\n",
    "for frame_idx, frame_data in enumerate(tracking_data):\n",
    "    current_frame_objects = {obj['id']: obj for obj in frame_data}\n",
    "    for obj_id, track in tracks.items():\n",
    "        if obj_id in current_frame_objects:\n",
    "            obj = current_frame_objects[obj_id]\n",
    "            box = convert_box(obj['box'])\n",
    "            last_seen_boxes[obj_id] = box\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "        elif tracks[obj_id].attrib['label'] == 'ball' and last_seen_boxes[obj_id] is not None:\n",
    "            # For balls, if absent in a frame, use the last known box coordinates\n",
    "            box = last_seen_boxes[obj_id]\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='0', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "# Write to XML file\n",
    "prettify(annotations)\n",
    "tree = ET.ElementTree(annotations)\n",
    "tree.write('annotations.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(\"annotations.xml file generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd600d41-6f23-47d5-8b01-cee5190120c1",
   "metadata": {},
   "source": [
    "# Exporting annotations from CVAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39408fea-5480-4594-bb00-5b3f4f411630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('annotations 4.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Initialize arrays for results\n",
    "ball_results = []\n",
    "player_results = []\n",
    "other_results = []\n",
    "\n",
    "# Function to convert box attributes to a dictionary\n",
    "def get_box_dict(box_element):\n",
    "    return [\n",
    "        float(box_element.get('xtl')),\n",
    "        float(box_element.get('ytl')),\n",
    "        float(box_element.get('xbr')),\n",
    "        float(box_element.get('ybr'))]\n",
    "\n",
    "# Process each track (object)\n",
    "for track in root.findall('track'):\n",
    "    label = track.get('label')\n",
    "    obj_id = int(track.get('id'))\n",
    "\n",
    "    # Dictionary to store boxes for each frame\n",
    "    frames = {}\n",
    "\n",
    "    # Process each box (appearance in a frame)\n",
    "    for box in track.findall('box'):\n",
    "        frame_num = int(box.get('frame'))\n",
    "        outside = box.get('outside') == '1'\n",
    "\n",
    "        if not outside:\n",
    "            box_dict = get_box_dict(box)\n",
    "            frames[frame_num] = {'box': box_dict, 'class': label, 'id': obj_id}\n",
    "\n",
    "    # Sort frames and add to the respective result array\n",
    "    sorted_frames = sorted(frames.items())\n",
    "    for frame_num, frame_data in sorted_frames:\n",
    "        while len(ball_results) <= frame_num:\n",
    "            ball_results.append([])\n",
    "        while len(player_results) <= frame_num:\n",
    "            player_results.append([])\n",
    "        while len(other_results) <= frame_num:\n",
    "            other_results.append([])\n",
    "\n",
    "        if label == 'ball':\n",
    "            ball_results[frame_num].append(frame_data)\n",
    "        elif label == 'player':\n",
    "            player_results[frame_num].append(frame_data)\n",
    "        elif label in ['goalkeeper', 'referee']:\n",
    "            other_results[frame_num].append(frame_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe0c93-7eaf-470a-a27e-c51e9ac5e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white.mp4\")\n",
    "    \n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white_test1.mp4\", codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "color_dict = {  \n",
    "\"referee\": (0, 0, 0), \n",
    "\"goalkeeper\":  (255, 0, 0), \n",
    "\"player\": (0, 255, 255), \n",
    "\"ball\": (0, 0, 255)  \n",
    "}\n",
    "\n",
    "for i in range(len(player_results)):\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in player_results[i]:\n",
    "        class_name = obj['class']\n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "\n",
    "        cv2.putText(img,'id' + str(obj['id']), (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "\n",
    "    for obj in other_results[i]:\n",
    "        class_name = obj['class']\n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "    \n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "    \n",
    "    for obj in ball_results[i]:\n",
    "        class_name = obj['class']\n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "    \n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "    \n",
    "        out.write(img)\n",
    "\n",
    "    \n",
    "            \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585081ed-ab3e-4bb0-b1d4-68149d85bb83",
   "metadata": {},
   "source": [
    "# Processing and plotting of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0711a7-32c0-49f7-8566-94489539352d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ball_results=ball_data_absolute.copy()\n",
    "# player_results=[]\n",
    "# other_results=[]\n",
    "# for frame in all_frames_data:\n",
    "#     players=[]\n",
    "#     others=[]\n",
    "#     for obj in frame:\n",
    "#         if obj['class']=='player':\n",
    "#             players.append(obj)\n",
    "#         else:\n",
    "#             others.append(obj)\n",
    "#     player_results.append(players)\n",
    "#     other_results.append(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90896ee2-8df5-468a-ae6d-a3eaeca2aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "vid = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white.mp4\")\n",
    "\n",
    "current=1\n",
    "\n",
    "for frame in player_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:\n",
    "        xmin, ymin, xmax, ymax = obj['box']\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        crop_img = img[ymin:ymax, xmin:xmax]\n",
    "        if crop_img.size > 0:\n",
    "             # if red(crop_img)>0.01 or yellow(crop_img)>0.01:\n",
    "             #     if red(crop_img)>yellow(crop_img):\n",
    "             #        obj['class']='team1'\n",
    "             #     else:\n",
    "             #        obj['class']='team2'\n",
    "            team_color_v2(img,frame)\n",
    "    print(f'frame{current} ', end=\" \")\n",
    "    current+=1\n",
    "vid.release()\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "# Print the execution time in minutes\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b649278-9ae7-40a4-b237-e609377544fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white.mp4\")\n",
    "    \n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white_test1.mp4\", codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "color_dict = {\n",
    "\"team 1\": (0, 0, 255),    \n",
    "\"team 2\": (255, 255, 255),    \n",
    "\"referee\": (255,255,0), \n",
    "\"goalkeeper\": (255, 0, 0), \n",
    "\"ball\": (255, 255, 255)  \n",
    "}\n",
    "\n",
    "for i in range(len(player_results)):\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in player_results[i]:\n",
    "        class_name = obj['class']\n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "\n",
    "        cv2.putText(img,'id' + str(obj['id']), (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "\n",
    "    for obj in other_results[i]:\n",
    "        class_name = obj['class']\n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "    \n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "    \n",
    "    for obj in ball_results[i]:\n",
    "        class_name = obj['class']\n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "    \n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "    \n",
    "        out.write(img)\n",
    "\n",
    "    \n",
    "            \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789477b-e503-4c86-9e5a-6bd6a8eb023b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert(obj):\n",
    "    \"\"\"\n",
    "    Convert objects that are not serializable by default.\n",
    "    This includes numpy arrays and numpy data types.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    raise TypeError(\"Object of type '%s' is not JSON serializable\" % type(obj).__name__)\n",
    "\n",
    "# Assuming all_detection_data is your array of arrays of dictionaries\n",
    "# ...\n",
    "\n",
    "# Write the array to a JSON file with custom serialization for numpy arrays and data types\n",
    "with open('player_results_5min.json', 'w') as file:\n",
    "    json.dump(player_results, file, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdaaf4-a6fb-4c1e-b72d-f2f39ff491b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the array from the JSON file\n",
    "with open('player_results_5min.json', 'r') as file:\n",
    "    player_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4c864-92ce-4653-95b1-615d2c7ed8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_per_id = defaultdict(list)\n",
    "\n",
    "for frame in player_results:  # Iterate over each frame\n",
    "    for obj in frame:  # Iterate over each detection in the frame\n",
    "        player_id = obj['id']\n",
    "        label = obj['class']\n",
    "        labels_per_id[player_id].append(label)\n",
    "\n",
    "# Dictionary to store the most common label for each ID\n",
    "most_common_label_per_id = {}\n",
    "\n",
    "for player_id, labels in labels_per_id.items():\n",
    "    # Determine the most common label\n",
    "    most_common_label = Counter(labels).most_common(1)[0][0]\n",
    "    most_common_label_per_id[player_id] = most_common_label\n",
    "\n",
    "for frame in player_results:\n",
    "    for obj in frame:\n",
    "        id = obj['id']\n",
    "        # Assign the most common label to this detection\n",
    "        obj['class'] = most_common_label_per_id[id]\n",
    "\n",
    "\n",
    "new_id_mapping = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for old_id, label in most_common_label_per_id.items():\n",
    "    # Increment the counter for the label and assign the new ID\n",
    "    new_id_mapping[label]['counter'] += 1\n",
    "    new_id_mapping[label][old_id] = new_id_mapping[label]['counter']\n",
    "\n",
    "def get_new_id(old_id, label):\n",
    "    \"\"\"\n",
    "    Given an old ID and its label, returns a new ID that is unique within the label category.\n",
    "    \"\"\"\n",
    "    return new_id_mapping[label][old_id]\n",
    "\n",
    "for frame in player_results:\n",
    "    for obj in frame:\n",
    "        obj['id']=get_new_id(obj['id'],obj['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc218969-1430-49ec-8701-9c010cd60865",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('a9f16c_8.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59479d3-f200-4243-9e16-62ca082926a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_false_detections_with_fps(ball_results, vid_fps, max_speed=500):\n",
    "    def get_centroid(box):\n",
    "        return ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
    "\n",
    "    def calculate_speed(box1, box2, frame_diff):\n",
    "        centroid1 = get_centroid(box1)\n",
    "        centroid2 = get_centroid(box2)\n",
    "        distance = ((centroid2[0] - centroid1[0])**2 + (centroid2[1] - centroid1[1])**2)**0.5\n",
    "        time_diff = frame_diff / vid_fps\n",
    "        return distance / time_diff\n",
    "\n",
    "    # Previous frame where the ball was detected\n",
    "    prev_frame = -1\n",
    "    prev_box = None\n",
    "\n",
    "    for i, frame_result in enumerate(ball_results):\n",
    "        if frame_result:  # Ball is detected in this frame\n",
    "            if prev_frame != -1:\n",
    "                # Calculate the speed between this detection and the previous detection\n",
    "                speed = calculate_speed(prev_box, frame_result[0]['box'], i - prev_frame)\n",
    "                if speed > max_speed:\n",
    "                    # If the speed exceeds the threshold, assume it's a false detection\n",
    "                    ball_results[i] = []\n",
    "                else:\n",
    "                    # Update the previous detection\n",
    "                    prev_frame = i\n",
    "                    prev_box = frame_result[0]['box']\n",
    "            else:\n",
    "                # First detection\n",
    "                prev_frame = i\n",
    "                prev_box = frame_result[0]['box']\n",
    "\n",
    "    return ball_results\n",
    "\n",
    "ball_results = remove_false_detections_with_fps(ball_results, vid_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3aa2f8-95c0-4d5d-8936-7b9e5fa7e910",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_false_detections_with_fps(ball_results, vid_fps, max_speed=500):\n",
    "    def get_centroid(box):\n",
    "        return ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
    "\n",
    "    def calculate_speed(box1, box2, frame_diff):\n",
    "        centroid1 = get_centroid(box1)\n",
    "        centroid2 = get_centroid(box2)\n",
    "        distance = ((centroid2[0] - centroid1[0])**2 + (centroid2[1] - centroid1[1])**2)**0.5\n",
    "        time_diff = frame_diff / vid_fps\n",
    "        return distance / time_diff\n",
    "\n",
    "    prev_frame = -1\n",
    "    prev_box = None\n",
    "\n",
    "    for i, detections in enumerate(ball_results):\n",
    "        if detections:  # If there are detections in this frame\n",
    "            valid_detections = []\n",
    "            speeds = []\n",
    "\n",
    "            if prev_box is not None:\n",
    "                for detection in detections:\n",
    "                        # Calculate speed for each detection relative to the last known position\n",
    "                        speed = calculate_speed(prev_box, detection['box'], i - prev_frame)\n",
    "                        if speed <= max_speed:\n",
    "                            valid_detections.append(detection)\n",
    "                            speeds.append(speed)\n",
    "    \n",
    "                if valid_detections:\n",
    "                    # Select the detection with the minimum speed deviation from the last known position\n",
    "                    min_speed_index = speeds.index(min(speeds))\n",
    "                    selected_detection = valid_detections[min_speed_index]\n",
    "    \n",
    "                    # Update for the next iteration\n",
    "                    ball_results[i] = [selected_detection]\n",
    "                    prev_frame = i\n",
    "                    prev_box = selected_detection['box']\n",
    "                else:\n",
    "                    # No valid detections, mark the frame as having no ball detections\n",
    "                    ball_results[i] = []\n",
    "            else:\n",
    "                prev_frame = i\n",
    "                prev_box = detections[0]['box']\n",
    "        else:\n",
    "            # No detections in this frame, continue\n",
    "            continue\n",
    "\n",
    "    return ball_results\n",
    "\n",
    "ball_results = remove_false_detections_with_fps(ball_results, vid_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cbe52-dfdc-486e-99e7-1d35ea310562",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a counter for frames with no ball detections\n",
    "no_detection_count = 0\n",
    "\n",
    "# Total number of frames\n",
    "total_frames = len(ball_results)\n",
    "\n",
    "# Iterate through each frame in your ball_data array\n",
    "for frame in ball_results:\n",
    "    # Check if there are no ball detections in the frame\n",
    "    if len(frame) == 0:\n",
    "        no_detection_count += 1\n",
    "\n",
    "# Calculate the percentage of frames with no ball detections\n",
    "no_detection_percentage = (no_detection_count / total_frames) * 100\n",
    "\n",
    "# Print or return the percentage\n",
    "print(f\"Percentage of frames with no ball detections: {no_detection_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f4418-61a4-4eca-a603-3effb3ced6bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def interpolate_bounding_boxes(ball_results):\n",
    "    # Initialize a list to hold interpolated results\n",
    "    interpolated_results = []\n",
    "\n",
    "    # Previous frame where the ball was detected\n",
    "    prev_frame = -1\n",
    "    prev_box = None\n",
    "\n",
    "    for i, frame_result in enumerate(ball_results):\n",
    "        if frame_result:  # Ball is detected in this frame\n",
    "            interpolated_results.append(frame_result[0]['box'])\n",
    "            prev_frame = i\n",
    "            prev_box = frame_result[0]['box']\n",
    "        else:  # Ball is not detected in this frame\n",
    "            # Find the next frame where the ball is detected\n",
    "            next_frame = -1\n",
    "            next_box = None\n",
    "            for j in range(i + 1, len(ball_results)):\n",
    "                if ball_results[j]:\n",
    "                    next_frame = j\n",
    "                    next_box = ball_results[j][0]['box']\n",
    "                    break\n",
    "\n",
    "            if prev_frame != -1 and next_frame != -1:\n",
    "                # Interpolate the bounding box for the current frame\n",
    "                interpolated_box = [\n",
    "                    prev_box[k] + (next_box[k] - prev_box[k]) * ((i - prev_frame) / (next_frame - prev_frame))\n",
    "                    for k in range(4)\n",
    "                ]\n",
    "                interpolated_results.append(interpolated_box)\n",
    "            else:\n",
    "                # If there's no previous or next detection, we can't interpolate\n",
    "                interpolated_results.append(None)\n",
    "\n",
    "    return interpolated_results\n",
    "\n",
    "interpolated_results= interpolate_bounding_boxes(ball_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29b5f0-c7ce-429f-94f3-918e55ce8f56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('a9f16c_8.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-new_vid_2.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "\n",
    "for frame in interpolated_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break  \n",
    "    bbox=frame\n",
    "    xmin = bbox[0]\n",
    "    ymin = bbox[1]\n",
    "    xmax = bbox[2]\n",
    "    ymax = bbox[3]\n",
    "    \n",
    "    cv2.rectangle(img, (int(xmin),int(ymin)), (int(xmax),int(ymax)),(255, 255, 255), 2)\n",
    "\n",
    "\n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8614fc9-f16a-4be7-bb1c-3fec76d66ec0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "time_stops=[(242 , 401, 'ball out'),\n",
    "(535 , 763, 'ball out'),\n",
    "(4218 , 4852, 'foul'),\n",
    "(5076 , 5995, 'ball out'),\n",
    "(6850 , 7007, 'ball out'),\n",
    "(7974 , 8384, 'ball out')]\n",
    "\n",
    "def is_frame_in_time_stops(frame_number):\n",
    "    for start, end, text in time_stops:\n",
    "        if start <= frame_number <= end:\n",
    "            return True, text\n",
    "    return False, None\n",
    "\n",
    "color_dict = {\n",
    "\"team 1\": (0, 0, 255),    \n",
    "\"team 2\": (255, 255, 255),    \n",
    "\"referee\": (255,255,0), \n",
    "\"goalkeeper\": (255, 0, 0), \n",
    "\"player\": (255, 255, 0), \n",
    "\"ball\": (0, 255, 255)  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2bb4c-b828-4047-9c44-68ca2240c568",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "vid = cv2.VideoCapture('./data/video/5min.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-5min-2.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "\n",
    "for frame in player_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    ok , text= is_frame_in_time_stops(current)\n",
    "    if ok:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "                    \n",
    "        font_scale = 10 \n",
    "        thickness = 20 \n",
    "        text_size = cv2.getTextSize(text, 0, font_scale, thickness)[0]\n",
    "\n",
    "\n",
    "        text_x = (vid_width - text_size[0]) // 2\n",
    "        text_y = (vid_height + text_size[1]) // 2\n",
    "        position = (text_x, text_y) \n",
    "\n",
    "        font_color = (255, 255, 255, 0.5)  # White color with transparency\n",
    "\n",
    "        \n",
    "        # Overlay the text\n",
    "        cv2.putText(img, text, position, 0, font_scale, font_color, thickness)\n",
    "\n",
    "    else:\n",
    "        for obj in frame:     \n",
    "            if obj['class']!='':\n",
    "                class_name = obj['class']\n",
    "        \n",
    "                bbox=obj['box']\n",
    "                cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "        \n",
    "                cv2.putText(img,'id' + str(obj['id']), (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                            color_dict[obj['class']], 2)\n",
    "                \n",
    "        for obj in ball_results[current]:   \n",
    "            bbox=obj['box']\n",
    "            cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "            cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.5,\n",
    "                        color_dict[obj['class']], 1)\n",
    "    \n",
    "        # bbox=interpolated_results[current]\n",
    "        # if bbox!=None:\n",
    "        #     cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict['ball'], 2)\n",
    "        #     cv2.putText(img, 'ball', (int(bbox[0]), int(bbox[1]-10)), 0, 0.5,\n",
    "        #                 color_dict['ball'], 1)\n",
    "        \n",
    "        for obj in other_results[current]:   \n",
    "            bbox=obj['box']\n",
    "            cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "            cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.5,\n",
    "                        color_dict[obj['class']], 1)\n",
    "    \n",
    "    \n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "# Print the execution time in minutes\n",
    "print()\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db5847-0e92-4cab-8b5b-91726388a936",
   "metadata": {},
   "source": [
    "### Player identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbcb9e-b245-49b8-bb93-14eefbcf17c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "team1 = {}\n",
    "team2 = {}\n",
    "\n",
    "for frame_number, frame in enumerate(player_results):\n",
    "    for player in frame:\n",
    "        team = team1 if player['class'] == 'team 1' else team2\n",
    "        player_id = player['id']\n",
    "        \n",
    "        if player_id not in team:\n",
    "            team[player_id] = []\n",
    "        \n",
    "        team[player_id].append({'frame': frame_number, 'box': player['box']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1c69d-0554-45aa-bc57-e388edc0e7e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "\n",
    "frames=[]\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()  \n",
    "    if not ret:\n",
    "        break  # If no frame is read, end of video is reached\n",
    "    frames.append(frame)\n",
    "\n",
    "\n",
    "vid.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa447862-9a1f-494f-84d8-ecaf43f8db86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def select_frames(player_info, num_frames=6):\n",
    "    \"\"\"\n",
    "    Select up to num_frames non-consecutive frames for a player.\n",
    "    \"\"\"\n",
    "    selected_frames = []\n",
    "    total_frames = len(player_info)\n",
    "    \n",
    "    if total_frames <= num_frames:\n",
    "        return player_info\n",
    "    else:\n",
    "        step = total_frames // num_frames\n",
    "        for i in range(0, total_frames, step):\n",
    "            selected_frames.append(player_info[i])\n",
    "            if len(selected_frames) == num_frames:\n",
    "                break\n",
    "    return selected_frames\n",
    "\n",
    "def crop_and_show(frames, player_info):\n",
    "    \"\"\"\n",
    "    Crop images for a player, concatenate them horizontally, and display.\n",
    "    \"\"\"\n",
    "    cropped_images = []\n",
    "    for info in player_info:\n",
    "        frame = frames[info['frame']]\n",
    "        box = info['box']\n",
    "        cropped = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "        cropped_images.append(cropped)\n",
    "    \n",
    "    # Ensure all images have the same height for a uniform display\n",
    "    max_height = max(img.shape[0] for img in cropped_images)\n",
    "    resized_images = [cv2.resize(img, (int(img.shape[1] * max_height / img.shape[0]), max_height)) for img in cropped_images]\n",
    "    \n",
    "    # If you have less than 6 images, you can append blank images to fill the gaps\n",
    "    while len(resized_images) < 6:\n",
    "        blank_image = np.zeros((max_height, max_height, 3), dtype=np.uint8)\n",
    "        resized_images.append(blank_image)\n",
    "    \n",
    "    # Concatenate images horizontally\n",
    "    concatenated_image = np.hstack(resized_images)\n",
    "    \n",
    "    # Display the concatenated image\n",
    "    cv2.imshow('Player Images', concatenated_image)\n",
    "    cv2.waitKey(0)  # Wait for a key press to continue\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def crop_and_show_inline(frames, player_info):\n",
    "    \"\"\"\n",
    "    Crop images for a player, then display them inline in the notebook.\n",
    "    \"\"\"\n",
    "    cropped_images = []\n",
    "    for info in player_info:\n",
    "        frame = frames[info['frame']]\n",
    "        box = info['box']\n",
    "        cropped = frame[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "        cropped_images.append(cropped)\n",
    "    \n",
    "    # Ensure all images have the same height for a uniform display\n",
    "    max_height = max(img.shape[0] for img in cropped_images)\n",
    "    resized_images = [cv2.resize(img, (int(img.shape[1] * max_height / img.shape[0]), max_height)) for img in cropped_images]\n",
    "    \n",
    "    # If you have less than 6 images, you can append blank images to fill the gaps\n",
    "    while len(resized_images) < 6:\n",
    "        blank_image = np.zeros((max_height, max_height, 3), dtype=np.uint8)\n",
    "        resized_images.append(blank_image)\n",
    "    \n",
    "    # Plotting all the images in a row\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(20, 10))\n",
    "    for ax, img in zip(axes, resized_images):\n",
    "        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for correct plotting\n",
    "        ax.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "def collect_player_data(team1, team2, frames):\n",
    "    player_data = {}\n",
    "    for team in [team1, team2]:\n",
    "        for player_id, occurrences in team.items():\n",
    "            print(f\"Player ID: {player_id}\")\n",
    "            selected_occurrences = select_frames(occurrences)\n",
    "            crop_and_show_inline(frames, selected_occurrences)\n",
    "            name = input(\"Enter player's name: \")\n",
    "            jersey_number = input(\"Enter player's jersey number: \")\n",
    "            player_data[player_id] = {'name': name, 'jersey_number': jersey_number}\n",
    "    return player_data\n",
    "\n",
    "player_data = collect_player_data(team1, team2, frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979423f8-726d-48d8-bb42-6414de224849",
   "metadata": {},
   "source": [
    "### results with circles and ball posession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914777a-90bb-427d-a216-32dd25aeb276",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to draw an ellipse under detected objects\n",
    "def draw_ellipse(image, box, color, thickness=4):\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    bottom_center = (int((x_min + x_max) / 2), int(y_max))\n",
    "    axes = (int((x_max - x_min) / 2), int(0.35 * (x_max - x_min)))\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, -45, 235, color, thickness, cv2.LINE_4)\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, 0, 360, color, thickness)\n",
    "    cv2.ellipse(\n",
    "        image,\n",
    "        center=bottom_center,\n",
    "        axes=axes,\n",
    "        angle=0.0,\n",
    "        startAngle=-45,\n",
    "        endAngle=235,\n",
    "        color=color,\n",
    "        thickness=thickness,\n",
    "        lineType=cv2.LINE_4\n",
    "    )\n",
    "# Color definitions for different objects\n",
    "COLORS = {\n",
    "    \"team1\": (0, 0, 255),    # Red color for team1 (BGR format)\n",
    "    \"team2\": (255, 255, 0),    # light blue\n",
    "    \"referee\": (0, 255, 255) ,     # Yellow\n",
    "    \"goalkeeper\": (255, 0, 255), \n",
    "    \"player\": (255, 0, 255), \n",
    "    \"ball\": (0, 255, 255)  # White color for ball\n",
    "}\n",
    "THICKNESS = 4\n",
    "\n",
    "for object in frame:\n",
    "    box = object[\"box\"]\n",
    "    class_name = object[\"class\"]\n",
    "    color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "    draw_ellipse(img, box, color, THICKNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e50b8-c4f1-4116-b984-b044651d943b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PLAYER_IN_POSSESSION_PROXIMITY = 30\n",
    "PLAYER_MARKER_COLOR = (255, 0, 255)  # Green in BGR\n",
    "BALL_MARKER_COLOR = (255,0, 0)  # Blue in BGR\n",
    "MARKER_CONTOUR_COLOR = (0, 0, 0)  # Black in BGR\n",
    "MARKER_CONTOUR_THICKNESS = 1\n",
    "MARKER_WIDTH = 20\n",
    "MARKER_HEIGHT = 20 \n",
    "def calculate_marker(anchor,MARKER_MARGIN = 20):\n",
    "    x, y = anchor\n",
    "    return np.array([\n",
    "        [x - MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN],\n",
    "        [x, y - MARKER_MARGIN],\n",
    "        [x + MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN]\n",
    "    ], np.int32)\n",
    "\n",
    "def draw_marker(image, anchor, color, MARKER_MARGIN = 20):\n",
    "    marker_contour = calculate_marker(anchor,MARKER_MARGIN)\n",
    "    cv2.fillPoly(image, [marker_contour], color)\n",
    "    cv2.polylines(image, [marker_contour], isClosed=True, color=MARKER_CONTOUR_COLOR, thickness=MARKER_CONTOUR_THICKNESS)\n",
    "\n",
    "def get_player_in_possession(players, ball):\n",
    "    if len(ball) != 1:\n",
    "        return None\n",
    "    ball_x, ball_y = ball[0]['center']\n",
    "    for player in players:\n",
    "        player_x, player_y = player['center']\n",
    "        if abs(player_x - ball_x) <= PLAYER_IN_POSSESSION_PROXIMITY and abs(player_y - ball_y) <= PLAYER_IN_POSSESSION_PROXIMITY:\n",
    "            return player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039503e1-3dfc-42fa-b5d4-d4f518c39741",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_green.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_green7.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "color_dict=COLORS\n",
    "\n",
    "for frame in player_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:\n",
    "        x_min, y_min, x_max, y_max = obj['box']\n",
    "        obj['center']=[(x_min+x_max)//2, (y_min+y_max)//2]  \n",
    "        if obj['class']!='player':\n",
    "            bbox = obj[\"box\"]\n",
    "            class_name = obj[\"class\"]\n",
    "            color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "            draw_ellipse(img, bbox, color, THICKNESS)\n",
    "    \n",
    "            cv2.putText(img,str(obj['id']), (int((bbox[0]+bbox[2])/2), int(bbox[3]+30)), 0, 0.75,\n",
    "                        color_dict[obj['class']], 2)\n",
    "    for obj in ball_results[current]:\n",
    "        x_min, y_min, x_max, y_max = obj['box']\n",
    "        obj['center']=[(x_min+x_max)//2, (y_min+y_max)//2]  \n",
    "        bbox = obj[\"box\"]\n",
    "        class_name = obj[\"class\"]\n",
    "        color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "        draw_ellipse(img, bbox, color, THICKNESS)\n",
    "#        cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[3]+10)), 0, 0.5,\n",
    "#                    color_dict[obj['class']], 1)\n",
    "    \n",
    "    for obj in other_results[current]:\n",
    "        x_min, y_min, x_max, y_max = obj['box']\n",
    "        obj['center']=[(x_min+x_max)//2, (y_min+y_max)//2]  \n",
    "        bbox = obj[\"box\"]\n",
    "        class_name = obj[\"class\"]\n",
    "        color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "        draw_ellipse(img, bbox, color, THICKNESS)\n",
    "#        cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[3]+10)), 0, 0.5,\n",
    "#                    color_dict[obj['class']], 1)\n",
    "\n",
    "    \n",
    "    players = player_results[current]\n",
    "    ball = ball_results[current]\n",
    "    player_in_possession = get_player_in_possession(players, ball)\n",
    "    \n",
    "    if player_in_possession!= None:\n",
    "        draw_marker(img, player_in_possession['center'], PLAYER_MARKER_COLOR, MARKER_MARGIN = 40)\n",
    "    for b in ball:\n",
    "        draw_marker(img, b['center'], BALL_MARKER_COLOR ,MARKER_MARGIN = 5)\n",
    "    \n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51878e97-85c0-4a24-b7f4-0dc654b5f30d",
   "metadata": {},
   "source": [
    "# Strong-Sort Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7aabad-7ac3-44d6-ba90-81834ab2f7af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from strong_sort.utils.parser import get_config\n",
    "from strong_sort.strong_sort import StrongSORT\n",
    "from strong_sort.sort.tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fa99f-1039-44cc-a9fc-fe075df645be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "strong_sort_weights='strong_sort/osnet_x0_25_msmt17.pt'\n",
    "tracker= StrongSORT(model_weights= strong_sort_weights,\n",
    "                 device='cpu',\n",
    "                 fp16=False,\n",
    "                 max_dist=0.2,\n",
    "                 max_iou_distance=0.7,\n",
    "                 max_age=70, n_init=3,\n",
    "                 nn_budget=100,\n",
    "                 mc_lambda=0.995,\n",
    "                 ema_alpha=0.9\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df23611-5cde-4d39-8a78-6694f9086289",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_white.mp4')\n",
    "all_frames_data = []\n",
    "current=0\n",
    "\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    boxes, scores, names, class_ids= [] , [], [], []\n",
    "    for obj in all_data[current]:\n",
    "        if obj['score']>0.6:\n",
    "            boxes.append(obj['box'])\n",
    "            scores.append(obj['score'])\n",
    "            names.append(obj['class'])\n",
    "            class_ids.append(obj['class_id'])\n",
    "\n",
    "    boxes, scores, names,class_ids= np.array(boxes), np.array(scores), np.array(names), np.array(class_ids)\n",
    "    \n",
    "    converted_boxes = np.array(convert_boxes(img, boxes))\n",
    "    #print(converted_boxes)\n",
    "    #break\n",
    "    tracks=tracker.update(converted_boxes, scores, class_ids,img)\n",
    "\n",
    "\n",
    "    for track in tracker.tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "        track_id=track.track_id\n",
    "        hits=track.hits\n",
    "        class_id=track.class_id\n",
    "        bbox = track.to_tlbr()\n",
    "        #class_name= track.get_class()\n",
    "\n",
    "        object_data = {\n",
    "            \"box\": bbox,\n",
    "            \"class\":class_id,\n",
    "            \"id\": track_id\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_frames_data.append(frame_objects)\n",
    "        \n",
    "    print(f\"frame{current+1} done\")\n",
    "    current+=1\n",
    "\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758eff9-fd8f-4a19-9b8c-da0319f8f08c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for frame in all_frames_data:\n",
    "    for detection in frame:\n",
    "        xmin, ymin, xmax, ymax = detection['box']\n",
    "        w=xmax-xmin\n",
    "        h=ymax-ymin\n",
    "        xmin, ymin, xmax, ymax = xmin+w/2, ymin+h/2, xmax+w/2, ymax+h/2\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        detection['box']=[xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16236e2-ed59-4007-9a83-ab78d89d07bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your label map\n",
    "labels = [\n",
    "    {'name': 'ball', 'id': 1},\n",
    "    {'name': 'goalkeeper', 'id': 2},\n",
    "    {'name': 'player', 'id': 3},\n",
    "    {'name': 'referee', 'id': 4}\n",
    "]\n",
    "\n",
    "# Convert label map to a dictionary for easy lookup\n",
    "id_to_name_dict = {item['id']: item['name'] for item in labels}\n",
    "\n",
    "\n",
    "# Updating 'class' from class ID to class name\n",
    "for frame in all_frames_data:\n",
    "    for detection in frame:\n",
    "        class_id = detection['class']\n",
    "        class_name = id_to_name_dict.get(class_id)\n",
    "        detection['class'] = class_name\n",
    "\n",
    "# all_data now has 'class' as class name\n",
    "all_frames_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc41c7-ae03-40df-95b1-bdea8534436e",
   "metadata": {},
   "source": [
    "# Getting detections and tracking with yolov8 model\n",
    "detections : https://docs.ultralytics.com/modes/predict/#inference-sources\n",
    "\n",
    "tracking : https://docs.ultralytics.com/modes/track/#tracking\n",
    "\n",
    "model names: https://docs.ultralytics.com/models/yolov8/#performance-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75489b0-ac4f-417a-a405-0350165bdab0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "import ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45051fe-76a8-4608-958f-fbcd872f7481",
   "metadata": {},
   "source": [
    "### import dataset from roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fc614-1e03-4654-8ed7-894dbcd3b0d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fed413-9f0c-455a-89f0-dc219aea46b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!mkdir {HOME}/datasets\n",
    "%cd {HOME}/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f397229-2627-4b31-9fbf-3e38388dc343",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install roboflow --quiet\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
    "project = rf.workspace(\"roboflow-jvuqo\").project(\"football-players-detection-3zvbc\")\n",
    "dataset = project.version(1).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390e091-ab92-49c8-86e5-1b39af919834",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_file_path=f'{dataset.location}/data.yaml'\n",
    "# Load the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_content = yaml.safe_load(file)\n",
    "\n",
    "# Modify the data\n",
    "yaml_content['test'] = f\"{dataset.location}/test/images\"\n",
    "yaml_content['train'] = f\"{dataset.location}/train/images\"\n",
    "yaml_content['val'] = f\"{dataset.location}/valid/images\"\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.safe_dump(yaml_content, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1f34e-7447-48c9-bdf6-83ba3b94fef3",
   "metadata": {},
   "source": [
    "### train custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f6ff6-e895-4702-839f-3e5ca41f94d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=train model=yolov8x.pt data={dataset.location}/data.yaml epochs=250 imgsz=6400 plots=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f32907-09b3-4a68-84d3-2ef12b41d166",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64853520-a7a9-4b91-95ea-9788447a677a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbefe8c-350a-489b-842b-842bfa323dbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863ec10-c6f4-441e-9c0b-7fbaf82a8df2",
   "metadata": {},
   "source": [
    "### validate custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fceff-3d12-4e9b-8ab5-bf6d92c5caf2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d28f0-9724-4b03-9357-f921be15e160",
   "metadata": {},
   "source": [
    "### inference with custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaddf9c-4f3b-4a2a-8d43-bc32de6248d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82f398-7659-43ad-8268-98fd83f4e151",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"best3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08c415-fc16-4d18-b4ef-ee9f31d7166f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d7618-258a-449d-9971-1e625a1d8fd6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results = model.predict(source='data/video/red_green.mp4', device=\"cpu\", conf=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27358037-da31-42b0-b4f6-873f97e158ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_detection_data = []\n",
    "for r in results:\n",
    "    boxes = r.boxes\n",
    "    print(boxes.conf)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411c700-c6e0-4701-862c-634385906f6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Colors palettes and team prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b5b82-2bf6-43e2-a679-cdc7a6d2f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb41f7-5780-4bcd-ab32-a79e0113ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white.mp4\")\n",
    "vid.set(cv2.CAP_PROP_POS_FRAMES, 100)\n",
    "_, frame = vid.read()\n",
    "vid.release()\n",
    "players = player_results[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78252a1f-b466-44e0-97ec-be087932d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(images, rows, cols):\n",
    "    \"\"\"\n",
    "    Stitch multiple images into a single image for display.\n",
    "    :param images: List of images to be stitched.\n",
    "    :param rows: Number of rows in the final stitched image.\n",
    "    :param cols: Number of columns in the final stitched image.\n",
    "    :return: Stitched image.\n",
    "    \"\"\"\n",
    "    # Calculate max height and total width\n",
    "    max_height = max(im.shape[0] for im in images)\n",
    "    total_width = sum(im.shape[1] for im in images)\n",
    "\n",
    "    # Create a blank canvas with max height and total width\n",
    "    stitched_image = np.zeros((max_height, total_width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Paste images into the canvas\n",
    "    current_x = 0\n",
    "    for im in images:\n",
    "        stitched_image[:im.shape[0], current_x:current_x+im.shape[1]] = im\n",
    "        current_x += im.shape[1]\n",
    "\n",
    "    return cv2.resize(stitched_image, (cols, rows))\n",
    "\n",
    "def select_team_colors(stitched_image, team_name):\n",
    "    \"\"\"\n",
    "    Open a window to select colors for a team.\n",
    "    :param stitched_image: Image containing all players.\n",
    "    :param team_name: Name of the team (for window title).\n",
    "    :return: List of selected colors.\n",
    "    \"\"\"\n",
    "    selected_colors = []\n",
    "\n",
    "    def mouse_click(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            # Append the color (in BGR format) where clicked\n",
    "            selected_colors.append(stitched_image[y, x, :].tolist())\n",
    "            print(f\"Color selected for {team_name}: {stitched_image[y, x, :]}\")\n",
    "\n",
    "    cv2.namedWindow(team_name)\n",
    "    cv2.setMouseCallback(team_name, mouse_click)\n",
    "    \n",
    "    while True:\n",
    "        cv2.imshow(team_name, stitched_image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return [selected_colors[-1]]\n",
    "\n",
    "cropped_images = []\n",
    "\n",
    "# Assuming 'frame_rgb' and 'results_players' are already defined as per your existing code\n",
    "for player in players:\n",
    "    bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "    cropped_img = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]): int(bbox[2])]\n",
    "    cropped_images.append(cropped_img)\n",
    "\n",
    "# Define the size of the stitched image\n",
    "rows, cols = 500, 1000  # Adjust as needed\n",
    "\n",
    "# Create the stitched image\n",
    "stitched_image = stitch_images(cropped_images, rows, cols)\n",
    "\n",
    "# Get colors for each team's kits\n",
    "team1_player_colors = select_team_colors(stitched_image, \"Team 1 Player Kit\")\n",
    "team2_player_colors = select_team_colors(stitched_image, \"Team 2 Player Kit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751007c4-a911-4eb0-abe9-28a2eb653510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_image(image, scale_factor):\n",
    "    \"\"\"\n",
    "    Resize an image by a scale factor.\n",
    "    :param image: Image to resize.\n",
    "    :param scale_factor: Factor to scale by.\n",
    "    :return: Resized image.\n",
    "    \"\"\"\n",
    "    new_width = int(image.shape[1] * scale_factor)\n",
    "    new_height = int(image.shape[0] * scale_factor)\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_NEAREST)\n",
    "    return resized_image\n",
    "\n",
    "def stitch_images_in_rows(images, row_count=2):\n",
    "    \"\"\"\n",
    "    Stitch images into a given number of rows.\n",
    "    :param images: List of images to be stitched.\n",
    "    :param row_count: Number of rows.\n",
    "    :return: Image stitched in rows.\n",
    "    \"\"\"\n",
    "    images_per_row = len(images) // row_count + (1 if len(images) % row_count > 0 else 0)\n",
    "    rows = []\n",
    "    \n",
    "    for i in range(0, len(images), images_per_row):\n",
    "        row_images = images[i:i+images_per_row]\n",
    "        total_width = sum(im.shape[1] for im in row_images)\n",
    "        max_height = max(im.shape[0] for im in row_images)\n",
    "        \n",
    "        # Create a blank canvas for each row\n",
    "        row_image = np.zeros((max_height, total_width, 3), dtype=np.uint8)\n",
    "        current_x = 0\n",
    "        for im in row_images:\n",
    "            h = im.shape[0]\n",
    "            row_image[:h, current_x:current_x+im.shape[1]] = im\n",
    "            current_x += im.shape[1]\n",
    "        rows.append(row_image)\n",
    "    \n",
    "    # Stitch the rows vertically\n",
    "    max_width = max(row.shape[1] for row in rows)\n",
    "    total_height = sum(row.shape[0] for row in rows)\n",
    "    \n",
    "    stitched_image = np.zeros((total_height, max_width, 3), dtype=np.uint8)\n",
    "    current_y = 0\n",
    "    for row in rows:\n",
    "        w = row.shape[1]\n",
    "        stitched_image[current_y:current_y+row.shape[0], :w] = row\n",
    "        current_y += row.shape[0]\n",
    "\n",
    "    return stitched_image\n",
    "def select_team_colors(stitched_image, team_name):\n",
    "    \"\"\"\n",
    "    Open a window to select colors for a team.\n",
    "    :param stitched_image: Image containing all players.\n",
    "    :param team_name: Name of the team (for window title).\n",
    "    :return: List of selected colors.\n",
    "    \"\"\"\n",
    "    selected_colors = []\n",
    "\n",
    "    def mouse_click(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            # Append the color (in BGR format) where clicked\n",
    "            selected_colors.append(stitched_image[y, x, :].tolist())\n",
    "            print(f\"Color selected for {team_name}: {stitched_image[y, x, :]}\")\n",
    "\n",
    "    cv2.namedWindow(team_name)\n",
    "    cv2.setMouseCallback(team_name, mouse_click)\n",
    "    \n",
    "    while True:\n",
    "        cv2.imshow(team_name, stitched_image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return [selected_colors[-1]]\n",
    "\n",
    "cropped_images = []\n",
    "\n",
    "# Assuming 'frame_rgb' and 'results_players' are already defined as per your existing code\n",
    "for player in players:\n",
    "    bbox = player['box']  # Get bbox info (x,y,x,y)\n",
    "    cropped_img = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]): int(bbox[2])]\n",
    "    \n",
    "    # Resize the cropped image by a factor of 10\n",
    "    resized_cropped_img = resize_image(cropped_img, 5)\n",
    "    cropped_images.append(resized_cropped_img)\n",
    "\n",
    "# Stitch the images in two rows\n",
    "stitched_image = stitch_images_in_rows(cropped_images, 2)\n",
    "\n",
    "# Get colors for each team's kits (this part assumes you have a graphical interface to display the image)\n",
    "team1_player_colors = select_team_colors(stitched_image, \"Team 1 Player Kit\")\n",
    "team2_player_colors = select_team_colors(stitched_image, \"Team 2 Player Kit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae640a9-dbc7-4f20-95ff-cd8ebfd60849",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list= team1_player_colors + team2_player_colors\n",
    "colors_list_rgb = [color[::-1] for color in colors_list]\n",
    "color_list_lab = [skimage.color.rgb2lab([i/255 for i in c]) for c in colors_list_rgb] # Converting color_list to L*a*b* space\n",
    "color_list_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f729b-31f2-4385-84b3-f4d146fb10b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                                      # Convert frame to RGB\n",
    "obj_palette_list = []                                                                   # Initialize players color palette list\n",
    "palette_interval = (0,5)                                                                # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "annotated_frame = frame.copy()                                                                 # Create annotated frame \n",
    "for player in players:\n",
    "    bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "    obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "    \n",
    "    obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "    center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "    center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "    center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "    center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "    center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                            center_filter_x1:center_filter_x2]\n",
    "\n",
    "   \n",
    "    \n",
    "    obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "        \n",
    "    reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "    palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "    palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "    color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "#    RGB_df = pd.DataFrame(color_count, columns = ['cnt', 'RGB']).sort_values(       # Create dataframe based on defined palette interval\n",
    "#                          by = 'cnt', ascending = False).iloc[\n",
    "#                              palette_interval[0]:palette_interval[1],:]\n",
    "#    palette = list(RGB_df.RGB)                                                      # Convert palette to list (for faster processing)\n",
    "    annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "                                    (int(bbox[0])+center_filter_x1, \n",
    "                                     int(bbox[1])+ center_filter_y1),  \n",
    "                                    (int(bbox[0])+center_filter_x2, \n",
    "                                     int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "    player_palette = []\n",
    "    for count, color in color_count:\n",
    "            player_palette.append((color, count))\n",
    "\n",
    "    obj_palette_list.append(player_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9eec68-3878-47ed-8d84-1866beb94741",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "players_distance_features = []\n",
    "\n",
    "# Loop over detected players' extracted color palettes\n",
    "for player_palette in obj_palette_list:\n",
    "    palette_distance = []\n",
    "\n",
    "    # Loop over colors in the player's palette (color is a tuple of (RGB, frequency))\n",
    "    for color, freq in player_palette:\n",
    "        color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "        distance_list = []\n",
    "\n",
    "        # Loop over predefined list of teams colors (now only two colors, one for each team)\n",
    "        for c in color_list_lab:\n",
    "            distance = skimage.color.deltaE_cie76(color_lab, c)  # Calculate Euclidean distance in Lab color space\n",
    "            distance_list.append(distance)  # Update distance list for current color\n",
    "\n",
    "        palette_distance.append((distance_list, freq))  # Update distance list for current palette with frequency\n",
    "\n",
    "    players_distance_features.append(palette_distance)  # Update distance features list\n",
    "\n",
    "\n",
    "\n",
    "players_teams_list = []\n",
    "\n",
    "for player_palette in obj_palette_list:\n",
    "    team_votes = [0, 0]  # Initialize votes for two teams\n",
    "\n",
    "    for color, freq in player_palette:\n",
    "        color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "        distance_list = []\n",
    "\n",
    "        # Calculate distances for the color\n",
    "        for c in color_list_lab:\n",
    "            distance = skimage.color.deltaE_cie76(color_lab, c)\n",
    "            distance_list.append(distance)\n",
    "\n",
    "        # Weighted voting\n",
    "        team_idx = distance_list.index(min(distance_list))  # Closer team gets the vote\n",
    "        team_votes[team_idx] += freq  # Weight vote by frequency of the color\n",
    "\n",
    "    # Team with the highest weighted vote total is predicted\n",
    "    predicted_team = team_votes.index(max(team_votes))\n",
    "    players_teams_list.append(predicted_team)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4aa134-2c6e-4711-85f9-cc8b89169c7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "cropped_images = []\n",
    "\n",
    "# Assuming 'frame_rgb' and 'results_players' are already defined as per your existing code\n",
    "for player in players:\n",
    "    bbox = player['box']  # Get bbox info (x,y,x,y)\n",
    "    cropped_img = annotated_frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "    cropped_images.append(cropped_img)\n",
    "\n",
    "# Assuming colors_list is defined and contains two BGR colors, for example:\n",
    "# colors_list = [(0, 0, 255), (255, 0, 0)]  # BGR format\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Convert BGR to RGB and create a rectangle for each color\n",
    "for i, color in enumerate(colors_list):\n",
    "    # BGR to RGB conversion\n",
    "    rgb_color = color[::-1]  # Reverse the color order\n",
    "    rect = patches.Rectangle((i*1.1, 0), 1, 1, color=[c/255 for c in rgb_color])\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i*1.1 + 0.5, 1.1, f'Team {i+1}', ha='center')\n",
    "\n",
    "# Set limits and remove axes for clarity\n",
    "ax.set_xlim(0, len(colors_list)*1.1)\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "team_names = [\"Team1\", \"Team2\"]\n",
    "\n",
    "for i, (img, palette) in enumerate(zip(cropped_images, obj_palette_list)):\n",
    "    team = team_names[players_teams_list[i]]\n",
    "    print(f'Player {i+1}: {team}')\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "    # Display the cropped image\n",
    "    ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(f'Player {i+1}')\n",
    "\n",
    "    # Prepare data for the histogram\n",
    "    colors = [color for color, _ in palette]\n",
    "    frequencies = [freq for _, freq in palette]\n",
    "\n",
    "    # Display the histogram\n",
    "    bars = ax2.bar(range(len(colors)), frequencies, color=[(r/255, g/255, b/255) for r, g, b in colors])\n",
    "    ax2.set_title('Color Histogram')\n",
    "    ax2.set_xlabel('Colors')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_xticks([])  # Hide x-ticks as they are not meaningful in this context\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c311bd7-a747-475b-adaf-6f7a1e44cce4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def team_color_v1(frame, players):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                                      # Convert frame to RGB\n",
    "    obj_palette_list = []                                                                   # Initialize players color palette list\n",
    "    palette_interval = (0,5)                                                                # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "    annotated_frame = frame.copy()                                                                 # Create annotated frame \n",
    "    for player in players:\n",
    "        bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "        obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "        \n",
    "        obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "        center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "        center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "        center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "        center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "        center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                                center_filter_x1:center_filter_x2]\n",
    "    \n",
    "       \n",
    "        \n",
    "        obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "            \n",
    "        reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "        palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "        palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "        color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "    #    RGB_df = pd.DataFrame(color_count, columns = ['cnt', 'RGB']).sort_values(       # Create dataframe based on defined palette interval\n",
    "    #                          by = 'cnt', ascending = False).iloc[\n",
    "    #                              palette_interval[0]:palette_interval[1],:]\n",
    "    #    palette = list(RGB_df.RGB)                                                      # Convert palette to list (for faster processing)\n",
    "        annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "                                        (int(bbox[0])+center_filter_x1, \n",
    "                                         int(bbox[1])+ center_filter_y1),  \n",
    "                                        (int(bbox[0])+center_filter_x2, \n",
    "                                         int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "        player_palette = []\n",
    "        for count, color in color_count:\n",
    "                player_palette.append((color, count))\n",
    "    \n",
    "        obj_palette_list.append(player_palette)\n",
    "        \n",
    "    ## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "    players_distance_features = []\n",
    "    \n",
    "    # Loop over detected players' extracted color palettes\n",
    "    for player_palette in obj_palette_list:\n",
    "        palette_distance = []\n",
    "    \n",
    "        # Loop over colors in the player's palette (color is a tuple of (RGB, frequency))\n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Loop over predefined list of teams colors (now only two colors, one for each team)\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)  # Calculate Euclidean distance in Lab color space\n",
    "                distance_list.append(distance)  # Update distance list for current color\n",
    "    \n",
    "            palette_distance.append((distance_list, freq))  # Update distance list for current palette with frequency\n",
    "    \n",
    "        players_distance_features.append(palette_distance)  # Update distance features list\n",
    "    \n",
    "    \n",
    "    \n",
    "    players_teams_list = []\n",
    "    \n",
    "    for player_palette in obj_palette_list:\n",
    "        team_votes = [0, 0]  # Initialize votes for two teams\n",
    "    \n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Calculate distances for the color\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)\n",
    "                distance_list.append(distance)\n",
    "    \n",
    "            # Weighted voting\n",
    "            team_idx = distance_list.index(min(distance_list))  # Closer team gets the vote\n",
    "            team_votes[team_idx] += freq  # Weight vote by frequency of the color\n",
    "    \n",
    "        # Team with the highest weighted vote total is predicted\n",
    "        predicted_team = team_votes.index(max(team_votes))\n",
    "        players_teams_list.append(predicted_team)\n",
    "    for i in range (len(players)):\n",
    "        player=players[i]\n",
    "        player['class']=f'team {players_teams_list[i]+1}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cdac2-006f-4e8b-b302-dc391156aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_color_v2(frame, players):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                                      # Convert frame to RGB\n",
    "    obj_palette_list = []                                                                   # Initialize players color palette list\n",
    "    palette_interval = (0,5)                                                                # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "    annotated_frame = frame.copy()                                                                 # Create annotated frame \n",
    "    for player in players:\n",
    "        bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "        obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "\n",
    "        # Reduce resolution of obj_img\n",
    "        scale_factor = 0.5  # Adjust this based on experimentation\n",
    "        obj_img = cv2.resize(obj_img, (0, 0), fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        \n",
    "        obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "        center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "        center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "        center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "        center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "        center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                                center_filter_x1:center_filter_x2]\n",
    "    \n",
    "       \n",
    "        \n",
    "        obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "            \n",
    "        reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "        palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "        palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "        color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "        # Sort color_count by frequency (descending) and keep the top 5\n",
    "        color_count = sorted(color_count, key=lambda x: x[0], reverse=True)[:5]\n",
    "\n",
    "        annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "                                        (int(bbox[0])+center_filter_x1, \n",
    "                                         int(bbox[1])+ center_filter_y1),  \n",
    "                                        (int(bbox[0])+center_filter_x2, \n",
    "                                         int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "        player_palette = []\n",
    "        for count, color in color_count:\n",
    "                player_palette.append((color, count))\n",
    "    \n",
    "        obj_palette_list.append(player_palette)\n",
    "        \n",
    "    ## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "    players_distance_features = []\n",
    "    \n",
    "    # Loop over detected players' extracted color palettes\n",
    "    for player_palette in obj_palette_list:\n",
    "        palette_distance = []\n",
    "    \n",
    "        # Loop over colors in the player's palette (color is a tuple of (RGB, frequency))\n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Loop over predefined list of teams colors (now only two colors, one for each team)\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)  # Calculate Euclidean distance in Lab color space\n",
    "                distance_list.append(distance)  # Update distance list for current color\n",
    "    \n",
    "            palette_distance.append((distance_list, freq))  # Update distance list for current palette with frequency\n",
    "    \n",
    "        players_distance_features.append(palette_distance)  # Update distance features list\n",
    "    \n",
    "    \n",
    "    \n",
    "    players_teams_list = []\n",
    "    \n",
    "    for player_palette in obj_palette_list:\n",
    "        team_votes = [0, 0]  # Initialize votes for two teams\n",
    "    \n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Calculate distances for the color\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)\n",
    "                distance_list.append(distance)\n",
    "    \n",
    "            # Weighted voting\n",
    "            team_idx = distance_list.index(min(distance_list))  # Closer team gets the vote\n",
    "            team_votes[team_idx] += freq  # Weight vote by frequency of the color\n",
    "    \n",
    "        # Team with the highest weighted vote total is predicted\n",
    "        predicted_team = team_votes.index(max(team_votes))\n",
    "        players_teams_list.append(predicted_team)\n",
    "    for i in range (len(players)):\n",
    "        player=players[i]\n",
    "        player['class']=f'team {players_teams_list[i]+1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69f780-ce66-4e72-9557-ca5081418e6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def team_color_v3(frame, players):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame to RGB\n",
    "    obj_palette_list = []  # Initialize players color palette list\n",
    "    annotated_frame = frame.copy()  # Create annotated frame\n",
    "\n",
    "    for player in players:\n",
    "        bbox = player['box']  # Get bbox info (x,y,x,y)\n",
    "        obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]  # Crop bbox out of the frame\n",
    "        if obj_img.size > 0:\n",
    "            # Reduce resolution of obj_img\n",
    "            scale_factor = 0.5  # Adjust based on experimentation\n",
    "            obj_img = cv2.resize(obj_img, (0, 0), fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "            obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "            center_filter_x1 = np.max([(obj_img_w // 2) - (obj_img_w // 5), 1])\n",
    "            center_filter_x2 = (obj_img_w // 2) + (obj_img_w // 5)\n",
    "            center_filter_y1 = np.max([(obj_img_h // 3) - (obj_img_h // 5), 1])\n",
    "            center_filter_y2 = (obj_img_h // 3) + (obj_img_h // 5)\n",
    "            center_filter = obj_img[center_filter_y1:center_filter_y2, center_filter_x1:center_filter_x2]\n",
    "    \n",
    "            obj_pil_img = Image.fromarray(np.uint8(center_filter))  # Convert to pillow image\n",
    "            reduced = obj_pil_img.convert(\"P\", palette=Image.ADAPTIVE, colors=16)  # Limit colors to top 16 for faster processing\n",
    "            palette = reduced.getpalette()  # Get palette as [r,g,b,r,g,b,...]\n",
    "            palette = [palette[3 * n:3 * n + 3] for n in range(16)]  # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "            color_count = [(n, palette[m]) for n, m in reduced.getcolors()]  # Create list of palette colors with their frequency\n",
    "            color_count = sorted(color_count, key=lambda x: x[0], reverse=True)[:5]  # Keep top 5 colors\n",
    "    \n",
    "            player_palette = [(color, count) for count, color in color_count]\n",
    "            obj_palette_list.append(player_palette)\n",
    "\n",
    "    # Prepare all player palette colors for batch conversion to Lab\n",
    "    all_colors = np.array([color for player_palette in obj_palette_list for color, _ in player_palette])\n",
    "    all_colors = all_colors / 255.0  # Normalize for conversion\n",
    "    all_colors_lab = skimage.color.rgb2lab(all_colors.reshape(-1, 1, 3)).reshape(-1, 3)  # Convert all colors to Lab in batch\n",
    "\n",
    "    # Vectorized distance calculation\n",
    "    players_distance_features = []\n",
    "    color_idx = 0\n",
    "    for player_palette in obj_palette_list:\n",
    "        palette_distance = []\n",
    "        for _, freq in player_palette:\n",
    "            color_lab = all_colors_lab[color_idx]\n",
    "            color_idx += 1\n",
    "            distances = np.linalg.norm(color_list_lab - color_lab, axis=1)  # Calculate distances to team colors\n",
    "            palette_distance.append((distances, freq))\n",
    "        players_distance_features.append(palette_distance)\n",
    "\n",
    "    players_teams_list = []\n",
    "    \n",
    "    for player_palette in obj_palette_list:\n",
    "        team_votes = [0, 0]  # Initialize votes for two teams\n",
    "    \n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Calculate distances for the color\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)\n",
    "                distance_list.append(distance)\n",
    "    \n",
    "            # Weighted voting\n",
    "            team_idx = distance_list.index(min(distance_list))  # Closer team gets the vote\n",
    "            team_votes[team_idx] += freq  # Weight vote by frequency of the color\n",
    "    \n",
    "        # Team with the highest weighted vote total is predicted\n",
    "        predicted_team = team_votes.index(max(team_votes))\n",
    "        players_teams_list.append(predicted_team)\n",
    "    for i in range (len(players)):\n",
    "        player=players[i]\n",
    "        if i < len(players_teams_list):     \n",
    "            player['class']=f'team {players_teams_list[i]+1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164b9c4-2718-4cff-9a46-2a550caf8a77",
   "metadata": {},
   "source": [
    "# Determine team color before tracking to improve tracking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d95fd-6b61-4939-81b3-6f277bdb6894",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Video frame dimensions\n",
    "frame_width = 1920\n",
    "frame_height = 1080\n",
    "\n",
    "# Convert normalized box coordinates to absolute pixel values\n",
    "def convert_normalized_box(box, width, height):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return [xtl * width, ytl * height, xbr * width, ybr * height]\n",
    "\n",
    "\n",
    "# Adjust ball_data to use absolute pixel values\n",
    "before_tracking_data_absolute = []\n",
    "for frame in before_tracking_data:\n",
    "    objects=[]\n",
    "    for obj in frame:\n",
    "        normalized_box = obj['box']\n",
    "        absolute_box = convert_normalized_box(normalized_box, frame_width, frame_height)\n",
    "        objects.append({'score': obj['score'], 'class': obj['class'], 'box': absolute_box})\n",
    "    before_tracking_data_absolute.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e654f9-bba2-49c4-8d63-c203902eb2ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "current=0\n",
    "\n",
    "    \n",
    "while True:\n",
    "    _, frame = vid.read()\n",
    "    if frame is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "        \n",
    "    frame_data=before_tracking_data_absolute[current]\n",
    "    players=[obj for obj in frame_data if obj['class']=='player']\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                                      # Convert frame to RGB\n",
    "    obj_palette_list = []                                                                   # Initialize players color palette list\n",
    "#    palette_interval = (0,5)                                                                # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "#    annotated_frame = frame.copy()                                                                 # Create annotated frame \n",
    "    for player in players:\n",
    "        bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "        obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "        \n",
    "        obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "        center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "        center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "        center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "        center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "        center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                                center_filter_x1:center_filter_x2]\n",
    "    \n",
    "       \n",
    "        \n",
    "        obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "            \n",
    "        reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "        palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "        palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "        color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "    #    RGB_df = pd.DataFrame(color_count, columns = ['cnt', 'RGB']).sort_values(       # Create dataframe based on defined palette interval\n",
    "    #                          by = 'cnt', ascending = False).iloc[\n",
    "    #                              palette_interval[0]:palette_interval[1],:]\n",
    "    #    palette = list(RGB_df.RGB)                                                      # Convert palette to list (for faster processing)\n",
    "    #    annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "    #                                    (int(bbox[0])+center_filter_x1, \n",
    "    #                                     int(bbox[1])+ center_filter_y1),  \n",
    "    #                                    (int(bbox[0])+center_filter_x2, \n",
    "    #                                     int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "        player_palette = []\n",
    "        for count, color in color_count:\n",
    "                player_palette.append((color, count))\n",
    "    \n",
    "        obj_palette_list.append(player_palette)\n",
    "\n",
    "    ## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "    players_distance_features = []\n",
    "    \n",
    "    # Loop over detected players' extracted color palettes\n",
    "    for player_palette in obj_palette_list:\n",
    "        palette_distance = []\n",
    "    \n",
    "        # Loop over colors in the player's palette (color is a tuple of (RGB, frequency))\n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Loop over predefined list of teams colors (now only two colors, one for each team)\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)  # Calculate Euclidean distance in Lab color space\n",
    "                distance_list.append(distance)  # Update distance list for current color\n",
    "    \n",
    "            palette_distance.append((distance_list, freq))  # Update distance list for current palette with frequency\n",
    "    \n",
    "        players_distance_features.append(palette_distance)  # Update distance features list\n",
    "    \n",
    "    \n",
    "    \n",
    "    players_teams_list = []\n",
    "    \n",
    "    for player_palette in obj_palette_list:\n",
    "        team_votes = [0, 0]  # Initialize votes for two teams\n",
    "    \n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Calculate distances for the color\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)\n",
    "                distance_list.append(distance)\n",
    "    \n",
    "            # Weighted voting\n",
    "            team_idx = distance_list.index(min(distance_list))  # Closer team gets the vote\n",
    "            team_votes[team_idx] += freq  # Weight vote by frequency of the color\n",
    "    \n",
    "        # Team with the highest weighted vote total is predicted\n",
    "        predicted_team = team_votes.index(max(team_votes))\n",
    "        players_teams_list.append(predicted_team)\n",
    "\n",
    "    for i in range(len(players)):\n",
    "        team=f\"team{players_teams_list[i]+1}\"\n",
    "        players[i]['class']=team\n",
    "        \n",
    "    current+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afe8ca-71d5-4abf-941a-df921c466cec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_yellow8.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "\n",
    "color_dict = {\n",
    "\"team1\": (0, 0, 255),    \n",
    "\"team2\": (0, 255, 255),    \n",
    "\"referee\": (255,255,0), \n",
    "\"goalkeeper\": (255, 0, 0), \n",
    "\"player\": (255, 255, 0), \n",
    "\"ball\": (255, 255, 255)  \n",
    "}\n",
    "\n",
    "for frame in before_tracking_data_absolute:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:     \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "\n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "    for obj in ball_data_absolute[current]:   \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "\n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "\n",
    "\n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e21867-1d1f-408c-8688-701d527c789e",
   "metadata": {},
   "source": [
    "# Tactical map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6aa95-3cb9-4594-98b1-7a75e17795f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "key_points_list = [\n",
    "    \"TLC\", \"TRC\", \"TR6MC\", \"TL6MC\", \"TR6ML\", \"TL6ML\",\n",
    "    \"TR18MC\", \"TL18MC\", \"TR18ML\", \"TL18ML\", \"TRArc\", \"TLArc\",\n",
    "    \"RML\", \"RMC\", \"LMC\", \"LML\", \"BLC\", \"BRC\", \"BR6MC\", \"BL6MC\",\n",
    "    \"BR6ML\", \"BL6ML\", \"BR18MC\", \"BL18MC\", \"BR18ML\", \"BL18ML\",\n",
    "    \"BRArc\", \"BLArc\"\n",
    "]\n",
    "\n",
    "new_key_points = {}  # Dictionary to store new key points\n",
    "current_key_point = 0  # Index to keep track of current key point\n",
    "\n",
    "# Function to handle mouse click events\n",
    "def click_event(event, x, y, flags, param):\n",
    "    global current_key_point, new_key_points\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and current_key_point < len(key_points_list):\n",
    "        new_key_points[key_points_list[current_key_point]] = [x, y]\n",
    "        print(f\"Key Point {key_points_list[current_key_point]}: ({x}, {y})\")\n",
    "        current_key_point += 1\n",
    "        if current_key_point < len(key_points_list):\n",
    "            print(f\"Click the position of {key_points_list[current_key_point]}\")\n",
    "        else:\n",
    "            print(\"All key points have been defined.\")\n",
    "        cv2.circle(img, (x, y), radius=5, color=(0, 255, 0), thickness=-1)\n",
    "        cv2.imshow('Tactical Map', img)\n",
    "\n",
    "# Path to your new tactical map image\n",
    "image_path = 'unnamed-chunk-2-1 (1).png'\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(image_path)\n",
    "cv2.imshow('Tactical Map', img)\n",
    "\n",
    "# Prompt for the first key point\n",
    "print(f\"Click the position of {key_points_list[current_key_point]}\")\n",
    "\n",
    "# Set the mouse callback function to 'click_event'\n",
    "cv2.setMouseCallback('Tactical Map', click_event)\n",
    "\n",
    "# Wait until a key is pressed and close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the new key points dictionary\n",
    "print(\"New Key Points:\")\n",
    "print(new_key_points)\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0d765-3965-422c-abb4-bce49f96f6fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "new_key_points={'TLC': [100, 862], 'TRC': [101, 69], 'TR6MC': [159, 366], 'TL6MC': [159, 566], 'TR6ML': [100, 366], 'TL6ML': [100, 565], 'TR18MC': [278, 248], 'TL18MC': [278, 682], 'TR18ML': [101, 249], 'TL18ML': [100, 684], 'TRArc': [280, 388], 'TLArc': [279, 544], 'RML': [694, 70], 'RMC': [696, 366], 'LMC': [694, 562], 'LML': [696, 860], 'BLC': [1288, 862], 'BRC': [1287, 71], 'BR6MC': [1228, 367], 'BL6MC': [1228, 563], 'BR6ML': [1287, 365], 'BL6ML': [1286, 564], 'BR18MC': [1110, 249], 'BL18MC': [1110, 683], 'BR18ML': [1286, 248], 'BL18ML': [1287, 680], 'BRArc': [1109, 385], 'BLArc': [1110, 545]}\n",
    "# Specify the filename for the JSON file\n",
    "json_filename = 'updated_key_points.json'\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(json_filename, 'w') as file:\n",
    "    json.dump(new_key_points, file, indent=4)\n",
    "\n",
    "print(f\"Key points saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657ae22-957e-41f2-8a93-8359c73a885c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import json\n",
    "\n",
    "# Load the JSON file with the key points\n",
    "with open('updated_key_points.json', 'r') as file:\n",
    "    key_points = json.load(file)\n",
    "\n",
    "# Load the tactical map image\n",
    "tactical_map = mpimg.imread('unnamed-chunk-2-1 (1).png')\n",
    "\n",
    "# Create a figure with a larger size\n",
    "plt.figure(figsize=(15, 10))  # Adjust the size as needed\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(tactical_map)\n",
    "plt.axis('off')  # Optionally remove the axis\n",
    "\n",
    "# Plot each key point and annotate\n",
    "for label, coordinates in key_points.items():\n",
    "    x, y = coordinates\n",
    "    plt.scatter(x, y, marker='o')  # Plot the point\n",
    "    plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=((-3),5), \n",
    "                 ha='left', fontsize=8, color='red')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('annotated_tactical_map2.jpg', format='jpg', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024ea72-b961-49c1-8cb0-5433d4f48f51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('annotations 9.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Initialize arrays for results\n",
    "keypoint_results = []\n",
    "\n",
    "# Function to convert box attributes to a dictionary\n",
    "def get_box_dict(box_element):\n",
    "    return [\n",
    "        float(box_element.get('xtl')),\n",
    "        float(box_element.get('ytl')),\n",
    "        float(box_element.get('xbr')),\n",
    "        float(box_element.get('ybr'))]\n",
    "\n",
    "# Process each track (object)\n",
    "for track in root.findall('track'):\n",
    "    label = track.get('label')\n",
    "\n",
    "    # Dictionary to store boxes for each frame\n",
    "    frames = {}\n",
    "\n",
    "    # Process each box (appearance in a frame)\n",
    "    for box in track.findall('box'):\n",
    "        frame_num = int(box.get('frame'))\n",
    "        outside = box.get('outside') == '1'\n",
    "\n",
    "        if not outside:\n",
    "            box_dict = get_box_dict(box)\n",
    "            frames[frame_num] = {'box': box_dict, 'class': label}\n",
    "\n",
    "    # Sort frames and add to the respective result array\n",
    "    sorted_frames = sorted(frames.items())\n",
    "    for frame_num, frame_data in sorted_frames:\n",
    "        while len(keypoint_results) <= frame_num:\n",
    "            keypoint_results.append([])\n",
    "        keypoint_results[frame_num].append(frame_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f241e-02da-4462-849d-7b0d1165247a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_central_coordinates(results, is_ball=False):\n",
    "    central_pts = []\n",
    "    for obj in results:\n",
    "        # Convert from xyxy to xywh format\n",
    "        x1, y1, x2, y2 = obj['box']\n",
    "        x = x1\n",
    "        y = y1\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        # Calculate central coordinates based on object type\n",
    "        if is_ball:  # For balls\n",
    "            central_pts.append([x + w / 2, y + h / 2])\n",
    "        else:  # For players, goalkeepers, and referees\n",
    "            central_pts.append([x + w / 2, y + h])\n",
    "\n",
    "    return np.array(central_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bdbbc-4d7b-4984-b94b-7ca964671c38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "key_points = new_key_points\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Set keypoints average displacement tolerance level (in pixels) [set to -1 to always update homography matrix]\n",
    "keypoints_displacement_mean_tol = 10\n",
    "tac_map = cv2.imread('unnamed-chunk-2-1 (1).png')\n",
    "\n",
    "player_pos=[]\n",
    "other_pos=[]\n",
    "ball_pos=[]\n",
    "\n",
    "for frame_nbr in range (len(player_results)):\n",
    "    \n",
    "    detected_labels=[obj['class'] for obj in keypoint_results[frame_nbr]]\n",
    "    detected_labels_src_pts_xyxy=[obj['box'] for obj in keypoint_results[frame_nbr]]\n",
    "    detected_labels_src_pts= np.array([np.round([(x1 + x2) / 2, (y1 + y2) / 2]).astype(int) for x1, y1, x2, y2 in detected_labels_src_pts_xyxy])\n",
    "    detected_labels_dst_pts=np.array([key_points[obj['class']] for obj in keypoint_results[frame_nbr]])\n",
    "    \n",
    "    ## Calculate Homography transformation matrix when more than 4 keypoints are detected\n",
    "    if len(detected_labels) > 3:\n",
    "        # Always calculate homography matrix on the first frame\n",
    "        if frame_nbr > 0:\n",
    "            # Determine common detected field keypoints between previous and current frames\n",
    "            common_labels = set(detected_labels_prev) & set(detected_labels)\n",
    "            # When at least 4 common keypoints are detected, determine if they are displaced on average beyond a certain tolerance level\n",
    "            if len(common_labels) > 3:\n",
    "                common_label_idx_prev = [detected_labels_prev.index(i) for i in common_labels]   # Get labels indexes of common detected keypoints from previous frame\n",
    "                common_label_idx_curr = [detected_labels.index(i) for i in common_labels]        # Get labels indexes of common detected keypoints from current frame\n",
    "                coor_common_label_prev = detected_labels_src_pts_prev[common_label_idx_prev]     # Get labels coordiantes of common detected keypoints from previous frame\n",
    "                coor_common_label_curr = detected_labels_src_pts[common_label_idx_curr]          # Get labels coordiantes of common detected keypoints from current frame\n",
    "                coor_error = mean_squared_error(coor_common_label_prev, coor_common_label_curr)  # Calculate error between previous and current common keypoints coordinates\n",
    "                update_homography = coor_error > keypoints_displacement_mean_tol                 # Check if error surpassed the predefined tolerance level\n",
    "            else:\n",
    "                update_homography = True                                                         \n",
    "        else:\n",
    "            update_homography = True\n",
    "    \n",
    "        if  update_homography:\n",
    "            h, mask = cv2.findHomography(detected_labels_src_pts,                   # Calculate homography matrix\n",
    "                                          detected_labels_dst_pts)                  \n",
    "\n",
    "        detected_labels_prev = detected_labels.copy()                               # Save current detected keypoint labels for next frame\n",
    "        detected_labels_src_pts_prev = detected_labels_src_pts.copy()               # Save current detected keypoint coordiantes for next frame\n",
    "\n",
    "        detected_player_src_pts = calculate_central_coordinates(player_results[frame_nbr])\n",
    "        detected_other_src_pts = calculate_central_coordinates(other_results[frame_nbr])\n",
    "        detected_ball_src_pts = calculate_central_coordinates(ball_results[frame_nbr], is_ball=True)\n",
    "\n",
    "        # Transform players coordinates from frame plane to tactical map plance using the calculated Homography matrix\n",
    "        pred_player_pts = []                                                           # Initialize players tactical map coordiantes list\n",
    "        for pt in detected_player_src_pts:                                            # Loop over players frame coordiantes\n",
    "            pt = np.append(np.array(pt), np.array([1]), axis=0)                     # Covert to homogeneous coordiantes\n",
    "            dest_point = np.matmul(h, np.transpose(pt))                              # Apply homography transofrmation\n",
    "            dest_point = dest_point/dest_point[2]                                   # Revert to 2D-coordiantes\n",
    "            pred_player_pts.append(list(np.transpose(dest_point)[:2]))                 # Update players tactical map coordiantes list\n",
    "        pred_player_pts = np.array(pred_player_pts)\n",
    "        player_pos.append(pred_player_pts)\n",
    "        \n",
    "        pred_other_pts = []                                                           # Initialize players tactical map coordiantes list\n",
    "        for pt in detected_other_src_pts:                                            # Loop over players frame coordiantes\n",
    "            pt = np.append(np.array(pt), np.array([1]), axis=0)                     # Covert to homogeneous coordiantes\n",
    "            dest_point = np.matmul(h, np.transpose(pt))                              # Apply homography transofrmation\n",
    "            dest_point = dest_point/dest_point[2]                                   # Revert to 2D-coordiantes\n",
    "            pred_other_pts.append(list(np.transpose(dest_point)[:2]))                 # Update players tactical map coordiantes list\n",
    "        pred_other_pts = np.array(pred_other_pts)\n",
    "        other_pos.append(pred_other_pts)\n",
    "\n",
    "        pred_ball_pts = []                                                           # Initialize players tactical map coordiantes list\n",
    "        for pt in detected_ball_src_pts:                                            # Loop over players frame coordiantes\n",
    "            pt = np.append(np.array(pt), np.array([1]), axis=0)                     # Covert to homogeneous coordiantes\n",
    "            dest_point = np.matmul(h, np.transpose(pt))                              # Apply homography transofrmation\n",
    "            dest_point = dest_point/dest_point[2]                                   # Revert to 2D-coordiantes\n",
    "            pred_ball_pts.append(list(np.transpose(dest_point)[:2]))                 # Update players tactical map coordiantes list\n",
    "        pred_ball_pts = np.array(pred_ball_pts)\n",
    "        ball_pos.append(pred_ball_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e2e25-b075-4894-8ba0-b7cf85bf7be4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "top_left=(67, 36)\n",
    "bottom_right=(1321, 893)\n",
    "new_width = bottom_right[0] - top_left[0]\n",
    "new_height = bottom_right[1] - top_left[1]\n",
    "\n",
    "# Ball representation\n",
    "ball_image_path = 'pngegg.png'  # Path to the ball PNG image\n",
    "ball_image = cv2.imread(ball_image_path, cv2.IMREAD_UNCHANGED)  # Load with alpha channel\n",
    "ball_size = 20  # Diameter of the ball image, adjust as needed\n",
    "\n",
    "tac_map = cv2.imread('unnamed-chunk-2-1 (1).png')\n",
    "\n",
    "\n",
    "\n",
    "overlay_width = 400  # Adjust to your preference\n",
    "overlay_height = int(new_height * (overlay_width / new_width))\n",
    "\n",
    "# Setup the video capture\n",
    "vid = cv2.VideoCapture('./data/video/results-red_green6.mp4')\n",
    "vid_fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width, vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "pos_x = vid_width - overlay_width\n",
    "pos_y = vid_height - overlay_height\n",
    "# Setup the video writer with the size of the tactical map\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('./data/video/results-red_green_tacmap_overlay.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "frame_nbr=0\n",
    "\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    detected_labels=[obj['class'] for obj in keypoint_results[frame_nbr]]\n",
    "    if len(detected_labels) > 3:\n",
    "        tac_map_copy = tac_map.copy()\n",
    "\n",
    "        pred_player_pts=player_pos[frame_nbr]\n",
    "        pred_other_pts=other_pos[frame_nbr]\n",
    "        pred_ball_pts=ball_pos[frame_nbr]\n",
    "        \n",
    "        # Player representation\n",
    "        player_radius = 15  # Adjust the radius as needed\n",
    "        team1_color = (0, 0, 255)  # Red\n",
    "        team2_color = (0, 255, 0)  # Yellow\n",
    "        contour_color = (0, 0, 0)  # White color for contour\n",
    "        contour_thickness = 2  # Thickness of the white contour\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.7  # Adjust as needed based on your image size\n",
    "        font_color = (0, 0, 0)  # White color for text\n",
    "        font_thickness = 1\n",
    "        \n",
    "        for i, player in enumerate(pred_player_pts):\n",
    "            player_center = (int(player[0]), int(player[1]))\n",
    "            team_color = team1_color if player_results[frame_nbr][i]['class'] == 'team1' else team2_color\n",
    "        \n",
    "            # Draw the contour circle first (slightly larger)\n",
    "            cv2.circle(tac_map_copy, player_center, player_radius + contour_thickness, contour_color, thickness=-1)\n",
    "        \n",
    "            # Then draw the filled circle with the team color\n",
    "            cv2.circle(tac_map_copy, player_center, player_radius, team_color, -1)\n",
    "\n",
    "            # Get the player's ID and convert it to string\n",
    "            player_id = str(player_results[frame_nbr][i]['id'])\n",
    "        \n",
    "            # Calculate text size and position, then put the player's ID inside the circle\n",
    "            text_size = cv2.getTextSize(player_id, font, font_scale, font_thickness)[0]\n",
    "            text_x = player_center[0] - text_size[0] // 2\n",
    "            text_y = player_center[1] + text_size[1] // 2\n",
    "            cv2.putText(tac_map_copy, player_id, (text_x, text_y), font, font_scale, font_color, font_thickness)\n",
    "\n",
    "        other_radius = 15  # Adjust the radius as needed\n",
    "        ref_color = (255, 255, 0)  \n",
    "        gk_color = (255, 0, 0)  \n",
    "        contour_color = (255, 255, 255)  # White color for contour\n",
    "        contour_thickness = 2  # Thickness of the white contour\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.7  # Adjust as needed based on your image size\n",
    "        font_color = (0, 0, 0)  # White color for text\n",
    "        font_thickness = 1\n",
    "        \n",
    "        for i, other in enumerate(pred_other_pts):\n",
    "            other_center = (int(other[0]), int(other[1]))\n",
    "            team_color = ref_color if other_results[frame_nbr][i]['class'] == 'referee' else gk_color\n",
    "        \n",
    "            # Draw the contour circle first (slightly larger)\n",
    "            cv2.circle(tac_map_copy, other_center, other_radius + contour_thickness, contour_color, thickness=-1)\n",
    "        \n",
    "            # Then draw the filled circle with the team color\n",
    "            cv2.circle(tac_map_copy, other_center, other_radius, team_color, -1)\n",
    "\n",
    "            # Get the player's ID and convert it to string\n",
    "            id = 'R' if other_results[frame_nbr][i]['class'] == 'referee' else 'GK'\n",
    "        \n",
    "            # Calculate text size and position, then put the player's ID inside the circle\n",
    "            text_size = cv2.getTextSize(id, font, font_scale, font_thickness)[0]\n",
    "            text_x = other_center[0] - text_size[0] // 2\n",
    "            text_y = other_center[1] + text_size[1] // 2\n",
    "            cv2.putText(tac_map_copy, id, (text_x, text_y), font, font_scale, font_color, font_thickness)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        for ball in pred_ball_pts:  # Assuming pred_ball_pts is a list of [x, y] tuples\n",
    "            # Calculate the top-left corner for overlay\n",
    "            x, y = int(ball[0] - ball_size // 2), int(ball[1] - ball_size // 2)\n",
    "        \n",
    "            # Resize the ball image\n",
    "            ball_resized = cv2.resize(ball_image, (ball_size, ball_size))\n",
    "        \n",
    "            # Handle transparency if the ball image has an alpha channel\n",
    "            if ball_resized.shape[2] == 4:  # 4 channels: RGBA\n",
    "                alpha_mask = ball_resized[:, :, 3] / 255.0  # Normalized alpha channel\n",
    "                alpha_inv = 1.0 - alpha_mask\n",
    "        \n",
    "                for c in range(0, 3):  # RGB channels\n",
    "                    tac_map_copy[y:y+ball_size, x:x+ball_size, c] = \\\n",
    "                        alpha_inv * tac_map_copy[y:y+ball_size, x:x+ball_size, c] + \\\n",
    "                        alpha_mask * ball_resized[:, :, c]\n",
    "            else:\n",
    "                # If no alpha channel, overlay directly\n",
    "                tac_map_copy[y:y+ball_size, x:x+ball_size] = ball_resized\n",
    "\n",
    "    frame_nbr+=1\n",
    "\n",
    "    cropped_tac_map = tac_map_copy[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
    "    \n",
    "    tac_map_resized=cv2.resize(cropped_tac_map, (overlay_width, overlay_height))\n",
    "    img[pos_y:pos_y + overlay_height, pos_x:pos_x + overlay_width] = tac_map_resized\n",
    "    \n",
    "    out.write(img)\n",
    "\n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ac31b-4abd-4333-a511-9fac975abfe8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_green.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_green_keypoints.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "\n",
    "for frame in keypoint_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:     \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),(255,255,255), 2)\n",
    "\n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    (255,255,255), 2)\n",
    "   \n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06fcd5-3ba1-4aa5-ab3a-90eea7420b0d",
   "metadata": {},
   "source": [
    "## Ball possession with inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48e93f-7d58-40e9-80db-1888095151d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_center(box):\n",
    "    \"\"\"Calculate the center of a bounding box.\"\"\"\n",
    "    x_center = (box[0] + box[2]) / 2\n",
    "    y_center = (box[1] + box[3]) / 2\n",
    "    return np.array([x_center, y_center])\n",
    "\n",
    "def calculate_foot_positions(player_box):\n",
    "    \"\"\"Calculate the positions of the left and right foot for a player.\"\"\"\n",
    "    left_foot = np.array([player_box[0], player_box[3]])  # Bottom left corner\n",
    "    right_foot = np.array([player_box[2], player_box[3]])  # Bottom right corner\n",
    "    return left_foot, right_foot\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return np.linalg.norm(point1 - point2)\n",
    "\n",
    "def find_closest_player(ball, players):\n",
    "    \"\"\"Determine which player is closest to the ball.\"\"\"\n",
    "    ball_center = calculate_center(ball['box'])\n",
    "    min_distance = float('inf')\n",
    "    closest_player_id = None\n",
    "\n",
    "    for i in range(len(players)):\n",
    "        player_box=players[i]['box']\n",
    "        left_foot, right_foot = calculate_foot_positions(player_box)\n",
    "        distance_left = calculate_distance(left_foot, ball_center)\n",
    "        distance_right = calculate_distance(right_foot, ball_center)\n",
    "        min_distance_foot = min(distance_left, distance_right)\n",
    "\n",
    "        if min_distance_foot < min_distance:\n",
    "            min_distance = min_distance_foot\n",
    "            closest_player_id = i\n",
    "\n",
    "    return closest_player_id, min_distance\n",
    "\n",
    "distance_threshold = 50  # Define a threshold for possession\n",
    "possession=[]\n",
    "for i in range(len(player_results)):\n",
    "    if ball_results[i]:\n",
    "        closest_player_id, min_distance = find_closest_player(ball_results[i][0], player_results[i])\n",
    "    else:\n",
    "        closest_player_id=None\n",
    "    if closest_player_id is not None and min_distance < distance_threshold:\n",
    "     possession.append(player_results[i][closest_player_id])  \n",
    "    else:\n",
    "     possession.append(None)\n",
    "\n",
    "possession_team=[]\n",
    "for player in possession:\n",
    "    if player==None:\n",
    "        possession_team.append(None)\n",
    "    else:\n",
    "        possession_team.append(player['class'])\n",
    "\n",
    "print(possession_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1255399-015c-4c2d-b356-c300339a80f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inertia_threshold = 20  # Number of consecutive frames required to confirm a change in possession\n",
    "\n",
    "def determine_possession_with_inertia(possession_team, inertia_threshold):\n",
    "    final_possession = []  # To store the processed possession status with inertia applied\n",
    "    current_possession = None  # Track the current possession status\n",
    "    consecutive_count = 0  # Count consecutive frames for the current team\n",
    "    last_known_possession = None  # Keep track of the last non-None possession\n",
    "\n",
    "    for team in possession_team:\n",
    "        if team is not None:\n",
    "            if team == current_possession or current_possession is None:\n",
    "                # Update current possession if it's the first team possession detected or matches the current possession\n",
    "                current_possession = team\n",
    "                last_known_possession = team\n",
    "                consecutive_count = 1  # Start or reset the count\n",
    "            else:\n",
    "                # If there's a potential change in possession\n",
    "                if consecutive_count < inertia_threshold:\n",
    "                    # If below threshold, count the consecutive frame but don't change possession yet\n",
    "                    consecutive_count += 1\n",
    "                else:\n",
    "                    # Change possession if the count exceeds the threshold, and reset count\n",
    "                    current_possession = team\n",
    "                    last_known_possession = team\n",
    "                    consecutive_count = 1\n",
    "        else:\n",
    "            # If team is None, we just increment the consecutive count if already counting,\n",
    "            # otherwise, we keep the last known possession\n",
    "            if consecutive_count > 0 and consecutive_count < inertia_threshold:\n",
    "                consecutive_count += 1\n",
    "            else:\n",
    "                current_possession = last_known_possession  # Use the last known possession during None periods\n",
    "        \n",
    "        # Append the current or last known possession status to the final list\n",
    "        final_possession.append(current_possession)\n",
    "    \n",
    "    return final_possession\n",
    "\n",
    "# Process the possession list with inertia\n",
    "processed_possession = determine_possession_with_inertia(possession_team, inertia_threshold)\n",
    "\n",
    "for frame in processed_possession:\n",
    "    if frame is not None:\n",
    "        break\n",
    "team=frame\n",
    "for i, frame in enumerate(processed_possession):\n",
    "    if frame is not None:\n",
    "        break\n",
    "    processed_possession[i]=team\n",
    "# Example output\n",
    "for i, team in enumerate(processed_possession):\n",
    "    print(f\"Frame {i}: {team}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cc1e3-8b2d-405e-810e-b6f72fcee846",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "possession_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942338c3-eff3-4cf4-8dd9-d739fb53d68e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "possession_team = processed_possession\n",
    "possession_res=[None]*len(possession)\n",
    "for i,player in enumerate(possession):\n",
    "    if player is not None:\n",
    "        possession_res[i]=(player['class'], player['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207fa64-12ec-46fd-b8c1-daf463db729b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i,player in enumerate(possession_res):\n",
    "    if player is not None:\n",
    "        if player[0]!=processed_possession[i]:\n",
    "            possession_res[i]= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a757f1d-b5fc-4de7-87f2-0fa7eb366a96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i,player in enumerate(possession_res):\n",
    "    if player is None:\n",
    "        team=processed_possession[i]\n",
    "        players= [player for player in player_results[i] if player['class']==team]\n",
    "        if ball_results[i]:\n",
    "            closest_player_id, _ = find_closest_player(ball_results[i][0], players)\n",
    "            possession_res[i]=(team,closest_player_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560e0db-f8f7-41e2-94b4-6883717f433b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_passes(data):\n",
    "    passes = []\n",
    "\n",
    "    for i in range(1,len(data)):\n",
    "        # Check for None values to skip them\n",
    "        if data[i] is not None and data[i-1] is not None:\n",
    "            current_possession, current_play = data[i]\n",
    "            previous_possession, previous_play = data[i-1]\n",
    "\n",
    "            if current_possession == previous_possession and current_play != previous_play:\n",
    "\n",
    "                pass_event = {\n",
    "                    \"sender\": data[i-1],\n",
    "                    \"receiver\": data[i],\n",
    "                    \"frame\": i\n",
    "                }\n",
    "                passes.append(pass_event)\n",
    "\n",
    "    return(passes)\n",
    "\n",
    "passes = detect_passes(possession_res)\n",
    "passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb0863-f900-458b-a470-68ab9cab85b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'passes' is your list of pass events and it's already generated\n",
    "\n",
    "# Load your video\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_yellow_test1.mp4\")\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_yellow_pass.mp4\", fourcc, fps, (width + 200, height))\n",
    "\n",
    "# Initialize a variable to keep track of the text position for each pass\n",
    "text_start_y = 10\n",
    "\n",
    "# Load a font\n",
    "font = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "\n",
    "for i in range(len(passes)):\n",
    "    passes[i][\"text_position\"] = text_start_y\n",
    "    text_start_y += 30  # Adjust based on font size and desired spacing\n",
    "\n",
    "current_pass_index = 0\n",
    "\n",
    "# Process the video\n",
    "frame_number = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Create a black column to the right\n",
    "    right_column = np.zeros((height, 200, 3), dtype=np.uint8)\n",
    "    combined_frame = np.hstack((frame, right_column))\n",
    "    \n",
    "\n",
    "    # Check if the current frame is a pass frame and draw the text\n",
    "    while current_pass_index < len(passes) and frame_number == passes[current_pass_index][\"frame\"]:\n",
    "        pass_info = passes[current_pass_index]\n",
    "        text = f\"Successful pass from player {pass_info['sender']} to player {pass_info['receiver']}\"\n",
    "        draw.text((width + 10, pass_info[\"text_position\"]), text, font=font, fill=(255, 255, 255))\n",
    "\n",
    "        cv2.putText(combined_frame,text, (width + 10, pass_info[\"text_position\"]), 0, 0.75,\n",
    "                    (255, 255, 255), 2)\n",
    "        \n",
    "        current_pass_index += 1\n",
    "\n",
    "\n",
    "\n",
    "    out.write(combined_frame)\n",
    "    frame_number += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Video processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb289809-924c-4aa8-9d98-75fdea828b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_center(box):\n",
    "    \"\"\"Calculate the center of a bounding box.\"\"\"\n",
    "    x_center = (box[0] + box[2]) / 2\n",
    "    y_center = (box[1] + box[3]) / 2\n",
    "    return np.array([x_center, y_center])\n",
    "\n",
    "def calculate_foot_positions(player_box):\n",
    "    \"\"\"Calculate the positions of the left and right foot for a player.\"\"\"\n",
    "    left_foot = np.array([player_box[0], player_box[3]])  # Bottom left corner\n",
    "    right_foot = np.array([player_box[2], player_box[3]])  # Bottom right corner\n",
    "    return left_foot, right_foot\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return np.linalg.norm(point1 - point2)\n",
    "\n",
    "def find_closest_player(ball, players):\n",
    "    \"\"\"Determine which player is closest to the ball.\"\"\"\n",
    "    ball_center = calculate_center(ball['box'])\n",
    "    min_distance = float('inf')\n",
    "    closest_player = None\n",
    "\n",
    "    for player in players:\n",
    "        player_box = player['box']\n",
    "        left_foot, right_foot = calculate_foot_positions(player_box)\n",
    "        distance_left = calculate_distance(left_foot, ball_center)\n",
    "        distance_right = calculate_distance(right_foot, ball_center)\n",
    "        min_distance_foot = min(distance_left, distance_right)\n",
    "\n",
    "        if min_distance_foot < min_distance:\n",
    "            min_distance = min_distance_foot\n",
    "            closest_player = player\n",
    "\n",
    "    return closest_player, min_distance\n",
    "\n",
    "distance_threshold = 50  # Define a threshold for possession\n",
    "\n",
    "def process_possession(player_results, ball_results, distance_threshold):\n",
    "    possession = []\n",
    "    for i in range(len(player_results)):\n",
    "        if ball_results[i]:\n",
    "            closest_player, min_distance = find_closest_player(ball_results[i][0], player_results[i])\n",
    "        else:\n",
    "            closest_player = None\n",
    "        if closest_player is not None and min_distance < distance_threshold:\n",
    "            possession.append(closest_player)  \n",
    "        else:\n",
    "            possession.append(None)\n",
    "    return possession\n",
    "\n",
    "def determine_possession_with_inertia(possession, inertia_threshold):\n",
    "    final_possession = []  # To store the processed possession status with inertia applied\n",
    "    current_possession = None  # Track the current possession status (team and player)\n",
    "    consecutive_count = 0  # Count consecutive frames\n",
    "    last_known_possession = None  # Keep track of the last non-None possession\n",
    "\n",
    "    for possession_info in possession:\n",
    "        if possession_info is not None:\n",
    "            team = possession_info['class']\n",
    "            player_id = possession_info['id']\n",
    "            if (current_possession is None) or (team == current_possession['team']):\n",
    "                current_possession = {'team': team, 'player_id': player_id}\n",
    "                last_known_possession = current_possession\n",
    "                consecutive_count = 1  # Reset count\n",
    "            else:\n",
    "                if consecutive_count < inertia_threshold:\n",
    "                    consecutive_count += 1\n",
    "                else:\n",
    "                    current_possession = {'team': team, 'player_id': player_id}\n",
    "                    last_known_possession = current_possession\n",
    "                    consecutive_count = 1\n",
    "        else:\n",
    "            if consecutive_count > 0 and consecutive_count < inertia_threshold:\n",
    "                consecutive_count += 1\n",
    "            else:\n",
    "                current_possession = last_known_possession\n",
    "        \n",
    "        final_possession.append(current_possession)\n",
    "    \n",
    "    # Fill initial None values with the first known possession\n",
    "    first_known_possession = next((item for item in final_possession if item is not None), None)\n",
    "    for i, possession_info in enumerate(final_possession):\n",
    "        if possession_info is None and first_known_possession is not None:\n",
    "            final_possession[i] = first_known_possession\n",
    "    \n",
    "    return final_possession\n",
    "\n",
    "# Example usage\n",
    "inertia_threshold = 20  # Number of consecutive frames required to confirm a change in possession\n",
    "# Assuming player_results and ball_results are already defined\n",
    "processed_possession = process_possession(player_results, ball_results, distance_threshold)\n",
    "processed_possession_with_inertia = determine_possession_with_inertia(processed_possession, inertia_threshold)\n",
    "\n",
    "processed_possession_with_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561c1bd-e9cb-4921-948d-b61e0fa42797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_passes(processed_possession):\n",
    "    passes = []  # Store details of detected passes\n",
    "    pass_start_frame = None  # Track the start frame of a potential pass\n",
    "    for i in range(1, len(processed_possession)):\n",
    "        current_possession = processed_possession[i]\n",
    "        previous_possession = processed_possession[i-1]\n",
    "\n",
    "        # Check for possession change\n",
    "        if current_possession is not None and previous_possession is not None:\n",
    "            if current_possession['player_id'] != previous_possession['player_id']:\n",
    "                # If the team is the same, it's a pass within the team\n",
    "                if current_possession['team'] == previous_possession['team']:\n",
    "                    passes.append({\n",
    "                        'team': previous_possession['team'],\n",
    "                        'sender': previous_possession['player_id'],\n",
    "                        'receiver': current_possession['player_id'],\n",
    "                        'frame': i  # Assuming 'i' is the frame index\n",
    "                    })\n",
    "                # If the team changes, it could be a pass intercepted or a lost ball recovered\n",
    "                else:\n",
    "                    # This logic can be adjusted based on how you want to handle interceptions or recoveries\n",
    "                    # For simplicity, we're considering them as passes in this example\n",
    "                    passes.append({\n",
    "                        'sender': previous_possession['player_id'],\n",
    "                        'receiver': current_possession['player_id'],\n",
    "                        'frame': i  # Assuming 'i' is the frame index\n",
    "                    })\n",
    "\n",
    "    return passes\n",
    "\n",
    "# Assuming processed_possession_with_inertia contains the possession data with inertia processed\n",
    "passes_detected = detect_passes(processed_possession_with_inertia)\n",
    "# passes_detected=passes_detected[2::]\n",
    "passes_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80775498-7226-4d3f-baef-9299bce8cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "\"team 1\": (0, 0, 255),    \n",
    "\"team 2\": (255, 255, 255),    \n",
    "\"referee\": (255,255,0), \n",
    "\"goalkeeper\": (255, 0, 0), \n",
    "\"ball\": (255, 255, 255)  \n",
    "}\n",
    "\n",
    "\n",
    "def annotate_video_with_passes(input_video_path, output_video_path, passes_detected):\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Prepare output video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Adjust according to your video codec\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # To keep track of which frame to annotate\n",
    "    frames_to_annotate = {}\n",
    "\n",
    "    current_frame = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Check if the current frame needs annotation\n",
    "        if current_frame in frames_to_annotate:\n",
    "            (text , color) = frames_to_annotate[current_frame]\n",
    "            cv2.putText(frame, text, (50, frame_height - 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        \n",
    "        # Update frames_to_annotate for keeping the text for 20 frames\n",
    "        for pass_info in passes_detected:\n",
    "            if pass_info['frame'] == current_frame:\n",
    "                text = f\"Pass: {pass_info['sender']} to {pass_info['receiver']}\"\n",
    "                color=color_dict[pass_info['team']]\n",
    "                for i in range(20):  # Show text for 20 frames\n",
    "                    frames_to_annotate[current_frame + i] = (text , color)\n",
    "        \n",
    "        # Write the frame\n",
    "        out.write(frame)\n",
    "        \n",
    "        current_frame += 1\n",
    "\n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Assuming `passes_detected` is your list of pass dictionaries and includes 'frame' key\n",
    "input_video_path = r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white_test1.mp4\"\n",
    "output_video_path = r\"C:\\Users\\Fadi\\Desktop\\Tracker\\data\\video\\red_white_pass.mp4\"\n",
    "\n",
    "annotate_video_with_passes(input_video_path, output_video_path, passes_detected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052c69d-9033-47fc-bbbd-b80f031caeaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_passes_v1(possession, possession_team):\n",
    "    passes = []  # List to store the details of detected passes\n",
    "    current_pass = {'sender': None, 'receiver': None, 'frame': None}\n",
    "    last_player_in_possession = None\n",
    "    last_team_in_possession = None\n",
    "\n",
    "    for frame_number, (player, team) in enumerate(zip(possession, possession_team)):\n",
    "        # Check for possession continuity within the same team\n",
    "        if team == last_team_in_possession and player is not None and player!= last_player_in_possession:\n",
    "            if last_player_in_possession is not None:\n",
    "                # A pass is detected\n",
    "                current_pass['sender'] = last_player_in_possession\n",
    "                current_pass['receiver'] = player\n",
    "                current_pass['frame'] = frame_number\n",
    "                # Save the pass and prepare for a potential new pass\n",
    "                passes.append(current_pass)\n",
    "                current_pass = {'sender': None, 'receiver': None, 'frame': None}\n",
    "            last_player_in_possession = player\n",
    "        elif team != last_team_in_possession:\n",
    "            # Reset for a new team possession\n",
    "            last_player_in_possession = player\n",
    "            current_pass = {'sender': None, 'receiver': None, 'frame': None}\n",
    "        \n",
    "        last_team_in_possession = team\n",
    "\n",
    "    return passes\n",
    "\n",
    "\n",
    "passes = detect_passes(possession, possession_team)\n",
    "\n",
    "for p in passes:\n",
    "    print(f\"Pass from {p['sender']} to {p['receiver']}, frame {p['frame']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7defb-2691-4c25-b768-4336b8d6d765",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_players_who_lost_ball(possession, possession_team):\n",
    "    ball_loss_events = []  # To store the players who lost the ball and the frame it happened\n",
    "    \n",
    "    for frame_number in range(1, len(possession_team)):\n",
    "        # Check for a possession change between teams\n",
    "        if possession_team[frame_number] != possession_team[frame_number - 1] and possession_team[frame_number - 1] is not None:\n",
    "            # Possession changed, backtrack to find the last player of the previous team in possession\n",
    "            for back_frame in range(frame_number, 0, -1):\n",
    "                if possession[back_frame - 1] is not None:  # Ensure there is a player in possession\n",
    "                    team, player_id = possession[back_frame - 1]\n",
    "                    if team == possession_team[frame_number - 1]:  # Check if the player belongs to the team that lost the ball\n",
    "                        # Found the player who lost the ball\n",
    "                        ball_loss_events.append({'player': (team, player_id), 'frame': frame_number})\n",
    "                        break\n",
    "    \n",
    "    return ball_loss_events\n",
    "\n",
    "ball_loss_events = find_players_who_lost_ball(possession, possession_team)\n",
    "\n",
    "for event in ball_loss_events:\n",
    "    print(f\"Player from {event['player'][0]} with ID {event['player'][1]} lost the ball on frame {event['frame']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a28717-a3ea-4901-bdea-7f59bede1405",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def draw_rounded_box(img, box, color, thickness=1, radius=5):\n",
    "    \"\"\"\n",
    "    Draw a bounding box with rounded corners.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The image on which to draw.\n",
    "    - box: The bounding box coordinates as (x_min, y_min, x_max, y_max).\n",
    "    - color: The color of the bounding box (B, G, R).\n",
    "    - thickness: The thickness of the lines.\n",
    "    - radius: The radius of the rounded corners.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "\n",
    "    # Top left corner\n",
    "    cv2.line(img, (x_min + radius, y_min), (x_max - radius, y_min), color, thickness)\n",
    "    cv2.line(img, (x_min, y_min + radius), (x_min, y_max - radius), color, thickness)\n",
    "    cv2.circle(img, (x_min + radius, y_min + radius), radius, color, thickness)\n",
    "\n",
    "    # Top right corner\n",
    "    cv2.line(img, (x_max - radius, y_min), (x_max, y_min + radius), color, thickness)\n",
    "    cv2.circle(img, (x_max - radius, y_min + radius), radius, color, thickness)\n",
    "\n",
    "    # Bottom left corner\n",
    "    cv2.line(img, (x_min + radius, y_max), (x_max - radius, y_max), color, thickness)\n",
    "    cv2.line(img, (x_min, y_min + radius), (x_min, y_max - radius), color, thickness)\n",
    "    cv2.circle(img, (x_min + radius, y_max - radius), radius, color, thickness)\n",
    "\n",
    "    # Bottom right corner\n",
    "    cv2.line(img, (x_max, y_min + radius), (x_max, y_max - radius), color, thickness)\n",
    "    cv2.circle(img, (x_max - radius, y_max - radius), radius, color, thickness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
