{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce7b83e-6674-42c4-8411-b5a742073f25",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "running detection model with low confidence threshold (0.3)\n",
    "\n",
    "getting rid of detections other than ball under confidence threshhold (0.6)\n",
    "\n",
    "\n",
    "running tracking models on it\n",
    "\n",
    "\n",
    "converting to cvat format\n",
    "\n",
    "adding missing ball detections manually with cvat (+ fixing other detection and tracking results)\n",
    "\n",
    "exporting the annotations\n",
    "\n",
    "processing results: team detection + ball possession\n",
    "\n",
    "plotting the tracking data on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15eb3c5-6802-410e-9209-3ed873041e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49548741-5768-4354-a635-9108e4ca7fc4",
   "metadata": {},
   "source": [
    "# Deep-Sort Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392b731-e233-4f12-8603-89a31399efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort import preprocessing\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "from tools import generate_detections as gdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736b4b2-b41f-4f50-bac4-d79f17ff255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_boxes(image, boxes):\n",
    "    returned_boxes = []\n",
    "    for box in boxes:\n",
    "        box[0] = (box[0] * image.shape[1]).astype(int)\n",
    "        box[1] = (box[1] * image.shape[0]).astype(int)\n",
    "        box[2] = (box[2] * image.shape[1]).astype(int)\n",
    "        box[3] = (box[3] * image.shape[0]).astype(int)\n",
    "        box[2] = int(box[2]-box[0])\n",
    "        box[3] = int(box[3]-box[1])\n",
    "        box = box.astype(int)\n",
    "        box = box.tolist()\n",
    "        if box != [0,0,0,0]:\n",
    "            returned_boxes.append(box)\n",
    "    return returned_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3a1c2-cf83-4d65-9f5c-aa8fd8f1d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cosine_distance = 0.5\n",
    "nn_budget = None\n",
    "nms_max_overlap = 0.8\n",
    "\n",
    "model_filename = 'deep_sort/mars-small128.pb'\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric('cosine', max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e1d79a-0355-4c7e-ac79-ac412c92f258",
   "metadata": {},
   "source": [
    "### Capture video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28a059-53ee-450e-ba05-799270b6c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/foot-5frames.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-5frames.mp4', codec, vid_fps, (vid_width, vid_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20c944-170b-4a72-8618-63ae6c248e30",
   "metadata": {},
   "source": [
    "### Tracking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5471b-d92f-427e-8ca2-3f291e132c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_data = []\n",
    "i=1\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "    img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    boxes, scores, names= detect_fn(img)\n",
    "\n",
    "    allowed_classes=['player']\n",
    "    deleted_indx = []\n",
    "    for j in range(len(boxes)):\n",
    "        if not (names[j] in allowed_classes):\n",
    "            deleted_indx.append(j)\n",
    "    boxes = np.delete(boxes, deleted_indx, axis=0)\n",
    "    names = np.delete(names, deleted_indx, axis=0)\n",
    "    scores = np.delete(scores, deleted_indx, axis=0)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "    converted_boxes = convert_boxes(img, boxes)\n",
    "    features = encoder(img, converted_boxes)\n",
    "\n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in\n",
    "                   zip(converted_boxes, scores, names, features)]\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "\n",
    "    cmap = plt.get_cmap('tab20b')\n",
    "    colors = [cmap(i)[:3] for i in np.linspace(0,1,20)]\n",
    "\n",
    "    current_count = int(0)\n",
    "\n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "        \n",
    "        bbox = track.to_tlbr()\n",
    "        class_name= track.get_class()\n",
    "        color = colors[int(track.track_id) % len(colors)]\n",
    "        color = [i * 255 for i in color]\n",
    "\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])), 2)\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)\n",
    "                    +len(str(track.track_id)))*17, int(bbox[1])), -1)\n",
    "        cv2.putText(img, class_name+\"-\"+str(track.track_id), (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    (255, 255, 255), 2)\n",
    "\n",
    "        object_data = {\n",
    "            \"box\": track.to_tlbr(),\n",
    "            \"id\": track.track_id,\n",
    "            \"class\": track.get_class()\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_frames_data.append(frame_objects)\n",
    "        \n",
    "    fps = 1./(time.time()-t1)\n",
    "    cv2.putText(img, \"FPS: {:.2f}\".format(fps), (0,30), 0, 1, (0,0,255), 2)\n",
    "    cv2.resizeWindow('output', 1024, 768)\n",
    "    cv2.imshow('output', img)\n",
    "    out.write(img)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "vid.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658402f-b848-4b03-838c-8aaf21671ab9",
   "metadata": {},
   "source": [
    "### Display video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8f8c5-56f7-4dbd-a7a7-eddcddd6a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_frames(video_path):\n",
    "    # Capture the video from the file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Read until video is completed\n",
    "    while cap.isOpened():\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_count += 1\n",
    "            # Convert the frame from BGR (OpenCV default) to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            plt.figure(figsize=(20,20))\n",
    "            # Display the resulting frame\n",
    "            plt.imshow(frame_rgb)\n",
    "            plt.title(f\"Frame {frame_count}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c4cc4-d5c7-486b-894a-914634a4a4e5",
   "metadata": {},
   "source": [
    "# Color filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b9121-ce30-40f4-8a05-545cbe678998",
   "metadata": {},
   "source": [
    "### Clustering by average color of player region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc04910-b194-4f9c-a115-de8bb4446cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=res\n",
    "colors=[]\n",
    "for i in range(len(frame)):\n",
    "    object=frame[i]\n",
    "    bbox=object['box']\n",
    "    bbox_int = bbox.astype(int)\n",
    "    # Extract each coordinate\n",
    "    xmin, ymin, xmax, ymax = bbox_int\n",
    "    # Extract the region of interest (ROI) using integer coordinates\n",
    "    player_region = img[ymin:ymax, xmin:xmax]\n",
    "    average_color = player_region.mean(axis=(0, 1))\n",
    "    colors.append(average_color)\n",
    "    plt.imshow(player_region)\n",
    "    plt.show()\n",
    "colors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e2c8e-08fa-4679-8b1c-1b661bf6964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Assuming 'colors' is a list of average color values of players\n",
    "# Each color is a list or array of [R, G, B] values\n",
    "colors = np.array(colors)  # Convert to NumPy array for sklearn\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=2, n_init=10, random_state=0).fit(colors)\n",
    "labels = kmeans.labels_  # Get cluster labels for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d8ddb-dadf-4840-9b76-48d240e1cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(frame)):\n",
    "    object=frame[i]\n",
    "    label=labels[i]\n",
    "    bbox=object['box']\n",
    "    bbox_int = bbox.astype(int)\n",
    "    # Extract each coordinate\n",
    "    xmin, ymin, xmax, ymax = bbox_int\n",
    "    # Extract the region of interest (ROI) using integer coordinates\n",
    "    player_region = img[ymin:ymax, xmin:xmax]\n",
    "    print(label)\n",
    "    plt.imshow(player_region)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4a277-bb2a-421d-9371-ce3a5adfbda4",
   "metadata": {},
   "source": [
    "### Applying color masks to filer green color of grass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab92348-c162-42b3-b041-bfea28c9ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv2.imread('/Users/fadijemmali/Desktop/Tracker/data/video/frames/image1.png')\n",
    "\n",
    "# Convert to HSV color space\n",
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define range of green color in HSV\n",
    "lower_green = np.array([40,40, 40])\n",
    "upper_green = np.array([70, 255, 255])\n",
    "\n",
    "# Create a binary mask for green color\n",
    "mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "# Optional: Apply the mask to the image (setting green areas to black)\n",
    "res = cv2.bitwise_and(image, image, mask=~mask)\n",
    "res_bgr = cv2.cvtColor(res,cv2.COLOR_HSV2BGR)\n",
    "res_gray = cv2.cvtColor(res,cv2.COLOR_BGR2GRAY)\n",
    "kernel = np.ones((13,13),np.uint8)\n",
    "thresh = cv2.threshold(res_gray,127,255,cv2.THRESH_BINARY_INV |  cv2.THRESH_OTSU)[1]\n",
    "thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "# Now, when processing player regions, use the mask to exclude green areas\n",
    "plt.imshow(res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b326b-6427-4902-91c8-158e166eff7d",
   "metadata": {},
   "source": [
    "# Other utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa13c1f-835c-4066-931e-4762f9dc3531",
   "metadata": {},
   "source": [
    "### Drawing circles under players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac45934-0e1e-4e16-8d60-4751f222f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw an ellipse under detected objects\n",
    "def draw_ellipse(image, box, color, thickness=4):\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    bottom_center = (int((x_min + x_max) / 2), int(y_max))\n",
    "    axes = (int((x_max - x_min) / 2), int(0.35 * (x_max - x_min)))\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, -45, 235, color, thickness, cv2.LINE_4)\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, 0, 360, color, thickness)\n",
    "    cv2.ellipse(\n",
    "        image,\n",
    "        center=bottom_center,\n",
    "        axes=axes,\n",
    "        angle=0.0,\n",
    "        startAngle=-45,\n",
    "        endAngle=235,\n",
    "        color=color,\n",
    "        thickness=thickness,\n",
    "        lineType=cv2.LINE_4\n",
    "    )\n",
    "# Color definitions for different objects\n",
    "COLORS = {\n",
    "    \"ball\": (255, 255, 255),      # White\n",
    "    \"goalkeeper\": (133, 1, 1),    # Dark Red\n",
    "    \"player\": (0, 212, 187),      # Greenish\n",
    "    \"referee\": (0, 255, 255)      # Yellow\n",
    "}\n",
    "THICKNESS = 4\n",
    "\n",
    "\n",
    "for object in frame:\n",
    "    box = object[\"box\"]\n",
    "    class_name = object[\"class\"]\n",
    "    color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "    draw_ellipse(img, box, color, THICKNESS)\n",
    "\n",
    "# Display the frame\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8e730-7029-4921-94f7-8531ed3d7ac9",
   "metadata": {},
   "source": [
    "### Detecting ball possesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78659df-8786-4663-8767-bb02ca2d719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_IN_POSSESSION_PROXIMITY = 30\n",
    "MARKER_WIDTH = 10\n",
    "MARKER_HEIGHT = 10\n",
    "MARKER_MARGIN = 20\n",
    "PLAYER_MARKER_COLOR = (0, 0, 255)  # Red in BGR\n",
    "BALL_MARKER_COLOR = (0, 255, 0)  # Green in BGR\n",
    "MARKER_CONTOUR_COLOR = (0, 0, 0)  # Black in BGR\n",
    "MARKER_CONTOUR_THICKNESS = 1\n",
    "\n",
    "def calculate_marker(anchor):\n",
    "    x, y = anchor\n",
    "    return np.array([\n",
    "        [x - MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN],\n",
    "        [x, y - MARKER_MARGIN],\n",
    "        [x + MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN]\n",
    "    ], np.int32)\n",
    "\n",
    "def draw_marker(image, anchor, color):\n",
    "    marker_contour = calculate_marker(anchor)\n",
    "    cv2.fillPoly(image, [marker_contour], color)\n",
    "    cv2.polylines(image, [marker_contour], isClosed=True, color=MARKER_CONTOUR_COLOR, thickness=MARKER_CONTOUR_THICKNESS)\n",
    "\n",
    "def get_player_in_possession(players, ball):\n",
    "    if len(ball) != 1:\n",
    "        return None\n",
    "    ball_x, ball_y = ball[0]['center']\n",
    "    for player in players:\n",
    "        player_x, player_y = player['center']\n",
    "        if abs(player_x - ball_x) <= PLAYER_IN_POSSESSION_PROXIMITY and abs(player_y - ball_y) <= PLAYER_IN_POSSESSION_PROXIMITY:\n",
    "            return player\n",
    "\n",
    "for object in frame:\n",
    "    x_min, y_min, x_max, y_max = object['box']\n",
    "    object['center']=[(x_min+x_max)//2, (y_min+y_max)//2]\n",
    "\n",
    "players = [det for det in frame if det['class'] == 'player']\n",
    "ball = [det for det in frame if det['class'] == 'ball']\n",
    "player_in_possession = get_player_in_possession(players, ball)\n",
    "img2=img.copy()\n",
    "if player_in_possession:\n",
    "    draw_marker(img2, player_in_possession['center'], PLAYER_MARKER_COLOR)\n",
    "for b in ball:\n",
    "    draw_marker(img2, b['center'], BALL_MARKER_COLOR)\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6da2f-8710-478e-a56b-93d71c1e8d7b",
   "metadata": {},
   "source": [
    "# Getting detections from tensorflow model (Fast-RCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e6160-22a4-4f07-8e80-4532646ea23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fab1d2-2b65-435f-a96f-714d8c8af0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='/Users/fadijemmali/Desktop/Tracker/faster_rcnn_inception_resnet_v2_1024x1024/export'\n",
    "configs = config_util.get_configs_from_pipeline_file(os.path.join(model_path,'pipeline.config'))\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "\n",
    "# Load the latest checkpoint\n",
    "latest_ckpt = tf.train.latest_checkpoint(os.path.join(model_path,'checkpoint'))\n",
    "if latest_ckpt:\n",
    "    ckpt.restore(latest_ckpt).expect_partial()\n",
    "    print(f\"Checkpoint loaded: {latest_ckpt}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No checkpoint found in\", os.path.join(model_path,'checkpoint'))\n",
    "\n",
    "@tf.function\n",
    "def rcnn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2b214-7e78-4ed7-a368-b5a551cf641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index ={1: {'id': 1, 'name': 'ball'},\n",
    "                 2: {'id': 2, 'name': 'goalkeeper'},  \n",
    "                 3: {'id': 3, 'name': 'player'},\n",
    "                 4: {'id': 4, 'name': 'referee'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76822eca-429d-4ef9-805f-1a2316cfd15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections = rcnn(input_tensor)\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "            for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'],\n",
    "          detections['detection_classes']+label_id_offset,\n",
    "          detections['detection_scores'],\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          min_score_thresh=.6,\n",
    "          agnostic_mode=False)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00b0d4-aca9-43d9-9e4c-9764a89be946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fn(img, threshold=0.1):\n",
    "    image_np = np.array(img)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = rcnn(input_tensor)\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    \n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    \n",
    "    boxes = detections['detection_boxes']\n",
    "    boxes = boxes[:, [1, 0, 3, 2]]  # Rearranging box coordinates\n",
    "    scores = detections['detection_scores']\n",
    "    classes = detections['detection_classes']\n",
    "\n",
    "    ## Filter out detections with scores less than the threshold\n",
    "    keep = scores > threshold\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    classes = classes[keep]\n",
    "\n",
    "    # Process class names\n",
    "    class_names = [category_index[class_id + label_id_offset]['name'] for class_id in classes]\n",
    "    \n",
    "    return boxes, scores, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ddae8-6779-4632-b4e8-0aa6edc49026",
   "metadata": {},
   "source": [
    "# Funtion to detect player team color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712862d-a064-4015-a67a-f2825f8ccf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nonblack_np(img):\n",
    "    \"\"\"Return the number of pixels in img that are not black.\n",
    "    img must be a Numpy array with colour values along the last axis.\n",
    "\n",
    "    \"\"\"\n",
    "    return img.any(axis=-1).sum()\n",
    "def red(image):\n",
    "    \n",
    "    # Convert the image from BGR to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define lower and upper range of yellow in HSV\n",
    "    lower_red1 = np.array([0, 120, 70])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 120, 70])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "   \n",
    "    mask_red1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "    \n",
    "    # Combine masks\n",
    "    full_mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "    red_regions = cv2.bitwise_and(image, image, mask=full_mask_red)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(red_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    \n",
    "    return ratio\n",
    "    \n",
    "def green(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([40, 40, 40])  # Lower bound of light green\n",
    "    upper_green = np.array([70, 255, 255])  # Upper bound of light green\n",
    "    \n",
    "    # Create masks for the yellow range\n",
    "    mask_green = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "    \n",
    "    green_regions = cv2.bitwise_and(image, image, mask=mask_green)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(green_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    return ratio\n",
    "def yellow(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([30, 255, 255])\n",
    "    \n",
    "    # Create masks for the yellow range\n",
    "    mask_yellow = cv2.inRange(hsv_image, lower_yellow, upper_yellow)\n",
    "    \n",
    "    yellow_regions = cv2.bitwise_and(image, image, mask=mask_yellow)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(yellow_regions)\n",
    "    ratio = color_pix/tot_pix\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "def white(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the range for white color\n",
    "    lower_white = np.array([0, 0, 200])\n",
    "    upper_white = np.array([180, 55, 255])\n",
    "\n",
    "    # Create masks for the white range\n",
    "    mask_white = cv2.inRange(hsv_image, lower_white, upper_white)\n",
    "    \n",
    "    white_regions = cv2.bitwise_and(image, image, mask=mask_white)\n",
    "    \n",
    "    tot_pix = count_nonblack_np(image)\n",
    "    color_pix = count_nonblack_np(white_regions)\n",
    "    ratio = color_pix / tot_pix\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6f1ed-262f-4a23-992c-f30288d5d7ec",
   "metadata": {},
   "source": [
    "# Getting detections from yolov5 model (custom weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762cbee7-8c51-440f-b59e-9f0747e5e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba18d0-0bd8-4ee0-a157-6cfcce9aa049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python yolov5/detect.py --weights best.pt --img 1280 --conf 0.25 --source data/video/red_white.mp4 --name custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd8d10-dd59-400e-9789-e0266666d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "WEIGHTS_PATH = \"best.pt\"\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', WEIGHTS_PATH, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e8e27-4a3f-4148-8cb0-e959b530e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conf = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdb089-2958-4f3a-94a1-305d1d419c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fn(img):\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    res= model(img, size=1280)\n",
    "    detections = res.pred[0]\n",
    "\n",
    "    # Extract bounding boxes, scores, and class IDs\n",
    "    boxes = detections[:, :4].numpy()  # x_center, y_center, width, height\n",
    "    scores = detections[:, 4].numpy()\n",
    "    class_ids = detections[:, 5].int().numpy()\n",
    "    \n",
    "    # Convert class IDs to class names\n",
    "    class_names = [model.names[i] for i in class_ids]\n",
    "\n",
    "\n",
    "    normalized_boxes = []\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        x_min_normalized = x_min / width\n",
    "        y_min_normalized = y_min / height\n",
    "        x_max_normalized = x_max / width\n",
    "        y_max_normalized = y_max / height\n",
    "        normalized_boxes.append([x_min_normalized, y_min_normalized, x_max_normalized, y_max_normalized])\n",
    "\n",
    "    return(np.array(normalized_boxes),scores,class_names,class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03663adf-80ef-41a9-8c5f-cd88fbd7d920",
   "metadata": {},
   "source": [
    "# Running detection and tracking on video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58f23c-af38-4655-901b-c81b8d30ac6b",
   "metadata": {},
   "source": [
    "### Getting detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08c40d-a543-4857-8b97-864651041740",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "all_detection_data = []\n",
    "current=1\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "\n",
    "    boxes, scores, names, class_ids= detect_fn(img)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        object_data = {\n",
    "            \"box\":boxes[i],\n",
    "            \"score\": scores[i],\n",
    "            \"class\": names[i]\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_detection_data.append(frame_objects)\n",
    "        \n",
    "    print(f\"frame{current} done\")\n",
    "    current+=1\n",
    "\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cecde-59c6-4620-be51-2596d79f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert(obj):\n",
    "    \"\"\"\n",
    "    Convert objects that are not serializable by default.\n",
    "    This includes numpy arrays and numpy data types.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    raise TypeError(\"Object of type '%s' is not JSON serializable\" % type(obj).__name__)\n",
    "\n",
    "# Assuming all_detection_data is your array of arrays of dictionaries\n",
    "# ...\n",
    "\n",
    "# Write the array to a JSON file with custom serialization for numpy arrays and data types\n",
    "with open('all_detection_data.json', 'w') as file:\n",
    "    json.dump(all_detection_data, file, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828f802-fbbf-46a6-801e-01f5e2286902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the array from the JSON file\n",
    "with open('all_detection_data.json', 'r') as file:\n",
    "    loaded_data = json.load(file)\n",
    "\n",
    "all_detection_data=loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23248e23-8514-43c4-8133-f78fe331d4fc",
   "metadata": {},
   "source": [
    "### getting ball data before tracking (new approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199271c-325f-4283-a569-d0cc532538d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "ball_data=[]\n",
    "for frame in all_detection_data:\n",
    "    all=[]\n",
    "    ball=[]\n",
    "    for object in frame:\n",
    "        if object['class']=='ball':\n",
    "            ball.append(object)\n",
    "        else:\n",
    "            all.append(object)\n",
    "    all_data.append(all)\n",
    "    ball_data.append(ball)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e56433-7f61-491e-ba0a-8f900088b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for frames with no ball detections\n",
    "no_detection_count = 0\n",
    "\n",
    "# Total number of frames\n",
    "total_frames = len(ball_data)\n",
    "\n",
    "# Iterate through each frame in your ball_data array\n",
    "for frame in ball_data:\n",
    "    # Check if there are no ball detections in the frame\n",
    "    if len(frame) == 0:\n",
    "        no_detection_count += 1\n",
    "\n",
    "# Calculate the percentage of frames with no ball detections\n",
    "no_detection_percentage = (no_detection_count / total_frames) * 100\n",
    "\n",
    "# Print or return the percentage\n",
    "print(f\"Percentage of frames with no ball detections: {no_detection_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c93da-266c-4ee0-8bc6-3727a0d6fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tracking_data=[]\n",
    "for i in range (len(all_data)):\n",
    "    frame = all_data[i]\n",
    "    objects=[]\n",
    "    for object in frame:\n",
    "        if object['score']>0.5:\n",
    "            objects.append(object)\n",
    "    for ball in ball_data[i]:\n",
    "        objects.append(ball)\n",
    "    before_tracking_data.append(objects) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881415c-036a-4133-93b0-0bfdda7bd101",
   "metadata": {},
   "source": [
    "### Getting tracking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e66c9-397e-44e4-9880-35cb5610d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_green.mp4')\n",
    "all_frames_data = []\n",
    "current=0\n",
    "\n",
    "    \n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    boxes, scores, names= [] , [], []\n",
    "    for obj in before_tracking_data[current]:\n",
    "        boxes.append(obj['box'])\n",
    "        scores.append(obj['score'])\n",
    "        names.append(obj['class'])\n",
    "\n",
    "    boxes, scores, names= np.array(boxes), np.array(scores), np.array(names)\n",
    "    \n",
    "    converted_boxes = convert_boxes(img, boxes)\n",
    "    features = encoder(img, converted_boxes)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in\n",
    "                   zip(converted_boxes, scores, names, features)]\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "\n",
    "\n",
    "    for track in tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "        \n",
    "        bbox = track.to_tlbr()\n",
    "        class_name= track.get_class()\n",
    "\n",
    "        object_data = {\n",
    "            \"box\": track.to_tlbr(),\n",
    "            \"id\": track.track_id,\n",
    "            \"class\": track.get_class()\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_frames_data.append(frame_objects)\n",
    "        \n",
    "    print(f\"frame{current+1} done\")\n",
    "    current+=1\n",
    "\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef5a57-9721-4258-8ce0-7bc82bf4a593",
   "metadata": {},
   "source": [
    "# Converting tracking data to different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58ff97-fb62-4351-8a5b-98f14bc1cbeb",
   "metadata": {},
   "source": [
    "### Pascal VOC  XML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645a710-4d83-49fc-8634-cbe83aae5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "\n",
    "detections=all_frames_data\n",
    "# Create the 'cvat' folder and subdirectories if they don't exist\n",
    "cvat_folder = 'cvat2'\n",
    "image_sets_path = os.path.join(cvat_folder, 'ImageSets/Main')\n",
    "annotations_path = os.path.join(cvat_folder, 'Annotations')\n",
    "os.makedirs(image_sets_path, exist_ok=True)\n",
    "os.makedirs(annotations_path, exist_ok=True)\n",
    "\n",
    "# Create default.txt file\n",
    "with open(os.path.join(image_sets_path, 'default.txt'), 'w') as f:\n",
    "    for i in range(len(detections)):\n",
    "        f.write(f'frame_{i:06d}\\n')\n",
    "\n",
    "\n",
    "width=1920\n",
    "height=1080\n",
    "\n",
    "# Iterate through each frame and each object\n",
    "for frame_idx, frame in enumerate(detections):\n",
    "    annotation = ET.Element('annotation')\n",
    "    folder = ET.SubElement(annotation, 'folder')\n",
    "    folder.text = 'frame'\n",
    "    filename = ET.SubElement(annotation, 'filename')\n",
    "    filename.text = f'frame_{frame_idx:06d}.PNG'\n",
    "    source = ET.SubElement(annotation, 'source')\n",
    "    ET.SubElement(source, 'database').text = 'Unknown'\n",
    "    ET.SubElement(source, 'annotation').text = 'Unknown'\n",
    "    ET.SubElement(source, 'image').text = 'Unknown'\n",
    "    size = ET.SubElement(annotation, 'size')\n",
    "    ET.SubElement(size, 'width').text = str(width)  # Adjust according to your video resolution\n",
    "    ET.SubElement(size, 'height').text = str(height)  # Adjust according to your video resolution\n",
    "    ET.SubElement(size, 'depth').text = '3'  # For color images, depth is 3\n",
    "    ET.SubElement(annotation, 'segmented').text = '0'\n",
    "\n",
    "    for obj in frame:\n",
    "        object_elem = ET.SubElement(annotation, 'object')\n",
    "        ET.SubElement(object_elem, 'name').text = obj['class']\n",
    "        ET.SubElement(object_elem, 'truncated').text = '0'\n",
    "        ET.SubElement(object_elem, 'occluded').text = '0'\n",
    "        ET.SubElement(object_elem, 'difficult').text = '0'\n",
    "        bndbox = ET.SubElement(object_elem, 'bndbox')\n",
    "        ET.SubElement(bndbox, 'xmin').text = str(obj['box'][0])\n",
    "        ET.SubElement(bndbox, 'ymin').text = str(obj['box'][1])\n",
    "        ET.SubElement(bndbox, 'xmax').text = str(obj['box'][2])\n",
    "        ET.SubElement(bndbox, 'ymax').text = str(obj['box'][3])\n",
    "        \n",
    "        attributes = ET.SubElement(object_elem, 'attributes')\n",
    "\n",
    "        # Rotation attribute\n",
    "        rotation_attr = ET.SubElement(attributes, 'attribute')\n",
    "        ET.SubElement(rotation_attr, 'name').text = 'rotation'\n",
    "        ET.SubElement(rotation_attr, 'value').text = '0.0'\n",
    "\n",
    "        # Track ID attribute\n",
    "        track_id_attr = ET.SubElement(attributes, 'attribute')\n",
    "        ET.SubElement(track_id_attr, 'name').text = 'track_id'\n",
    "        ET.SubElement(track_id_attr, 'value').text = str(obj['id'])\n",
    "\n",
    "        # Keyframe attribute\n",
    "        keyframe_attr = ET.SubElement(attributes, 'attribute')\n",
    "        ET.SubElement(keyframe_attr, 'name').text = 'keyframe'\n",
    "        if frame_idx % 10 == 0:\n",
    "            ET.SubElement(keyframe_attr, 'value').text = 'True'\n",
    "        else:\n",
    "            ET.SubElement(keyframe_attr, 'value').text = 'False'\n",
    "    \n",
    "\n",
    "    # Write to XML file in the 'cvat' folder\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    tree.write(os.path.join(annotations_path, f'frame_{frame_idx:06d}.xml'))\n",
    "\n",
    "print(\"XML files generated successfully in the 'cvat2' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9bf78a-1876-4064-bfa3-558a7ba768e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file),\n",
    "                       os.path.relpath(os.path.join(root, file),\n",
    "                                       os.path.join(path, '..')))\n",
    "\n",
    "zip_file_name = 'cvat2.zip'\n",
    "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipdir('cvat/', zipf)\n",
    "\n",
    "print(f\"Created ZIP file: {zip_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd09ea-7159-4739-a140-df58c2fe5b28",
   "metadata": {},
   "source": [
    "### MOT 1.1 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de38c32-abc5-4905-9c07-de1c7ef81e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Your tracking results\n",
    "tracking_results = all_frames_data\n",
    "\n",
    "# Label names\n",
    "labels = [\"player\", \"referee\", \"goalkeeper\", \"ball\"]\n",
    "\n",
    "# Create the 'gt' folder if it doesn't exist\n",
    "gt_folder = 'gt'\n",
    "os.makedirs(gt_folder, exist_ok=True)\n",
    "\n",
    "# Generate labels.txt and gt.txt inside the 'gt' folder\n",
    "with open(os.path.join(gt_folder, 'labels.txt'), 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write(f'{label}\\n')\n",
    "\n",
    "def xyxy_to_xywh(box):\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    return [round(x_min,1), round(y_min,1), round(x_max - x_min,1), round(y_max - y_min,1)]\n",
    "\n",
    "with open(os.path.join(gt_folder, 'gt.txt'), 'w') as f:\n",
    "    for frame_idx, frame in enumerate(tracking_results):\n",
    "        for obj in frame:\n",
    "            box = xyxy_to_xywh(obj['box'])\n",
    "            class_id = labels.index(obj['class']) + 1\n",
    "            line = f'{frame_idx + 1},{obj[\"id\"]},{box[0]},{box[1]},{box[2]},{box[3]},1,{class_id},1.0\\n'\n",
    "            f.write(line)\n",
    "\n",
    "# Zip the 'gt' folder\n",
    "zip_file_name = 'gt.zip'\n",
    "with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(gt_folder):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), gt_folder))\n",
    "\n",
    "print(\"Created ZIP file: \" + zip_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc8794-84f2-4832-a0eb-f409f29e902b",
   "metadata": {},
   "source": [
    "### CVAT 1.1 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe757a4-a7c7-4fa5-b8eb-fb50a7b79ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "def prettify(elem, level=0):\n",
    "    indent = \"\\n\" + level*\"  \"\n",
    "    if len(elem):\n",
    "        if not elem.text or not elem.text.strip():\n",
    "            elem.text = indent + \"  \"\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "        for elem in elem:\n",
    "            prettify(elem, level+1)\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "    else:\n",
    "        if level and (not elem.tail or not elem.tail.strip()):\n",
    "            elem.tail = indent\n",
    "\n",
    "# Example tracking data\n",
    "tracking_data = all_frames_data\n",
    "\n",
    "# Convert box coordinates from xyxy to xtl, ytl, xbr, ybr format\n",
    "def convert_box(box):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return {'xtl': xtl, 'ytl': ytl, 'xbr': xbr, 'ybr': ybr}\n",
    "\n",
    "# Create the root element\n",
    "annotations = ET.Element('annotations')\n",
    "ET.SubElement(annotations, 'version').text = '1.1'\n",
    "\n",
    "# Create tracks for each object\n",
    "tracks = {}\n",
    "for frame_data in tracking_data:\n",
    "    for obj in frame_data:\n",
    "        obj_id = obj['id']\n",
    "        if obj_id not in tracks:\n",
    "            track = ET.SubElement(annotations, 'track', id=str(obj_id), label=obj['class'], source='manual')\n",
    "            tracks[obj_id] = track\n",
    "\n",
    "# Process each frame for every object\n",
    "for frame_idx in range(len(tracking_data)):\n",
    "    current_frame_objects = {obj['id']: obj for obj in tracking_data[frame_idx]}\n",
    "    for obj_id, track in tracks.items():\n",
    "        if obj_id in current_frame_objects:\n",
    "            obj = current_frame_objects[obj_id]\n",
    "            box = convert_box(obj['box'])\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "        else:\n",
    "            # Object is not in the current frame\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='1', occluded='0', \n",
    "                          xtl='0', ytl='0', xbr='0', ybr='0', z_order='0')\n",
    "\n",
    "# Write to XML file\n",
    "prettify(annotations)\n",
    "tree = ET.ElementTree(annotations)\n",
    "tree.write('annotations.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(\"annotations.xml file generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450ee17-9d69-46ff-94f4-307d9ac0fa77",
   "metadata": {},
   "source": [
    "# Seperating results by class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af7c13-7a9f-46ec-9700-aa82f7c44201",
   "metadata": {},
   "source": [
    "### getting ball data before tracking (old approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cc832-c45b-4fa9-87ee-bd3c3df0ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "ball_data=[]\n",
    "for frame in all_detection_data:\n",
    "    all=[]\n",
    "    ball=[]\n",
    "    for object in frame:\n",
    "        if object['class']=='ball':\n",
    "            ball.append(object)\n",
    "        else:\n",
    "            all.append(object)\n",
    "    all_data.append(all)\n",
    "    ball_data.append(ball)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a8ddc-15c2-4187-93e6-849ecefb9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each frame in your ball_data array\n",
    "for i, frame in enumerate(ball_data):\n",
    "    # Check if there is more than one ball detection in the frame\n",
    "    if len(frame) > 1:\n",
    "        # Find the detection with the highest score\n",
    "        highest_score_detection = max(frame, key=lambda detection: detection['score'])\n",
    "        \n",
    "        # Keep only the detection with the highest score\n",
    "        ball_data[i] = [highest_score_detection]\n",
    "\n",
    "# Now, ball_data will contain only the ball with the highest score in frames with multiple detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1ee8c-572c-4d58-ac41-6295f14400d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for frames with no ball detections\n",
    "no_detection_count = 0\n",
    "\n",
    "# Total number of frames\n",
    "total_frames = len(ball_data)\n",
    "\n",
    "# Iterate through each frame in your ball_data array\n",
    "for frame in ball_data:\n",
    "    # Check if there are no ball detections in the frame\n",
    "    if len(frame) == 0:\n",
    "        no_detection_count += 1\n",
    "\n",
    "# Calculate the percentage of frames with no ball detections\n",
    "no_detection_percentage = (no_detection_count / total_frames) * 100\n",
    "\n",
    "# Print or return the percentage\n",
    "print(f\"Percentage of frames with no ball detections: {no_detection_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96758a0a-9f79-4139-b3e6-ed69cfa5199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video frame dimensions\n",
    "frame_width = 1920\n",
    "frame_height = 1080\n",
    "\n",
    "# Convert normalized box coordinates to absolute pixel values\n",
    "def convert_normalized_box(box, width, height):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return [xtl * width, ytl * height, xbr * width, ybr * height]\n",
    "\n",
    "\n",
    "# Adjust ball_data to use absolute pixel values\n",
    "ball_data_absolute = []\n",
    "for frame in ball_data:\n",
    "    if frame:\n",
    "        ball = frame[0]\n",
    "        normalized_box = ball['box']\n",
    "        absolute_box = convert_normalized_box(normalized_box, frame_width, frame_height)\n",
    "        ball_data_absolute.append([{'score': ball['score'], 'class': ball['class'], 'box': absolute_box}])\n",
    "    else:\n",
    "        ball_data_absolute.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8cf26-3fba-49dd-8218-21ee84b55c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tracking_data=[]\n",
    "for i in range (len(all_data)):\n",
    "    frame = all_data[i]\n",
    "    objects=[]\n",
    "    for object in frame:\n",
    "        if object['score']>0.5:\n",
    "            objects.append(object)\n",
    "    before_tracking_data.append(objects) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29833cf-348b-4f09-9416-477d767d39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracking_data_without_balls = [[obj for obj in frame if obj['class'] != 'ball'] for frame in all_frames_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc1214-59fa-4310-a181-8ed26d09ddf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Converting the seperated results to CVAT format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27659b57-215b-4ed2-90b3-8ce4e25cb298",
   "metadata": {},
   "source": [
    "##### Old approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a91090-588c-46ef-b68e-52e3689ef3d6",
   "metadata": {},
   "source": [
    "ball_data_abosulte : ball detection results without tracking (not normalized)\n",
    "\n",
    "all_frames_data: tracking results without ball (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71593918-d6f1-4429-b7b9-5d74c4f18eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_data=[]\n",
    "for frame in ball_data_absolute:\n",
    "    objects=[]\n",
    "    for object in frame:\n",
    "        if object['score']>0.3:\n",
    "            objects.append(object)\n",
    "    ball_data.append(objects) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f10af-a782-4bd4-a973-219d9ee3b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_data = tracking_data_without_balls\n",
    "\n",
    "def prettify(elem, level=0):\n",
    "    indent = \"\\n\" + level*\"  \"\n",
    "    if len(elem):\n",
    "        if not elem.text or not elem.text.strip():\n",
    "            elem.text = indent + \"  \"\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "        for elem in elem:\n",
    "            prettify(elem, level+1)\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "    else:\n",
    "        if level and (not elem.tail or not elem.tail.strip()):\n",
    "            elem.tail = indent\n",
    "\n",
    "def convert_box(box):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return {'xtl': xtl, 'ytl': ytl, 'xbr': xbr, 'ybr': ybr}\n",
    "\n",
    "annotations = ET.Element('annotations')\n",
    "ET.SubElement(annotations, 'version').text = '1.1'\n",
    "\n",
    "# Create tracks for each object in tracking_data\n",
    "tracks = {}\n",
    "for frame_data in tracking_data:\n",
    "    for obj in frame_data:\n",
    "        obj_id = obj['id']\n",
    "        if obj_id not in tracks:\n",
    "            track = ET.SubElement(annotations, 'track', id=str(obj_id), label=obj['class'], source='manual')\n",
    "            tracks[obj_id] = track\n",
    "\n",
    "# Add track for the ball\n",
    "ball_track_id = max(tracks.keys(), default=0) + 1\n",
    "ball_track = ET.SubElement(annotations, 'track', id=str(ball_track_id), label='ball', source='manual')\n",
    "last_ball_box = None\n",
    "\n",
    "# Process each frame for every object\n",
    "for frame_idx in range(len(tracking_data)):\n",
    "    current_frame_objects = {obj['id']: obj for obj in tracking_data[frame_idx]}\n",
    "    for obj_id, track in tracks.items():\n",
    "        if obj_id in current_frame_objects:\n",
    "            obj = current_frame_objects[obj_id]\n",
    "            box = convert_box(obj['box'])\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "        else:\n",
    "            # Object is not in the current frame\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='1', occluded='0', \n",
    "                          xtl='0', ytl='0', xbr='0', ybr='0', z_order='0')\n",
    "    \n",
    "    # Process ball\n",
    "    ball_in_frame = ball_data[frame_idx]\n",
    "    if ball_in_frame:\n",
    "        last_ball_box = convert_box(ball_in_frame[0]['box'])\n",
    "        ET.SubElement(ball_track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                      xtl=str(last_ball_box['xtl']), ytl=str(last_ball_box['ytl']), xbr=str(last_ball_box['xbr']), ybr=str(last_ball_box['ybr']), z_order='0')\n",
    "    elif last_ball_box:\n",
    "        ET.SubElement(ball_track, 'box', frame=str(frame_idx), keyframe='0', outside='0', occluded='0', \n",
    "                      xtl=str(last_ball_box['xtl']), ytl=str(last_ball_box['ytl']), xbr=str(last_ball_box['xbr']), ybr=str(last_ball_box['ybr']), z_order='0')\n",
    "\n",
    "# Write to XML file\n",
    "prettify(annotations)\n",
    "tree = ET.ElementTree(annotations)\n",
    "tree.write('annotations.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(\"annotations.xml file generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f23f6-142d-4e24-9302-5b18f7836e47",
   "metadata": {},
   "source": [
    "##### New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b6b0f-b308-487f-ac0e-d81ed620c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tracking_data = all_frames_data\n",
    "\n",
    "def prettify(elem, level=0):\n",
    "    indent = \"\\n\" + level*\"  \"\n",
    "    if len(elem):\n",
    "        if not elem.text or not elem.text.strip():\n",
    "            elem.text = indent + \"  \"\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "        for elem in elem:\n",
    "            prettify(elem, level+1)\n",
    "        if not elem.tail or not elem.tail.strip():\n",
    "            elem.tail = indent\n",
    "    else:\n",
    "        if level and (not elem.tail or not elem.tail.strip()):\n",
    "            elem.tail = indent\n",
    "\n",
    "def convert_box(box):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return {'xtl': xtl, 'ytl': ytl, 'xbr': xbr, 'ybr': ybr}\n",
    "\n",
    "annotations = ET.Element('annotations')\n",
    "ET.SubElement(annotations, 'version').text = '1.1'\n",
    "\n",
    "# Create tracks for each object\n",
    "tracks = {}\n",
    "last_seen_boxes = {}\n",
    "for frame_data in tracking_data:\n",
    "    for obj in frame_data:\n",
    "        obj_id = obj['id']\n",
    "        if obj_id not in tracks:\n",
    "            track = ET.SubElement(annotations, 'track', id=str(obj_id), label=obj['class'], source='manual')\n",
    "            tracks[obj_id] = track\n",
    "        last_seen_boxes[obj_id] = convert_box(obj['box'])\n",
    "\n",
    "# Process each frame for every object\n",
    "for frame_idx, frame_data in enumerate(tracking_data):\n",
    "    current_frame_objects = {obj['id']: obj for obj in frame_data}\n",
    "    for obj_id, track in tracks.items():\n",
    "        if obj_id in current_frame_objects:\n",
    "            obj = current_frame_objects[obj_id]\n",
    "            box = convert_box(obj['box'])\n",
    "            last_seen_boxes[obj_id] = box\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='1', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "        elif tracks[obj_id].attrib['label'] == 'ball' and last_seen_boxes[obj_id] is not None:\n",
    "            # For balls, if absent in a frame, use the last known box coordinates\n",
    "            box = last_seen_boxes[obj_id]\n",
    "            ET.SubElement(track, 'box', frame=str(frame_idx), keyframe='0', outside='0', occluded='0', \n",
    "                          xtl=str(box['xtl']), ytl=str(box['ytl']), xbr=str(box['xbr']), ybr=str(box['ybr']), z_order='0')\n",
    "# Write to XML file\n",
    "prettify(annotations)\n",
    "tree = ET.ElementTree(annotations)\n",
    "tree.write('annotations.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(\"annotations.xml file generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd600d41-6f23-47d5-8b01-cee5190120c1",
   "metadata": {},
   "source": [
    "# Exporting annotations from CVAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39408fea-5480-4594-bb00-5b3f4f411630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('annotations 7.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Initialize arrays for results\n",
    "ball_results = []\n",
    "player_results = []\n",
    "other_results = []\n",
    "\n",
    "# Function to convert box attributes to a dictionary\n",
    "def get_box_dict(box_element):\n",
    "    return [\n",
    "        float(box_element.get('xtl')),\n",
    "        float(box_element.get('ytl')),\n",
    "        float(box_element.get('xbr')),\n",
    "        float(box_element.get('ybr'))]\n",
    "\n",
    "# Process each track (object)\n",
    "for track in root.findall('track'):\n",
    "    label = track.get('label')\n",
    "    obj_id = int(track.get('id'))\n",
    "\n",
    "    # Dictionary to store boxes for each frame\n",
    "    frames = {}\n",
    "\n",
    "    # Process each box (appearance in a frame)\n",
    "    for box in track.findall('box'):\n",
    "        frame_num = int(box.get('frame'))\n",
    "        outside = box.get('outside') == '1'\n",
    "\n",
    "        if not outside:\n",
    "            box_dict = get_box_dict(box)\n",
    "            frames[frame_num] = {'box': box_dict, 'class': label, 'id': obj_id}\n",
    "\n",
    "    # Sort frames and add to the respective result array\n",
    "    sorted_frames = sorted(frames.items())\n",
    "    for frame_num, frame_data in sorted_frames:\n",
    "        while len(ball_results) <= frame_num:\n",
    "            ball_results.append([])\n",
    "        while len(player_results) <= frame_num:\n",
    "            player_results.append([])\n",
    "        while len(other_results) <= frame_num:\n",
    "            other_results.append([])\n",
    "\n",
    "        if label == 'ball':\n",
    "            ball_results[frame_num].append(frame_data)\n",
    "        elif label == 'player':\n",
    "            player_results[frame_num].append(frame_data)\n",
    "        elif label in ['goalkeeper', 'referee']:\n",
    "            other_results[frame_num].append(frame_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585081ed-ab3e-4bb0-b1d4-68149d85bb83",
   "metadata": {},
   "source": [
    "# Processing and plotting of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b649278-9ae7-40a4-b237-e609377544fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "vid = cv2.VideoCapture('./data/video/red_green.mp4')\n",
    "\n",
    "for frame in player_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:\n",
    "        xmin, ymin, xmax, ymax = obj['box']\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        crop_img = img[ymin:ymax, xmin:xmax]\n",
    "        if crop_img.size > 0:\n",
    "             if red(crop_img)>0.1:\n",
    "                obj['class']='team1'\n",
    "             else:\n",
    "                obj['class']='team2'\n",
    "    \n",
    "#    last_id_per_class = {}\n",
    "    color_dict = {\n",
    "    \"team1\": (0, 0, 255),    \n",
    "    \"team2\": (0, 255, 0),    \n",
    "    \"referee\": (255,255,0), \n",
    "    \"goalkeeper\": (255, 0, 0), \n",
    "    \"player\": (255, 255, 0), \n",
    "    \"ball\": (255, 255, 255)  \n",
    "    }\n",
    "vid.release()\n",
    "\n",
    "\n",
    "labels_per_id = defaultdict(list)\n",
    "\n",
    "for frame in player_results:  # Iterate over each frame\n",
    "    for obj in frame:  # Iterate over each detection in the frame\n",
    "        player_id = obj['id']\n",
    "        label = obj['class']\n",
    "        labels_per_id[player_id].append(label)\n",
    "\n",
    "# Dictionary to store the most common label for each ID\n",
    "most_common_label_per_id = {}\n",
    "\n",
    "for player_id, labels in labels_per_id.items():\n",
    "    # Determine the most common label\n",
    "    most_common_label = Counter(labels).most_common(1)[0][0]\n",
    "    most_common_label_per_id[player_id] = most_common_label\n",
    "\n",
    "for frame in player_results:\n",
    "    for obj in frame:\n",
    "        id = obj['id']\n",
    "        # Assign the most common label to this detection\n",
    "        obj['class'] = most_common_label_per_id[id]\n",
    "\n",
    "\n",
    "new_id_mapping = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for old_id, label in most_common_label_per_id.items():\n",
    "    # Increment the counter for the label and assign the new ID\n",
    "    new_id_mapping[label]['counter'] += 1\n",
    "    new_id_mapping[label][old_id] = new_id_mapping[label]['counter']\n",
    "\n",
    "def get_new_id(old_id, label):\n",
    "    \"\"\"\n",
    "    Given an old ID and its label, returns a new ID that is unique within the label category.\n",
    "    \"\"\"\n",
    "    return new_id_mapping[label][old_id]\n",
    "\n",
    "for frame in player_results:\n",
    "    for obj in frame:\n",
    "        obj['id']=get_new_id(obj['id'],obj['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2bb4c-b828-4047-9c44-68ca2240c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_green.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_green6.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "\n",
    "for frame in player_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:     \n",
    "        if obj['class']!='':\n",
    "            class_name = obj['class']\n",
    "    \n",
    "            bbox=obj['box']\n",
    "            cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "    \n",
    "            cv2.putText(img,'id' + str(obj['id']), (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                        color_dict[obj['class']], 2)\n",
    "    for obj in ball_results[current]:   \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "        cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.5,\n",
    "                    color_dict[obj['class']], 1)\n",
    "    \n",
    "    for obj in other_results[current]:   \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "        cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.5,\n",
    "                    color_dict[obj['class']], 1)\n",
    "\n",
    "\n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979423f8-726d-48d8-bb42-6414de224849",
   "metadata": {},
   "source": [
    "### results with circles and ball posession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914777a-90bb-427d-a216-32dd25aeb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw an ellipse under detected objects\n",
    "def draw_ellipse(image, box, color, thickness=4):\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    bottom_center = (int((x_min + x_max) / 2), int(y_max))\n",
    "    axes = (int((x_max - x_min) / 2), int(0.35 * (x_max - x_min)))\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, -45, 235, color, thickness, cv2.LINE_4)\n",
    "    #cv2.ellipse(image, bottom_center, axes, 0, 0, 360, color, thickness)\n",
    "    cv2.ellipse(\n",
    "        image,\n",
    "        center=bottom_center,\n",
    "        axes=axes,\n",
    "        angle=0.0,\n",
    "        startAngle=-45,\n",
    "        endAngle=235,\n",
    "        color=color,\n",
    "        thickness=thickness,\n",
    "        lineType=cv2.LINE_4\n",
    "    )\n",
    "# Color definitions for different objects\n",
    "COLORS = {\n",
    "    \"team1\": (0, 0, 255),    # Red color for team1 (BGR format)\n",
    "    \"team2\": (255, 255, 0),    # light blue\n",
    "    \"referee\": (0, 255, 255) ,     # Yellow\n",
    "    \"goalkeeper\": (255, 0, 255), \n",
    "    \"player\": (255, 0, 255), \n",
    "    \"ball\": (0, 255, 255)  # White color for ball\n",
    "}\n",
    "THICKNESS = 4\n",
    "\n",
    "for object in frame:\n",
    "    box = object[\"box\"]\n",
    "    class_name = object[\"class\"]\n",
    "    color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "    draw_ellipse(img, box, color, THICKNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e50b8-c4f1-4116-b984-b044651d943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_IN_POSSESSION_PROXIMITY = 30\n",
    "PLAYER_MARKER_COLOR = (255, 0, 255)  # Green in BGR\n",
    "BALL_MARKER_COLOR = (255,0, 0)  # Blue in BGR\n",
    "MARKER_CONTOUR_COLOR = (0, 0, 0)  # Black in BGR\n",
    "MARKER_CONTOUR_THICKNESS = 1\n",
    "MARKER_WIDTH = 20\n",
    "MARKER_HEIGHT = 20 \n",
    "def calculate_marker(anchor,MARKER_MARGIN = 20):\n",
    "    x, y = anchor\n",
    "    return np.array([\n",
    "        [x - MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN],\n",
    "        [x, y - MARKER_MARGIN],\n",
    "        [x + MARKER_WIDTH // 2, y - MARKER_HEIGHT - MARKER_MARGIN]\n",
    "    ], np.int32)\n",
    "\n",
    "def draw_marker(image, anchor, color, MARKER_MARGIN = 20):\n",
    "    marker_contour = calculate_marker(anchor,MARKER_MARGIN)\n",
    "    cv2.fillPoly(image, [marker_contour], color)\n",
    "    cv2.polylines(image, [marker_contour], isClosed=True, color=MARKER_CONTOUR_COLOR, thickness=MARKER_CONTOUR_THICKNESS)\n",
    "\n",
    "def get_player_in_possession(players, ball):\n",
    "    if len(ball) != 1:\n",
    "        return None\n",
    "    ball_x, ball_y = ball[0]['center']\n",
    "    for player in players:\n",
    "        player_x, player_y = player['center']\n",
    "        if abs(player_x - ball_x) <= PLAYER_IN_POSSESSION_PROXIMITY and abs(player_y - ball_y) <= PLAYER_IN_POSSESSION_PROXIMITY:\n",
    "            return player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039503e1-3dfc-42fa-b5d4-d4f518c39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_green.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_green7.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "color_dict=COLORS\n",
    "\n",
    "for frame in player_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:\n",
    "        x_min, y_min, x_max, y_max = obj['box']\n",
    "        obj['center']=[(x_min+x_max)//2, (y_min+y_max)//2]  \n",
    "        if obj['class']!='player':\n",
    "            bbox = obj[\"box\"]\n",
    "            class_name = obj[\"class\"]\n",
    "            color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "            draw_ellipse(img, bbox, color, THICKNESS)\n",
    "    \n",
    "            cv2.putText(img,str(obj['id']), (int((bbox[0]+bbox[2])/2), int(bbox[3]+30)), 0, 0.75,\n",
    "                        color_dict[obj['class']], 2)\n",
    "    for obj in ball_results[current]:\n",
    "        x_min, y_min, x_max, y_max = obj['box']\n",
    "        obj['center']=[(x_min+x_max)//2, (y_min+y_max)//2]  \n",
    "        bbox = obj[\"box\"]\n",
    "        class_name = obj[\"class\"]\n",
    "        color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "        draw_ellipse(img, bbox, color, THICKNESS)\n",
    "#        cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[3]+10)), 0, 0.5,\n",
    "#                    color_dict[obj['class']], 1)\n",
    "    \n",
    "    for obj in other_results[current]:\n",
    "        x_min, y_min, x_max, y_max = obj['box']\n",
    "        obj['center']=[(x_min+x_max)//2, (y_min+y_max)//2]  \n",
    "        bbox = obj[\"box\"]\n",
    "        class_name = obj[\"class\"]\n",
    "        color = COLORS.get(class_name, (0, 0, 255))  # Default to red if class not found\n",
    "        draw_ellipse(img, bbox, color, THICKNESS)\n",
    "#        cv2.putText(img, obj['class'], (int(bbox[0]), int(bbox[3]+10)), 0, 0.5,\n",
    "#                    color_dict[obj['class']], 1)\n",
    "\n",
    "    \n",
    "    players = player_results[current]\n",
    "    ball = ball_results[current]\n",
    "    player_in_possession = get_player_in_possession(players, ball)\n",
    "    \n",
    "    if player_in_possession!= None:\n",
    "        draw_marker(img, player_in_possession['center'], PLAYER_MARKER_COLOR, MARKER_MARGIN = 40)\n",
    "    for b in ball:\n",
    "        draw_marker(img, b['center'], BALL_MARKER_COLOR ,MARKER_MARGIN = 5)\n",
    "    \n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51878e97-85c0-4a24-b7f4-0dc654b5f30d",
   "metadata": {},
   "source": [
    "# Strong-Sort Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7aabad-7ac3-44d6-ba90-81834ab2f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strong_sort.utils.parser import get_config\n",
    "from strong_sort.strong_sort import StrongSORT\n",
    "from strong_sort.sort.tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fa99f-1039-44cc-a9fc-fe075df645be",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_sort_weights='strong_sort/osnet_x0_25_msmt17.pt'\n",
    "tracker= StrongSORT(model_weights= strong_sort_weights,\n",
    "                 device='cpu',\n",
    "                 fp16=False,\n",
    "                 max_dist=0.2,\n",
    "                 max_iou_distance=0.7,\n",
    "                 max_age=70, n_init=3,\n",
    "                 nn_budget=100,\n",
    "                 mc_lambda=0.995,\n",
    "                 ema_alpha=0.9\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df23611-5cde-4d39-8a78-6694f9086289",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_white.mp4')\n",
    "all_frames_data = []\n",
    "current=0\n",
    "\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    frame_objects = []\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    boxes, scores, names, class_ids= [] , [], [], []\n",
    "    for obj in all_data[current]:\n",
    "        if obj['score']>0.6:\n",
    "            boxes.append(obj['box'])\n",
    "            scores.append(obj['score'])\n",
    "            names.append(obj['class'])\n",
    "            class_ids.append(obj['class_id'])\n",
    "\n",
    "    boxes, scores, names,class_ids= np.array(boxes), np.array(scores), np.array(names), np.array(class_ids)\n",
    "    \n",
    "    converted_boxes = np.array(convert_boxes(img, boxes))\n",
    "    #print(converted_boxes)\n",
    "    #break\n",
    "    tracks=tracker.update(converted_boxes, scores, class_ids,img)\n",
    "\n",
    "\n",
    "    for track in tracker.tracker.tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "        track_id=track.track_id\n",
    "        hits=track.hits\n",
    "        class_id=track.class_id\n",
    "        bbox = track.to_tlbr()\n",
    "        #class_name= track.get_class()\n",
    "\n",
    "        object_data = {\n",
    "            \"box\": bbox,\n",
    "            \"class\":class_id,\n",
    "            \"id\": track_id\n",
    "        }\n",
    "\n",
    "        frame_objects.append(object_data)\n",
    "\n",
    "    all_frames_data.append(frame_objects)\n",
    "        \n",
    "    print(f\"frame{current+1} done\")\n",
    "    current+=1\n",
    "\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758eff9-fd8f-4a19-9b8c-da0319f8f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in all_frames_data:\n",
    "    for detection in frame:\n",
    "        xmin, ymin, xmax, ymax = detection['box']\n",
    "        w=xmax-xmin\n",
    "        h=ymax-ymin\n",
    "        xmin, ymin, xmax, ymax = xmin+w/2, ymin+h/2, xmax+w/2, ymax+h/2\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        detection['box']=[xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16236e2-ed59-4007-9a83-ab78d89d07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your label map\n",
    "labels = [\n",
    "    {'name': 'ball', 'id': 1},\n",
    "    {'name': 'goalkeeper', 'id': 2},\n",
    "    {'name': 'player', 'id': 3},\n",
    "    {'name': 'referee', 'id': 4}\n",
    "]\n",
    "\n",
    "# Convert label map to a dictionary for easy lookup\n",
    "id_to_name_dict = {item['id']: item['name'] for item in labels}\n",
    "\n",
    "\n",
    "# Updating 'class' from class ID to class name\n",
    "for frame in all_frames_data:\n",
    "    for detection in frame:\n",
    "        class_id = detection['class']\n",
    "        class_name = id_to_name_dict.get(class_id)\n",
    "        detection['class'] = class_name\n",
    "\n",
    "# all_data now has 'class' as class name\n",
    "all_frames_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc41c7-ae03-40df-95b1-bdea8534436e",
   "metadata": {},
   "source": [
    "# Getting detections and tracking with yolov8 model\n",
    "detections : https://docs.ultralytics.com/modes/predict/#inference-sources\n",
    "\n",
    "tracking : https://docs.ultralytics.com/modes/track/#tracking\n",
    "\n",
    "model names: https://docs.ultralytics.com/models/yolov8/#performance-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75489b0-ac4f-417a-a405-0350165bdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "import ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45051fe-76a8-4608-958f-fbcd872f7481",
   "metadata": {},
   "source": [
    "### import dataset from roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fc614-1e03-4654-8ed7-894dbcd3b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fed413-9f0c-455a-89f0-dc219aea46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {HOME}/datasets\n",
    "%cd {HOME}/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f397229-2627-4b31-9fbf-3e38388dc343",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow --quiet\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
    "project = rf.workspace(\"roboflow-jvuqo\").project(\"football-players-detection-3zvbc\")\n",
    "dataset = project.version(1).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390e091-ab92-49c8-86e5-1b39af919834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_file_path=f'{dataset.location}/data.yaml'\n",
    "# Load the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_content = yaml.safe_load(file)\n",
    "\n",
    "# Modify the data\n",
    "yaml_content['test'] = f\"{dataset.location}/test/images\"\n",
    "yaml_content['train'] = f\"{dataset.location}/train/images\"\n",
    "yaml_content['val'] = f\"{dataset.location}/valid/images\"\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.safe_dump(yaml_content, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1f34e-7447-48c9-bdf6-83ba3b94fef3",
   "metadata": {},
   "source": [
    "### train custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f6ff6-e895-4702-839f-3e5ca41f94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=train model=yolov8x.pt data={dataset.location}/data.yaml epochs=250 imgsz=6400 plots=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f32907-09b3-4a68-84d3-2ef12b41d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64853520-a7a9-4b91-95ea-9788447a677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbefe8c-350a-489b-842b-842bfa323dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863ec10-c6f4-441e-9c0b-7fbaf82a8df2",
   "metadata": {},
   "source": [
    "### validate custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fceff-3d12-4e9b-8ab5-bf6d92c5caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d28f0-9724-4b03-9357-f921be15e160",
   "metadata": {},
   "source": [
    "### inference with custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaddf9c-4f3b-4a2a-8d43-bc32de6248d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82f398-7659-43ad-8268-98fd83f4e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"best (1)a.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08c415-fc16-4d18-b4ef-ee9f31d7166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d7618-258a-449d-9971-1e625a1d8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source='data/video/red_white.mp4', device=\"cpu\", conf=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27358037-da31-42b0-b4f6-873f97e158ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detection_data = []\n",
    "for r in results:\n",
    "    boxes = r.boxes\n",
    "    print(boxes.conf)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411c700-c6e0-4701-862c-634385906f6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Colors palettes and team prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b5b82-2bf6-43e2-a679-cdc7a6d2f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a692ffd-eb23-4fb9-89db-bc803ff6c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = player_results[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78252a1f-b466-44e0-97ec-be087932d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(images, rows, cols):\n",
    "    \"\"\"\n",
    "    Stitch multiple images into a single image for display.\n",
    "    :param images: List of images to be stitched.\n",
    "    :param rows: Number of rows in the final stitched image.\n",
    "    :param cols: Number of columns in the final stitched image.\n",
    "    :return: Stitched image.\n",
    "    \"\"\"\n",
    "    # Calculate max height and total width\n",
    "    max_height = max(im.shape[0] for im in images)\n",
    "    total_width = sum(im.shape[1] for im in images)\n",
    "\n",
    "    # Create a blank canvas with max height and total width\n",
    "    stitched_image = np.zeros((max_height, total_width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Paste images into the canvas\n",
    "    current_x = 0\n",
    "    for im in images:\n",
    "        stitched_image[:im.shape[0], current_x:current_x+im.shape[1]] = im\n",
    "        current_x += im.shape[1]\n",
    "\n",
    "    return cv2.resize(stitched_image, (cols, rows))\n",
    "\n",
    "def select_team_colors(stitched_image, team_name):\n",
    "    \"\"\"\n",
    "    Open a window to select colors for a team.\n",
    "    :param stitched_image: Image containing all players.\n",
    "    :param team_name: Name of the team (for window title).\n",
    "    :return: List of selected colors.\n",
    "    \"\"\"\n",
    "    selected_colors = []\n",
    "\n",
    "    def mouse_click(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            # Append the color (in BGR format) where clicked\n",
    "            selected_colors.append(stitched_image[y, x, :].tolist())\n",
    "            print(f\"Color selected for {team_name}: {stitched_image[y, x, :]}\")\n",
    "\n",
    "    cv2.namedWindow(team_name)\n",
    "    cv2.setMouseCallback(team_name, mouse_click)\n",
    "    \n",
    "    while True:\n",
    "        cv2.imshow(team_name, stitched_image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return selected_colors\n",
    "\n",
    "cropped_images = []\n",
    "\n",
    "# Assuming 'frame_rgb' and 'results_players' are already defined as per your existing code\n",
    "for player in players:\n",
    "    bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "    cropped_img = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "    cropped_images.append(cropped_img)\n",
    "\n",
    "# Define the size of the stitched image\n",
    "rows, cols = 500, 1000  # Adjust as needed\n",
    "\n",
    "# Create the stitched image\n",
    "stitched_image = stitch_images(cropped_images, rows, cols)\n",
    "\n",
    "# Get colors for each team's kits\n",
    "team1_player_colors = select_team_colors(stitched_image, \"Team 1 Player Kit\")\n",
    "team2_player_colors = select_team_colors(stitched_image, \"Team 2 Player Kit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae640a9-dbc7-4f20-95ff-cd8ebfd60849",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list= team1_player_colors + team2_player_colors\n",
    "colors_list_rgb = [color[::-1] for color in colors_list]\n",
    "color_list_lab = [skimage.color.rgb2lab([i/255 for i in c]) for c in colors_list_rgb] # Converting color_list to L*a*b* space\n",
    "color_list_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb41f7-5780-4bcd-ab32-a79e0113ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "vid.set(cv2.CAP_PROP_POS_FRAMES, 100)\n",
    "_, frame = vid.read()\n",
    "vid.release()\n",
    "players = player_results[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f729b-31f2-4385-84b3-f4d146fb10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                                      # Convert frame to RGB\n",
    "obj_palette_list = []                                                                   # Initialize players color palette list\n",
    "palette_interval = (0,5)                                                                # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "annotated_frame = frame.copy()                                                                 # Create annotated frame \n",
    "for player in players:\n",
    "    bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "    obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "    \n",
    "    obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "    center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "    center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "    center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "    center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "    center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                            center_filter_x1:center_filter_x2]\n",
    "\n",
    "   \n",
    "    \n",
    "    obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "        \n",
    "    reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "    palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "    palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "    color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "#    RGB_df = pd.DataFrame(color_count, columns = ['cnt', 'RGB']).sort_values(       # Create dataframe based on defined palette interval\n",
    "#                          by = 'cnt', ascending = False).iloc[\n",
    "#                              palette_interval[0]:palette_interval[1],:]\n",
    "#    palette = list(RGB_df.RGB)                                                      # Convert palette to list (for faster processing)\n",
    "    annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "                                    (int(bbox[0])+center_filter_x1, \n",
    "                                     int(bbox[1])+ center_filter_y1),  \n",
    "                                    (int(bbox[0])+center_filter_x2, \n",
    "                                     int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "    player_palette = []\n",
    "    for count, color in color_count:\n",
    "            player_palette.append((color, count))\n",
    "\n",
    "    obj_palette_list.append(player_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9eec68-3878-47ed-8d84-1866beb94741",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "players_distance_features = []\n",
    "\n",
    "# Loop over detected players' extracted color palettes\n",
    "for player_palette in obj_palette_list:\n",
    "    palette_distance = []\n",
    "\n",
    "    # Loop over colors in the player's palette (color is a tuple of (RGB, frequency))\n",
    "    for color, freq in player_palette:\n",
    "        color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "        distance_list = []\n",
    "\n",
    "        # Loop over predefined list of teams colors (now only two colors, one for each team)\n",
    "        for c in color_list_lab:\n",
    "            distance = skimage.color.deltaE_cie76(color_lab, c)  # Calculate Euclidean distance in Lab color space\n",
    "            distance_list.append(distance)  # Update distance list for current color\n",
    "\n",
    "        palette_distance.append((distance_list, freq))  # Update distance list for current palette with frequency\n",
    "\n",
    "    players_distance_features.append(palette_distance)  # Update distance features list\n",
    "\n",
    "\n",
    "\n",
    "players_teams_list = []\n",
    "\n",
    "for player_palette in obj_palette_list:\n",
    "    team_votes = [0, 0]  # Initialize votes for two teams\n",
    "\n",
    "    for color, freq in player_palette:\n",
    "        color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "        distance_list = []\n",
    "\n",
    "        # Calculate distances for the color\n",
    "        for c in color_list_lab:\n",
    "            distance = skimage.color.deltaE_cie76(color_lab, c)\n",
    "            distance_list.append(distance)\n",
    "\n",
    "        # Weighted voting\n",
    "        team_idx = distance_list.index(min(distance_list))  # Closer team gets the vote\n",
    "        team_votes[team_idx] += freq  # Weight vote by frequency of the color\n",
    "\n",
    "    # Team with the highest weighted vote total is predicted\n",
    "    predicted_team = team_votes.index(max(team_votes))\n",
    "    players_teams_list.append(predicted_team)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4aa134-2c6e-4711-85f9-cc8b89169c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "cropped_images = []\n",
    "\n",
    "# Assuming 'frame_rgb' and 'results_players' are already defined as per your existing code\n",
    "for player in players:\n",
    "    bbox = player['box']  # Get bbox info (x,y,x,y)\n",
    "    cropped_img = annotated_frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "    cropped_images.append(cropped_img)\n",
    "\n",
    "# Assuming colors_list is defined and contains two BGR colors, for example:\n",
    "# colors_list = [(0, 0, 255), (255, 0, 0)]  # BGR format\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Convert BGR to RGB and create a rectangle for each color\n",
    "for i, color in enumerate(colors_list):\n",
    "    # BGR to RGB conversion\n",
    "    rgb_color = color[::-1]  # Reverse the color order\n",
    "    rect = patches.Rectangle((i*1.1, 0), 1, 1, color=[c/255 for c in rgb_color])\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i*1.1 + 0.5, 1.1, f'Team {i+1}', ha='center')\n",
    "\n",
    "# Set limits and remove axes for clarity\n",
    "ax.set_xlim(0, len(colors_list)*1.1)\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "team_names = [\"Team1\", \"Team2\"]\n",
    "\n",
    "for i, (img, palette) in enumerate(zip(cropped_images, obj_palette_list)):\n",
    "    team = team_names[players_teams_list[i]]\n",
    "    print(f'Player {i+1}: {team}')\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "    # Display the cropped image\n",
    "    ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(f'Player {i+1}')\n",
    "\n",
    "    # Prepare data for the histogram\n",
    "    colors = [color for color, _ in palette]\n",
    "    frequencies = [freq for _, freq in palette]\n",
    "\n",
    "    # Display the histogram\n",
    "    bars = ax2.bar(range(len(colors)), frequencies, color=[(r/255, g/255, b/255) for r, g, b in colors])\n",
    "    ax2.set_title('Color Histogram')\n",
    "    ax2.set_xlabel('Colors')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_xticks([])  # Hide x-ticks as they are not meaningful in this context\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164b9c4-2718-4cff-9a46-2a550caf8a77",
   "metadata": {},
   "source": [
    "# Determine team color before tracking to improve tracking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d95fd-6b61-4939-81b3-6f277bdb6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video frame dimensions\n",
    "frame_width = 1920\n",
    "frame_height = 1080\n",
    "\n",
    "# Convert normalized box coordinates to absolute pixel values\n",
    "def convert_normalized_box(box, width, height):\n",
    "    xtl, ytl, xbr, ybr = box\n",
    "    return [xtl * width, ytl * height, xbr * width, ybr * height]\n",
    "\n",
    "\n",
    "# Adjust ball_data to use absolute pixel values\n",
    "before_tracking_data_absolute = []\n",
    "for frame in before_tracking_data:\n",
    "    objects=[]\n",
    "    for obj in frame:\n",
    "        normalized_box = obj['box']\n",
    "        absolute_box = convert_normalized_box(normalized_box, frame_width, frame_height)\n",
    "        objects.append({'score': obj['score'], 'class': obj['class'], 'box': absolute_box})\n",
    "    before_tracking_data_absolute.append(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e654f9-bba2-49c4-8d63-c203902eb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "current=0\n",
    "\n",
    "    \n",
    "while True:\n",
    "    _, frame = vid.read()\n",
    "    if frame is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "        \n",
    "    frame_data=before_tracking_data_absolute[current]\n",
    "    players=[obj for obj in frame_data if obj['class']=='player']\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)                                      # Convert frame to RGB\n",
    "    obj_palette_list = []                                                                   # Initialize players color palette list\n",
    "#    palette_interval = (0,5)                                                                # Color interval to extract from dominant colors palette (1rd to 5th color)\n",
    "#    annotated_frame = frame.copy()                                                                 # Create annotated frame \n",
    "    for player in players:\n",
    "        bbox = player['box']                                                            # Get bbox info (x,y,x,y)\n",
    "        obj_img = frame_rgb[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]       # Crop bbox out of the frame\n",
    "        \n",
    "        obj_img_w, obj_img_h = obj_img.shape[1], obj_img.shape[0]\n",
    "        center_filter_x1 = np.max([(obj_img_w//2)-(obj_img_w//5), 1])\n",
    "        center_filter_x2 = (obj_img_w//2)+(obj_img_w//5)\n",
    "        center_filter_y1 = np.max([(obj_img_h//3)-(obj_img_h//5), 1])\n",
    "        center_filter_y2 = (obj_img_h//3)+(obj_img_h//5)\n",
    "        center_filter = obj_img[center_filter_y1:center_filter_y2, \n",
    "                                center_filter_x1:center_filter_x2]\n",
    "    \n",
    "       \n",
    "        \n",
    "        obj_pil_img = Image.fromarray(np.uint8(center_filter))                          # Convert to pillow image\n",
    "            \n",
    "        reduced = obj_pil_img.convert(\"P\", palette=Image.Palette.WEB)                   # Convert to web palette (216 colors)\n",
    "        palette = reduced.getpalette()                                                  # Get palette as [r,g,b,r,g,b,...]\n",
    "        palette = [palette[3*n:3*n+3] for n in range(256)]                              # Group 3 by 3 = [[r,g,b],[r,g,b],...]\n",
    "        color_count = [(n, palette[m]) for n,m in reduced.getcolors()]                  # Create list of palette colors with their frequency\n",
    "    #    RGB_df = pd.DataFrame(color_count, columns = ['cnt', 'RGB']).sort_values(       # Create dataframe based on defined palette interval\n",
    "    #                          by = 'cnt', ascending = False).iloc[\n",
    "    #                              palette_interval[0]:palette_interval[1],:]\n",
    "    #    palette = list(RGB_df.RGB)                                                      # Convert palette to list (for faster processing)\n",
    "    #    annotated_frame = cv2.rectangle(annotated_frame,                                # Add center filter bbox annotations\n",
    "    #                                    (int(bbox[0])+center_filter_x1, \n",
    "    #                                     int(bbox[1])+ center_filter_y1),  \n",
    "    #                                    (int(bbox[0])+center_filter_x2, \n",
    "    #                                     int(bbox[1])+center_filter_y2), (0,0,0), 2)\n",
    "        player_palette = []\n",
    "        for count, color in color_count:\n",
    "                player_palette.append((color, count))\n",
    "    \n",
    "        obj_palette_list.append(player_palette)\n",
    "\n",
    "    ## Calculate distances between each color from every detected player color palette and the predefined teams colors\n",
    "    players_distance_features = []\n",
    "    \n",
    "    # Loop over detected players' extracted color palettes\n",
    "    for player_palette in obj_palette_list:\n",
    "        palette_distance = []\n",
    "    \n",
    "        # Loop over colors in the player's palette (color is a tuple of (RGB, frequency))\n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Loop over predefined list of teams colors (now only two colors, one for each team)\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)  # Calculate Euclidean distance in Lab color space\n",
    "                distance_list.append(distance)  # Update distance list for current color\n",
    "    \n",
    "            palette_distance.append((distance_list, freq))  # Update distance list for current palette with frequency\n",
    "    \n",
    "        players_distance_features.append(palette_distance)  # Update distance features list\n",
    "    \n",
    "    \n",
    "    \n",
    "    players_teams_list = []\n",
    "    \n",
    "    for player_palette in obj_palette_list:\n",
    "        team_votes = [0, 0]  # Initialize votes for two teams\n",
    "    \n",
    "        for color, freq in player_palette:\n",
    "            color_lab = skimage.color.rgb2lab([i/255 for i in color])  # Convert color to L*a*b* space\n",
    "            distance_list = []\n",
    "    \n",
    "            # Calculate distances for the color\n",
    "            for c in color_list_lab:\n",
    "                distance = skimage.color.deltaE_cie76(color_lab, c)\n",
    "                distance_list.append(distance)\n",
    "    \n",
    "            # Weighted voting\n",
    "            team_idx = distance_list.index(min(distance_list))  # Closer team gets the vote\n",
    "            team_votes[team_idx] += freq  # Weight vote by frequency of the color\n",
    "    \n",
    "        # Team with the highest weighted vote total is predicted\n",
    "        predicted_team = team_votes.index(max(team_votes))\n",
    "        players_teams_list.append(predicted_team)\n",
    "\n",
    "    for i in range(len(players)):\n",
    "        team=f\"team{players_teams_list[i]+1}\"\n",
    "        players[i]['class']=team\n",
    "        \n",
    "    current+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afe8ca-71d5-4abf-941a-df921c466cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_yellow.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_yellow8.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "\n",
    "color_dict = {\n",
    "\"team1\": (0, 0, 255),    \n",
    "\"team2\": (0, 255, 255),    \n",
    "\"referee\": (255,255,0), \n",
    "\"goalkeeper\": (255, 0, 0), \n",
    "\"player\": (255, 255, 0), \n",
    "\"ball\": (255, 255, 255)  \n",
    "}\n",
    "\n",
    "for frame in before_tracking_data_absolute:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:     \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "\n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "    for obj in ball_data_absolute[current]:   \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),color_dict[obj['class']], 2)\n",
    "\n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    color_dict[obj['class']], 2)\n",
    "\n",
    "\n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e21867-1d1f-408c-8688-701d527c789e",
   "metadata": {},
   "source": [
    "# Tactical map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6aa95-3cb9-4594-98b1-7a75e17795f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "key_points_list = [\n",
    "    \"TLC\", \"TRC\", \"TR6MC\", \"TL6MC\", \"TR6ML\", \"TL6ML\",\n",
    "    \"TR18MC\", \"TL18MC\", \"TR18ML\", \"TL18ML\", \"TRArc\", \"TLArc\",\n",
    "    \"RML\", \"RMC\", \"LMC\", \"LML\", \"BLC\", \"BRC\", \"BR6MC\", \"BL6MC\",\n",
    "    \"BR6ML\", \"BL6ML\", \"BR18MC\", \"BL18MC\", \"BR18ML\", \"BL18ML\",\n",
    "    \"BRArc\", \"BLArc\"\n",
    "]\n",
    "\n",
    "new_key_points = {}  # Dictionary to store new key points\n",
    "current_key_point = 0  # Index to keep track of current key point\n",
    "\n",
    "# Function to handle mouse click events\n",
    "def click_event(event, x, y, flags, param):\n",
    "    global current_key_point, new_key_points\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and current_key_point < len(key_points_list):\n",
    "        new_key_points[key_points_list[current_key_point]] = [x, y]\n",
    "        print(f\"Key Point {key_points_list[current_key_point]}: ({x}, {y})\")\n",
    "        current_key_point += 1\n",
    "        if current_key_point < len(key_points_list):\n",
    "            print(f\"Click the position of {key_points_list[current_key_point]}\")\n",
    "        else:\n",
    "            print(\"All key points have been defined.\")\n",
    "        cv2.circle(img, (x, y), radius=5, color=(0, 255, 0), thickness=-1)\n",
    "        cv2.imshow('Tactical Map', img)\n",
    "\n",
    "# Path to your new tactical map image\n",
    "image_path = 'unnamed-chunk-2-1 (1).png'\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(image_path)\n",
    "cv2.imshow('Tactical Map', img)\n",
    "\n",
    "# Prompt for the first key point\n",
    "print(f\"Click the position of {key_points_list[current_key_point]}\")\n",
    "\n",
    "# Set the mouse callback function to 'click_event'\n",
    "cv2.setMouseCallback('Tactical Map', click_event)\n",
    "\n",
    "# Wait until a key is pressed and close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the new key points dictionary\n",
    "print(\"New Key Points:\")\n",
    "print(new_key_points)\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0d765-3965-422c-abb4-bce49f96f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "new_key_points={'TLC': [100, 862], 'TRC': [101, 69], 'TR6MC': [159, 366], 'TL6MC': [159, 566], 'TR6ML': [100, 366], 'TL6ML': [100, 565], 'TR18MC': [278, 248], 'TL18MC': [278, 682], 'TR18ML': [101, 249], 'TL18ML': [100, 684], 'TRArc': [280, 388], 'TLArc': [279, 544], 'RML': [694, 70], 'RMC': [696, 366], 'LMC': [694, 562], 'LML': [696, 860], 'BLC': [1288, 862], 'BRC': [1287, 71], 'BR6MC': [1228, 367], 'BL6MC': [1228, 563], 'BR6ML': [1287, 365], 'BL6ML': [1286, 564], 'BR18MC': [1110, 249], 'BL18MC': [1110, 683], 'BR18ML': [1286, 248], 'BL18ML': [1287, 680], 'BRArc': [1109, 385], 'BLArc': [1110, 545]}\n",
    "# Specify the filename for the JSON file\n",
    "json_filename = 'updated_key_points.json'\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(json_filename, 'w') as file:\n",
    "    json.dump(new_key_points, file, indent=4)\n",
    "\n",
    "print(f\"Key points saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657ae22-957e-41f2-8a93-8359c73a885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import json\n",
    "\n",
    "# Load the JSON file with the key points\n",
    "with open('updated_key_points.json', 'r') as file:\n",
    "    key_points = json.load(file)\n",
    "\n",
    "# Load the tactical map image\n",
    "tactical_map = mpimg.imread('unnamed-chunk-2-1 (1).png')\n",
    "\n",
    "# Create a figure with a larger size\n",
    "plt.figure(figsize=(15, 10))  # Adjust the size as needed\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(tactical_map)\n",
    "plt.axis('off')  # Optionally remove the axis\n",
    "\n",
    "# Plot each key point and annotate\n",
    "for label, coordinates in key_points.items():\n",
    "    x, y = coordinates\n",
    "    plt.scatter(x, y, marker='o')  # Plot the point\n",
    "    plt.annotate(label, (x, y), textcoords=\"offset points\", xytext=((-3),5), \n",
    "                 ha='left', fontsize=8, color='red')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('annotated_tactical_map2.jpg', format='jpg', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024ea72-b961-49c1-8cb0-5433d4f48f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('annotations 9.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Initialize arrays for results\n",
    "keypoint_results = []\n",
    "\n",
    "# Function to convert box attributes to a dictionary\n",
    "def get_box_dict(box_element):\n",
    "    return [\n",
    "        float(box_element.get('xtl')),\n",
    "        float(box_element.get('ytl')),\n",
    "        float(box_element.get('xbr')),\n",
    "        float(box_element.get('ybr'))]\n",
    "\n",
    "# Process each track (object)\n",
    "for track in root.findall('track'):\n",
    "    label = track.get('label')\n",
    "\n",
    "    # Dictionary to store boxes for each frame\n",
    "    frames = {}\n",
    "\n",
    "    # Process each box (appearance in a frame)\n",
    "    for box in track.findall('box'):\n",
    "        frame_num = int(box.get('frame'))\n",
    "        outside = box.get('outside') == '1'\n",
    "\n",
    "        if not outside:\n",
    "            box_dict = get_box_dict(box)\n",
    "            frames[frame_num] = {'box': box_dict, 'class': label}\n",
    "\n",
    "    # Sort frames and add to the respective result array\n",
    "    sorted_frames = sorted(frames.items())\n",
    "    for frame_num, frame_data in sorted_frames:\n",
    "        while len(keypoint_results) <= frame_num:\n",
    "            keypoint_results.append([])\n",
    "        keypoint_results[frame_num].append(frame_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f241e-02da-4462-849d-7b0d1165247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_central_coordinates(results, is_ball=False):\n",
    "    central_pts = []\n",
    "    for obj in results:\n",
    "        # Convert from xyxy to xywh format\n",
    "        x1, y1, x2, y2 = obj['box']\n",
    "        x = x1\n",
    "        y = y1\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        # Calculate central coordinates based on object type\n",
    "        if is_ball:  # For balls\n",
    "            central_pts.append([x + w / 2, y + h / 2])\n",
    "        else:  # For players, goalkeepers, and referees\n",
    "            central_pts.append([x + w / 2, y + h])\n",
    "\n",
    "    return np.array(central_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bdbbc-4d7b-4984-b94b-7ca964671c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_points = new_key_points\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Set keypoints average displacement tolerance level (in pixels) [set to -1 to always update homography matrix]\n",
    "keypoints_displacement_mean_tol = 10\n",
    "tac_map = cv2.imread('unnamed-chunk-2-1 (1).png')\n",
    "\n",
    "player_pos=[]\n",
    "other_pos=[]\n",
    "ball_pos=[]\n",
    "\n",
    "for frame_nbr in range (len(player_results)):\n",
    "    \n",
    "    detected_labels=[obj['class'] for obj in keypoint_results[frame_nbr]]\n",
    "    detected_labels_src_pts_xyxy=[obj['box'] for obj in keypoint_results[frame_nbr]]\n",
    "    detected_labels_src_pts= np.array([np.round([(x1 + x2) / 2, (y1 + y2) / 2]).astype(int) for x1, y1, x2, y2 in detected_labels_src_pts_xyxy])\n",
    "    detected_labels_dst_pts=np.array([key_points[obj['class']] for obj in keypoint_results[frame_nbr]])\n",
    "    \n",
    "    ## Calculate Homography transformation matrix when more than 4 keypoints are detected\n",
    "    if len(detected_labels) > 3:\n",
    "        # Always calculate homography matrix on the first frame\n",
    "        if frame_nbr > 0:\n",
    "            # Determine common detected field keypoints between previous and current frames\n",
    "            common_labels = set(detected_labels_prev) & set(detected_labels)\n",
    "            # When at least 4 common keypoints are detected, determine if they are displaced on average beyond a certain tolerance level\n",
    "            if len(common_labels) > 3:\n",
    "                common_label_idx_prev = [detected_labels_prev.index(i) for i in common_labels]   # Get labels indexes of common detected keypoints from previous frame\n",
    "                common_label_idx_curr = [detected_labels.index(i) for i in common_labels]        # Get labels indexes of common detected keypoints from current frame\n",
    "                coor_common_label_prev = detected_labels_src_pts_prev[common_label_idx_prev]     # Get labels coordiantes of common detected keypoints from previous frame\n",
    "                coor_common_label_curr = detected_labels_src_pts[common_label_idx_curr]          # Get labels coordiantes of common detected keypoints from current frame\n",
    "                coor_error = mean_squared_error(coor_common_label_prev, coor_common_label_curr)  # Calculate error between previous and current common keypoints coordinates\n",
    "                update_homography = coor_error > keypoints_displacement_mean_tol                 # Check if error surpassed the predefined tolerance level\n",
    "            else:\n",
    "                update_homography = True                                                         \n",
    "        else:\n",
    "            update_homography = True\n",
    "    \n",
    "        if  update_homography:\n",
    "            h, mask = cv2.findHomography(detected_labels_src_pts,                   # Calculate homography matrix\n",
    "                                          detected_labels_dst_pts)                  \n",
    "\n",
    "        detected_labels_prev = detected_labels.copy()                               # Save current detected keypoint labels for next frame\n",
    "        detected_labels_src_pts_prev = detected_labels_src_pts.copy()               # Save current detected keypoint coordiantes for next frame\n",
    "\n",
    "        detected_player_src_pts = calculate_central_coordinates(player_results[frame_nbr])\n",
    "        detected_other_src_pts = calculate_central_coordinates(other_results[frame_nbr])\n",
    "        detected_ball_src_pts = calculate_central_coordinates(ball_results[frame_nbr], is_ball=True)\n",
    "\n",
    "        # Transform players coordinates from frame plane to tactical map plance using the calculated Homography matrix\n",
    "        pred_player_pts = []                                                           # Initialize players tactical map coordiantes list\n",
    "        for pt in detected_player_src_pts:                                            # Loop over players frame coordiantes\n",
    "            pt = np.append(np.array(pt), np.array([1]), axis=0)                     # Covert to homogeneous coordiantes\n",
    "            dest_point = np.matmul(h, np.transpose(pt))                              # Apply homography transofrmation\n",
    "            dest_point = dest_point/dest_point[2]                                   # Revert to 2D-coordiantes\n",
    "            pred_player_pts.append(list(np.transpose(dest_point)[:2]))                 # Update players tactical map coordiantes list\n",
    "        pred_player_pts = np.array(pred_player_pts)\n",
    "        player_pos.append(pred_player_pts)\n",
    "        \n",
    "        pred_other_pts = []                                                           # Initialize players tactical map coordiantes list\n",
    "        for pt in detected_other_src_pts:                                            # Loop over players frame coordiantes\n",
    "            pt = np.append(np.array(pt), np.array([1]), axis=0)                     # Covert to homogeneous coordiantes\n",
    "            dest_point = np.matmul(h, np.transpose(pt))                              # Apply homography transofrmation\n",
    "            dest_point = dest_point/dest_point[2]                                   # Revert to 2D-coordiantes\n",
    "            pred_other_pts.append(list(np.transpose(dest_point)[:2]))                 # Update players tactical map coordiantes list\n",
    "        pred_other_pts = np.array(pred_other_pts)\n",
    "        other_pos.append(pred_other_pts)\n",
    "\n",
    "        pred_ball_pts = []                                                           # Initialize players tactical map coordiantes list\n",
    "        for pt in detected_ball_src_pts:                                            # Loop over players frame coordiantes\n",
    "            pt = np.append(np.array(pt), np.array([1]), axis=0)                     # Covert to homogeneous coordiantes\n",
    "            dest_point = np.matmul(h, np.transpose(pt))                              # Apply homography transofrmation\n",
    "            dest_point = dest_point/dest_point[2]                                   # Revert to 2D-coordiantes\n",
    "            pred_ball_pts.append(list(np.transpose(dest_point)[:2]))                 # Update players tactical map coordiantes list\n",
    "        pred_ball_pts = np.array(pred_ball_pts)\n",
    "        ball_pos.append(pred_ball_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e2e25-b075-4894-8ba0-b7cf85bf7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_left=(67, 36)\n",
    "bottom_right=(1321, 893)\n",
    "new_width = bottom_right[0] - top_left[0]\n",
    "new_height = bottom_right[1] - top_left[1]\n",
    "\n",
    "# Ball representation\n",
    "ball_image_path = 'pngegg.png'  # Path to the ball PNG image\n",
    "ball_image = cv2.imread(ball_image_path, cv2.IMREAD_UNCHANGED)  # Load with alpha channel\n",
    "ball_size = 20  # Diameter of the ball image, adjust as needed\n",
    "\n",
    "tac_map = cv2.imread('unnamed-chunk-2-1 (1).png')\n",
    "\n",
    "\n",
    "\n",
    "overlay_width = 400  # Adjust to your preference\n",
    "overlay_height = int(new_height * (overlay_width / new_width))\n",
    "\n",
    "# Setup the video capture\n",
    "vid = cv2.VideoCapture('./data/video/results-red_green6.mp4')\n",
    "vid_fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width, vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "pos_x = vid_width - overlay_width\n",
    "pos_y = vid_height - overlay_height\n",
    "# Setup the video writer with the size of the tactical map\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('./data/video/results-red_green_tacmap_overlay.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "frame_nbr=0\n",
    "\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    detected_labels=[obj['class'] for obj in keypoint_results[frame_nbr]]\n",
    "    if len(detected_labels) > 3:\n",
    "        tac_map_copy = tac_map.copy()\n",
    "\n",
    "        pred_player_pts=player_pos[frame_nbr]\n",
    "        pred_other_pts=other_pos[frame_nbr]\n",
    "        pred_ball_pts=ball_pos[frame_nbr]\n",
    "        \n",
    "        # Player representation\n",
    "        player_radius = 15  # Adjust the radius as needed\n",
    "        team1_color = (0, 0, 255)  # Red\n",
    "        team2_color = (0, 255, 0)  # Yellow\n",
    "        contour_color = (0, 0, 0)  # White color for contour\n",
    "        contour_thickness = 2  # Thickness of the white contour\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.7  # Adjust as needed based on your image size\n",
    "        font_color = (0, 0, 0)  # White color for text\n",
    "        font_thickness = 1\n",
    "        \n",
    "        for i, player in enumerate(pred_player_pts):\n",
    "            player_center = (int(player[0]), int(player[1]))\n",
    "            team_color = team1_color if player_results[frame_nbr][i]['class'] == 'team1' else team2_color\n",
    "        \n",
    "            # Draw the contour circle first (slightly larger)\n",
    "            cv2.circle(tac_map_copy, player_center, player_radius + contour_thickness, contour_color, thickness=-1)\n",
    "        \n",
    "            # Then draw the filled circle with the team color\n",
    "            cv2.circle(tac_map_copy, player_center, player_radius, team_color, -1)\n",
    "\n",
    "            # Get the player's ID and convert it to string\n",
    "            player_id = str(player_results[frame_nbr][i]['id'])\n",
    "        \n",
    "            # Calculate text size and position, then put the player's ID inside the circle\n",
    "            text_size = cv2.getTextSize(player_id, font, font_scale, font_thickness)[0]\n",
    "            text_x = player_center[0] - text_size[0] // 2\n",
    "            text_y = player_center[1] + text_size[1] // 2\n",
    "            cv2.putText(tac_map_copy, player_id, (text_x, text_y), font, font_scale, font_color, font_thickness)\n",
    "\n",
    "        other_radius = 15  # Adjust the radius as needed\n",
    "        ref_color = (255, 255, 0)  \n",
    "        gk_color = (255, 0, 0)  \n",
    "        contour_color = (255, 255, 255)  # White color for contour\n",
    "        contour_thickness = 2  # Thickness of the white contour\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.7  # Adjust as needed based on your image size\n",
    "        font_color = (0, 0, 0)  # White color for text\n",
    "        font_thickness = 1\n",
    "        \n",
    "        for i, other in enumerate(pred_other_pts):\n",
    "            other_center = (int(other[0]), int(other[1]))\n",
    "            team_color = ref_color if other_results[frame_nbr][i]['class'] == 'referee' else gk_color\n",
    "        \n",
    "            # Draw the contour circle first (slightly larger)\n",
    "            cv2.circle(tac_map_copy, other_center, other_radius + contour_thickness, contour_color, thickness=-1)\n",
    "        \n",
    "            # Then draw the filled circle with the team color\n",
    "            cv2.circle(tac_map_copy, other_center, other_radius, team_color, -1)\n",
    "\n",
    "            # Get the player's ID and convert it to string\n",
    "            id = 'R' if other_results[frame_nbr][i]['class'] == 'referee' else 'GK'\n",
    "        \n",
    "            # Calculate text size and position, then put the player's ID inside the circle\n",
    "            text_size = cv2.getTextSize(id, font, font_scale, font_thickness)[0]\n",
    "            text_x = other_center[0] - text_size[0] // 2\n",
    "            text_y = other_center[1] + text_size[1] // 2\n",
    "            cv2.putText(tac_map_copy, id, (text_x, text_y), font, font_scale, font_color, font_thickness)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        for ball in pred_ball_pts:  # Assuming pred_ball_pts is a list of [x, y] tuples\n",
    "            # Calculate the top-left corner for overlay\n",
    "            x, y = int(ball[0] - ball_size // 2), int(ball[1] - ball_size // 2)\n",
    "        \n",
    "            # Resize the ball image\n",
    "            ball_resized = cv2.resize(ball_image, (ball_size, ball_size))\n",
    "        \n",
    "            # Handle transparency if the ball image has an alpha channel\n",
    "            if ball_resized.shape[2] == 4:  # 4 channels: RGBA\n",
    "                alpha_mask = ball_resized[:, :, 3] / 255.0  # Normalized alpha channel\n",
    "                alpha_inv = 1.0 - alpha_mask\n",
    "        \n",
    "                for c in range(0, 3):  # RGB channels\n",
    "                    tac_map_copy[y:y+ball_size, x:x+ball_size, c] = \\\n",
    "                        alpha_inv * tac_map_copy[y:y+ball_size, x:x+ball_size, c] + \\\n",
    "                        alpha_mask * ball_resized[:, :, c]\n",
    "            else:\n",
    "                # If no alpha channel, overlay directly\n",
    "                tac_map_copy[y:y+ball_size, x:x+ball_size] = ball_resized\n",
    "\n",
    "    frame_nbr+=1\n",
    "\n",
    "    cropped_tac_map = tac_map_copy[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
    "    \n",
    "    tac_map_resized=cv2.resize(cropped_tac_map, (overlay_width, overlay_height))\n",
    "    img[pos_y:pos_y + overlay_height, pos_x:pos_x + overlay_width] = tac_map_resized\n",
    "    \n",
    "    out.write(img)\n",
    "\n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ac31b-4abd-4333-a511-9fac975abfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture('./data/video/red_green.mp4')\n",
    "\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vid_fps =int(vid.get(cv2.CAP_PROP_FPS))\n",
    "vid_width,vid_height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('./data/video/results-red_green_keypoints.mp4', codec, vid_fps, (vid_width, vid_height))\n",
    "\n",
    "current=0\n",
    "\n",
    "for frame in keypoint_results:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "    for obj in frame:     \n",
    "        bbox=obj['box']\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])),(255,255,255), 2)\n",
    "\n",
    "        cv2.putText(img,obj['class'], (int(bbox[0]), int(bbox[1]-10)), 0, 0.75,\n",
    "                    (255,255,255), 2)\n",
    "   \n",
    "    current+=1\n",
    "    out.write(img)\n",
    "\n",
    "    #if cv2.waitKey(1) == ord('q'):\n",
    "    #    break\n",
    "\n",
    "                \n",
    "vid.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6e292-5e3e-41af-a37a-b0d1c0eac7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
